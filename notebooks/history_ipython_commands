 2/1: from numpy import *
 2/2: who
 2/3: a= array(5)
 2/4: a
 2/5: a.shape
 3/1: import fastai
 5/1: import tensorflow as tf
 5/2: import keras
 7/1: import  tensorflow as tf
 7/2: import keras
 8/1: from sys import path
 8/2: path
 9/1: import tenorflow as tf
10/1: import tensorflow as tf
10/2: tf
11/1: import tensorflow as tf
11/2: from sys import path
11/3: paht
11/4: path
12/1: import tensorflow as tf
12/2: tf
12/3: import keras as k
13/1: import tensorflow as tf
14/1: from sys import path
14/2: path
15/1: from sys import path, executable
15/2: path
15/3: executable
16/1: from sys import path, executable
16/2: path
16/3: executable
17/1: from sys import executable
17/2: executable='/usr/bin/python3'
17/3: from sys import path
17/4: path
18/1: from sys import executable
18/2: executable='/usr/bin/python3'
18/3: import tensorflow as tf
19/1: from sys import path
19/2: path
20/1: import a_module
20/2: print a_module.__file__
20/3: import tensorflow
20/4: tensorlow.__file__
20/5: who
20/6: print(tensorflow.__file__)
21/1: from sys import path
21/2: path='/home/ubuntu/.local/lib/python3.5/site-packages'+path
21/3: path=['/home/ubuntu/.local/lib/python3.5/site-packages']+path
21/4: path
21/5: import tesnsorflow as tf
21/6: import tensorflow
22/1: import tensorflow
22/2: !pip3 install numpy
22/3: !pip install numpy
23/1: import tensorflwo
23/2: import tensorflow
24/1: import tensorflow as tf
25/1: help
25/2: help()
26/1: import tensorflow as tf
27/1: import tensorflow
27/2: import keras
28/1: import tensorflow as tf
28/2: import keras as k
28/3: from sys import path
28/4: path.append('~/.local/lib/python3.5/site-packages')
28/5: import keras as k
28/6: path
28/7: path.append('/home/ubuntu/.local/lib/python3.5/site-packages')
28/8: import keras
29/1:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages')
29/2:
import tensorflow as tf
import keraas as k
29/3:
import tensorflow as tf
import keras as k
29/4:
import tensorflow as tf
import keras as k
import numpy as np
import matplotlib.pyplot as plt
30/1:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages')
30/2:
import tensorflow as tf
import keras as k
import numpy as np
import matplotlib.pyplot as plt
30/3: from keras.datasets import mnist
30/4: import numpy as np
30/5:
a = np.ones(10000,1000000)
b = np.ones(1000000,10000)
30/6:
a = np.ones((10000,1000000))
b = np.ones((1000000,10000))
30/7:
a = np.ones((10000,1000))
b = np.ones((1000,10000))
30/8: c = a.dot(b)
30/9: print(a.shape)
30/10: print(a)
30/11: print(*a)
31/1:
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications import MobileNet
from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model

from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm
import gc
import cv2 as cv
31/2:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages/')
31/3:
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications import MobileNet
from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model

from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm
import gc
import cv2 as cv
32/1: import cv2
33/1: import cv2
33/2:
!pip install opencv-python
!apt update && apt install -y libsm6 libxext6
33/3: exi
34/1: import cv2
35/1: import cv2
36/1: import cv2
37/1: import cv2
38/1: import cv2
31/4:
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications import MobileNet
from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model

from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm
import gc
import cv2 as cv
31/5:
import numpy as np
import tensorflow as tf
import random as rn
31/6:
import numpy as np
import tensorflow as tf
import random as rn

# The below is necessary in Python 3.2.3 onwards to
# have reproducible behavior for certain hash-based operations.
# See these references for further details:
# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED
# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926

import os
os.environ['PYTHONHASHSEED'] = '0'

# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.

np.random.seed(42)

# The below is necessary for starting core Python generated random numbers
# in a well-defined state.

rn.seed(12345)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of
# non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res

session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

from keras import backend as K

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed

tf.set_random_seed(1234)

sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)
31/7:
img_width, img_height = (128, 128)

train_data_dir = '../dogs-vs-cats/train/'
test_data_dir = '../dogs-vs-cats/test1/'
epochs = 10
batch_size = 128

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)
31/8: Mobile_model = MobileNet(include_top=False, input_shape=input_shape)
31/9: K.image_data_format()
31/10:
def get_model():
# add a global spatial average pooling layer
    x = Mobile_model.output
    x = GlobalAveragePooling2D()(x)
    predictions = Dense(85, activation='sigmoid')(x)
    model = Model(inputs=Mobile_model.input, outputs=predictions)
    
    return model
31/11: model = get_model()
31/12:
#train only last layer
for layer in model.layers[:-1]:
    layer.trainable = False

model.summary()
39/1:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages/')
39/2:
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications import MobileNet
from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model

from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm
import gc
import cv2 as cv
39/3:
import numpy as np
import tensorflow as tf
import random as rn

# The below is necessary in Python 3.2.3 onwards to
# have reproducible behavior for certain hash-based operations.
# See these references for further details:
# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED
# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926

import os
os.environ['PYTHONHASHSEED'] = '0'

# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.

np.random.seed(42)

# The below is necessary for starting core Python generated random numbers
# in a well-defined state.

rn.seed(12345)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of
# non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res

session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

from keras import backend as K

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed

tf.set_random_seed(1234)

sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)
39/4:
img_width, img_height = (128, 128)

train_data_dir = '../dogs-vs-cats/train/'
test_data_dir = '../dogs-vs-cats/test1/'
epochs = 10
batch_size = 128

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)
39/5: Mobile_model = MobileNet(include_top=False, input_shape=input_shape)
39/6:
def get_model():
# add a global spatial average pooling layer
    x = Mobile_model.output
    x = GlobalAveragePooling2D()(x)
    predictions = Dense(85, activation='sigmoid')(x)
    model = Model(inputs=Mobile_model.input, outputs=predictions)
    
    return model
39/7: model = get_model()
39/8:
#train only last layer
for layer in model.layers[:-1]:
    layer.trainable = False

model.summary()
39/9:
# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(width_shift_range=0.2, height_shift_range=0.2,
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True, rotation_range = 20)

val_datagen = ImageDataGenerator(rescale=1./255)

test_datagen = ImageDataGenerator(rescale=1./255)
40/1:
TRAIN_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/train/'
TEST_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/test1/'

ROWS = 64
COLS = 64
CHANNELS = 3

train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset
train_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]
train_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]

img_width, img_height = (128, 128)
epochs = 10
batch_size = 128

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)
40/2:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages/')
40/3:
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications import MobileNet
from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model

from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm
import gc
import cv2, os
40/4:
import numpy as np
import tensorflow as tf
import random as rn

# The below is necessary in Python 3.2.3 onwards to
# have reproducible behavior for certain hash-based operations.
# See these references for further details:
# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED
# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926

import os
os.environ['PYTHONHASHSEED'] = '0'

# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.

np.random.seed(42)

# The below is necessary for starting core Python generated random numbers
# in a well-defined state.

rn.seed(12345)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of
# non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res

session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

from keras import backend as K

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed

tf.set_random_seed(1234)

sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)
40/5:
TRAIN_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/train/'
TEST_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/test1/'

ROWS = 64
COLS = 64
CHANNELS = 3

train_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset
train_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]
train_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]

img_width, img_height = (128, 128)
epochs = 10
batch_size = 128

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)
41/1:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages/')
41/2:
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications import MobileNet
from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model

from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm
import gc
import cv2, os
41/3:
import numpy as np
import tensorflow as tf
import random as rn

# The below is necessary in Python 3.2.3 onwards to
# have reproducible behavior for certain hash-based operations.
# See these references for further details:
# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED
# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926

import os
os.environ['PYTHONHASHSEED'] = '0'

# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.

np.random.seed(42)

# The below is necessary for starting core Python generated random numbers
# in a well-defined state.

rn.seed(12345)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of
# non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res

session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

from keras import backend as K

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed

tf.set_random_seed(1234)

sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)
41/4:
TRAIN_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/train/'
TEST_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/test1/'
41/5: os.listdir(train)
41/6: os.listdir(TRAIN_DIR)
41/7: os.listdir(TRAIN_DIR)[:5]
41/8:
files = os.listdir(TRAIN_DIR)[:5]
files
41/9:
img = plt.imread(f'{TRAIN_DIR}/files[0]')
plt.imshow(img)
41/10:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages/')
41/11:
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications import MobileNet
from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm
import gc
import cv2, os
41/12:
img = plt.imread(f'{TRAIN_DIR}/files[0]')
plt.imshow(img)
41/13:
img = plt.imread(f'{TRAIN_DIR}files[0]')
plt.imshow(img)
41/14:
img = plt.imread(f'{TRAIN_DIR}{files[0]}')
plt.imshow(img)
41/15:
img = plt.imread(f'{TRAIN_DIR}{files[0]}')
plt.imshow(img);
41/16:
%matplotlib inline
img = plt.imread(f'{TRAIN_DIR}{files[0]}')
plt.imshow(img)
41/17:
%matplotlib inline
img = plt.imread(f'{TRAIN_DIR}{files[0]}')
plt.imshow(img);
41/18: img.shape
41/19:
!mkdir ../ubuntu/.kaggle/competitions/dogs-vs-cats/train/cats
!mkdir ../ubuntu/.kaggle/competitions/dogs-vs-cats/train/dogs
41/20:
import shutil
 
def move(file, src="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/, /home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/):
    if file[0] == d:
        shutil.move(src+file, dest+'dogs')
    else:
         shutil.move(src+file, dest+'cats')
41/21:
import shutil
 
def move(file, src="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/", "/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/"):
    if file[0] == d:
        shutil.move(src+file, dest+'dogs')
    else:
         shutil.move(src+file, dest+'cats')
41/22:
import shutil
 
def move(file, src="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/", dest="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/"):
    if file[0] == d:
        shutil.move(src+file, dest+'dogs')
    else:
         shutil.move(src+file, dest+'cats')
41/23:
for file in files:
    move(file)
41/24:
for file in tqdm(files):
    move(file)
41/25:
import shutil
 
def move(file, src="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/", dest="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/"):
    if file[0] == 'd':
        shutil.move(src+file, dest+'dogs')
    else:
         shutil.move(src+file, dest+'cats')
41/26:
for file in tqdm(files):
    move(file)
41/27:
for file in tqdm(files):
    move(file)
41/28: files = os.listdir(TRAIN_DIR)
41/29: len(files)
41/30:
for file in tqdm(files):
    move(file)
41/31:
%matplotlib inline
img = plt.imread(f'{TRAIN_DIR}/cats/{files[0]}')
plt.imshow(img);
41/32:
%matplotlib inline
img = plt.imread(f'{TRAIN_DIR}cats/{files[0]}')
plt.imshow(img);
41/33:
img = plt.imread(f'{TRAIN_DIR}dogs/{files[0]}')
plt.imshow(img);
42/1:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages/')
42/2:
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications import MobileNet
from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm
import gc
import cv2, os
42/3:
import numpy as np
import tensorflow as tf
import random as rn

# The below is necessary in Python 3.2.3 onwards to
# have reproducible behavior for certain hash-based operations.
# See these references for further details:
# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED
# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926

import os
os.environ['PYTHONHASHSEED'] = '0'

# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.

np.random.seed(42)

# The below is necessary for starting core Python generated random numbers
# in a well-defined state.

rn.seed(12345)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of
# non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res

session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

from keras import backend as K

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed

tf.set_random_seed(1234)

sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)
42/4:
TRAIN_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/train/'
TEST_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/test1/'
42/5: files_train = os.listdir(TRAIN_DIR)
43/1:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages/')
43/2:
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications import MobileNet
from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm
import gc
import cv2, os
43/3:
import numpy as np
import tensorflow as tf
import random as rn

# The below is necessary in Python 3.2.3 onwards to
# have reproducible behavior for certain hash-based operations.
# See these references for further details:
# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED
# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926

import os
os.environ['PYTHONHASHSEED'] = '0'

# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.

np.random.seed(42)

# The below is necessary for starting core Python generated random numbers
# in a well-defined state.

rn.seed(12345)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of
# non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res

session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

from keras import backend as K

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed

tf.set_random_seed(1234)

sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)
43/4:
TRAIN_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/train/'
TEST_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/test/'
43/5: files_train = os.listdir(TRAIN_DIR)
43/6:
# import shutil
 
# def move(file, src="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/", dest="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/"):
#     if file[0] == 'd':
#         shutil.move(src+file, dest+'dogs')
#     else:
#          shutil.move(src+file, dest+'cats')
43/7:
# for file in tqdm(files_train):
#     move(file)
43/8: files_train = os.listdir(TRAIN_DIR+'dogs')
43/9:
img = plt.imread(f'{TRAIN_DIR}dogs/{files[0]}')
plt.imshow(img);
43/10:
img = plt.imread(f'{TRAIN_DIR}dogs/{files_train[0]}')
plt.imshow(img);
43/11:
ROWS = 64
COLS = 64
CHANNELS = 3

train_dogs = os.listdir(TRAIN_DIR+'dogs')
train_cats = os.listdir(TRAIN_DIR+'cats')
# train_images = train_dogs.extend(train_cats)

img_width, img_height = (128, 128)
epochs = 10
batch_size = 128

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)
43/12: Mobile_model = MobileNet(include_top=False, input_shape=input_shape)
43/13:
def get_model():
# add a global spatial average pooling layer
    x = Mobile_model.output
    x = GlobalAveragePooling2D()(x)
    predictions = Dense(85, activation='sigmoid')(x)
    model = Model(inputs=Mobile_model.input, outputs=predictions)
    
    return model
43/14: model = get_model()
43/15:
#train only last layer
for layer in model.layers[:-1]:
    layer.trainable = False

model.summary()
44/1: from IPython.lib import passwd
44/2: passwd()
45/1: from IPython.lib import passwd
45/2: passwd()
47/1: import fastai
48/1: import fastai
50/1:
# This file contains all the main external libs we'll use
from fastai.imports import *
51/1: import fastai
51/2: from fastai.imports import *
51/3:
# This file contains all the main external libs we'll use
from fastai.imports import *
51/4: from fastai import imports
51/5: import fastai
51/6: help(fastai)
51/7: from fastai.fastai.imports import *
51/8: from  fastai import fastai
51/9: help(fastai)
51/10: from fastai.imports import *
51/11: from fastai.fastai.imports import *
51/12: from fastai.fastai.imports import *
52/1: from fastai.fastai.imports import *
52/2: from fastai.fastai.imports import *
53/1: from fastai.fastai.imports import *
54/1: exi
55/1: from fastai.imports import *
56/1: from fastai.imports import *
57/1: from fastai.imports import *
58/1: from fastai.imports import *
59/1: from fastai.imports import *
60/1: from fastai.imports import *
60/2: cd ..
60/3: cd ..
60/4: from fastai.imports import *
60/5: cd ..
60/6: from fastai.imports import *
60/7: ls
60/8: cd ubuntu/
60/9: cd fastai
60/10: from fastai import imports
61/1: from fastai import imports
50/2:
from fastai.transforms import *
from fastai.conv_learner import *
from fastai.model import *
from fastai.dataset import *
from fastai.sgdr import *
from fastai.plots import *
50/3: cd fastai
50/4: cd ../fastai
50/5:
from fastai.transforms import *
from fastai.conv_learner import *
from fastai.model import *
from fastai.dataset import *
from fastai.sgdr import *
from fastai.plots import *
50/6:
try:
    from fastai.imports import *
50/7:
try:
    from fastai.imports import *
except:
    print('not possible')
50/8: from fastai.imports import *
50/9: torch.cuda.is_available()
62/1: from random import shuffle
62/2: from shutil import move
62/3: from os import *
62/4: listdir('~/')
62/5: listdir('~')
62/6: listdir('/')
50/10:
# This file contains all the main external libs we'll use
from fastai.imports import *
50/11: cd ../fastai
63/1: from random import shuffle
63/2: from shutil import copy
63/3: from os import listdir
63/4: ls
63/5: dogs=listdir('test/')
63/6: dogs
63/7: dogs=listdir('test/dogs')
63/8: cats=listdir('test/cats')
63/9: dogs,cats = shuffle(dogs), shuffle(cats)
63/10: from shutil import copy, move
50/12:
from shutil import move
move?
63/11:
for i in range(100):
    move(f'/home/ubuntu/.kaggle/competitions/dogs-vs-cats/test/dogs/{dogs[i]}', 'sample/dogs')
63/12: dogs[i]
63/13: dogs=listdir('test/dogs')
63/14: cats=listdir('test/cats')
63/15: dogs[i]
63/16: shuffle(dogs)
63/17: dogs[i]
63/18: shuffle(cats)
63/19:
for i in range(100):
    move(f'/home/ubuntu/.kaggle/competitions/dogs-vs-cats/test/dogs/{dogs[i]}', 'sample/dogs')
63/20:
for i in range(100):
    move(f'/home/ubuntu/.kaggle/competitions/dogs-vs-cats/test/cats/{dogs[i]}', 'sample/cats')
63/21:
for i in range(100):
    move(f'/home/ubuntu/.kaggle/competitions/dogs-vs-cats/test/cats/{cats[i]}', 'sample/cats')
50/13: ls
50/14:
PATH = "../.kaggle/competitions/dogs-vs-cats/sample/"
sz=224
50/15: ls
50/16: os.listdir(PATH)
50/17:
PATH = "../.kaggle/competitions/dogs-vs-cats/"
sz=224
50/18: os.listdir(PATH)
50/19:
arch=resnet34
data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))
learn = ConvLearner.pretrained(arch, data, precompute=True)
learn.fit(0.01, 2)
50/20:
# !mkdir ../.kaggle/competitions/dogs-vs-cats/valid
!cp ../.kaggle/competitions/dogs-vs-cats/sample/ ../.kaggle/competitions/dogs-vs-cats/valid
50/21: ls
50/22: ls ../.kaggle/competitions/dogs-vs-cats/
50/23: ls ../.kaggle/competitions/dogs-vs-cats/sample/
50/24:
!mkdir ../.kaggle/competitions/dogs-vs-cats/valid
!cp ../.kaggle/competitions/dogs-vs-cats/sample/ ../.kaggle/competitions/dogs-vs-cats/valid
50/25: ls ../.kaggle/competitions/dogs-vs-cats/sample/
50/26: ls ../.kaggle/competitions/dogs-vs-cats/valid
50/27:
!mkdir ../.kaggle/competitions/dogs-vs-cats/valid
!cp -r ../.kaggle/competitions/dogs-vs-cats/sample/ ../.kaggle/competitions/dogs-vs-cats/valid
50/28: ls ../.kaggle/competitions/dogs-vs-cats/valid/
50/29:
!mkdir ../.kaggle/competitions/dogs-vs-cats/valid
!cp -r ../.kaggle/competitions/dogs-vs-cats/sample/* ../.kaggle/competitions/dogs-vs-cats/valid
50/30: ls ../.kaggle/competitions/dogs-vs-cats/valid/
50/31: rm -r ../.kaggle/competitions/dogs-vs-cats/valid/sample
50/32:
arch=resnet34
data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))
learn = ConvLearner.pretrained(arch, data, precompute=True)
learn.fit(0.01, 2)
64/1:
arch=resnet34
data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))
learn = ConvLearner.pretrained(arch, data, precompute=True)
learn.fit(0.01, 2)
64/2: cd ../fastai
64/3:
# This file contains all the main external libs we'll use
from fastai.imports import *
64/4:
from fastai.transforms import *
from fastai.conv_learner import *
from fastai.model import *
from fastai.dataset import *
from fastai.sgdr import *
from fastai.plots import *
64/5:
PATH = "../.kaggle/competitions/dogs-vs-cats/"
sz=224
64/6: os.listdir(PATH)
64/7:
arch=resnet34
data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))
learn = ConvLearner.pretrained(arch, data, precompute=True)
learn.fit(0.01, 2)
65/1: cd ../fastai
65/2:
# This file contains all the main external libs we'll use
from fastai.imports import *
65/3:
from fastai.transforms import *
from fastai.conv_learner import *
from fastai.model import *
from fastai.dataset import *
from fastai.sgdr import *
from fastai.plots import *
65/4:
PATH = "../.kaggle/competitions/dogs-vs-cats/"
sz=224
65/5: os.listdir(PATH)
65/6:
arch=resnet34
data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))
learn = ConvLearner.pretrained(arch, data, precompute=True)
learn.fit(0.01, 2)
66/1: cd ../fastai
66/2:
# This file contains all the main external libs we'll use
from fastai.imports import *
66/3:
from fastai.transforms import *
from fastai.conv_learner import *
from fastai.model import *
from fastai.dataset import *
from fastai.sgdr import *
from fastai.plots import *
66/4:
PATH = "../.kaggle/competitions/dogs-vs-cats/"
sz=224
66/5: os.listdir(PATH)
66/6:
arch=resnet34
data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))
learn = ConvLearner.pretrained(arch, data, precompute=True)
learn.fit(0.01, 2)
67/1:
arch=resnet34
data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))
learn = ConvLearner.pretrained(arch, data, precompute=True)
learn.fit(0.01, 2)
67/2: cd ../fastai
67/3:
# This file contains all the main external libs we'll use
from fastai.imports import *
67/4:
from fastai.transforms import *
from fastai.conv_learner import *
from fastai.model import *
from fastai.dataset import *
from fastai.sgdr import *
from fastai.plots import *
67/5:
PATH = "../.kaggle/competitions/dogs-vs-cats/"
sz=224
67/6: os.listdir(PATH)
67/7:
arch=resnet34
data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))
learn = ConvLearner.pretrained(arch, data, precompute=True)
learn.fit(0.01, 2)
49/1:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages/')
49/2:
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications import MobileNet
from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm
import gc
import cv2, os
49/3:
import numpy as np
import tensorflow as tf
import random as rn

# The below is necessary in Python 3.2.3 onwards to
# have reproducible behavior for certain hash-based operations.
# See these references for further details:
# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED
# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926

import os
os.environ['PYTHONHASHSEED'] = '0'

# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.

np.random.seed(42)

# The below is necessary for starting core Python generated random numbers
# in a well-defined state.

rn.seed(12345)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of
# non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res

session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

from keras import backend as K

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed

tf.set_random_seed(1234)

sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)
49/4:
TRAIN_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/train/'
TEST_DIR = '../ubuntu/.kaggle/competitions/dogs-vs-cats/test/'
49/5:
img = plt.imread(f'{TRAIN_DIR}dogs/{os.listdir(TRAIN_DIR+"dogs/")[0]}')
plt.imshow(img);
49/6: ls
49/7: pwd
49/8:
TRAIN_DIR = '../../ubuntu/.kaggle/competitions/dogs-vs-cats/train/'
TEST_DIR = '../../ubuntu/.kaggle/competitions/dogs-vs-cats/test/'
49/9:
# import shutil
 
# def move(file, src="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/", dest="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/"):
#     if file[0] == 'd':
#         shutil.move(src+file, dest+'dogs')
#     else:
#          shutil.move(src+file, dest+'cats')
49/10:
# for file in tqdm(files_train):
#     move(file)
49/11:
img = plt.imread(f'{TRAIN_DIR}dogs/{os.listdir(TRAIN_DIR+"dogs/")[0]}')
plt.imshow(img);
49/12: pwd
49/13:
ROWS = 64
COLS = 64
CHANNELS = 3

train_dogs = os.listdir(TRAIN_DIR+'dogs')
train_cats = os.listdir(TRAIN_DIR+'cats')
# train_images = train_dogs.extend(train_cats)

img_width, img_height = (128, 128)
epochs = 10
batch_size = 128

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)
49/14: Mobile_model = MobileNet(include_top=False, input_shape=input_shape)
49/15:
!pip install h5py
Mobile_model = MobileNet(include_top=False, input_shape=input_shape)
49/16: !pip show h5py
49/17:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages/')
path.append('/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages')
49/18: Mobile_model = MobileNet(include_top=False, input_shape=input_shape)
49/19:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages/')
path.append('/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages')
import h5py
49/20: Mobile_model = MobileNet(include_top=False, input_shape=input_shape)
49/21: print(h5py)
49/22: Mobile_model = MobileNet(include_top=False, input_shape=input_shape)
49/23:
!pip show h5py
!pip3 install h5py
!conda install h5py
49/24: Mobile_model = MobileNet(include_top=False, input_shape=input_shape)
69/1:
from sys import path
path.append('/home/ubuntu/.local/lib/python3.5/site-packages/')
path.append('/home/ubuntu/anaconda3/envs/fastai/lib/python3.6/site-packages')
69/2:
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.applications import MobileNet
from keras import optimizers
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import Model
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn.model_selection import train_test_split
import pandas as pd
from tqdm import tqdm
import gc
import cv2, os
69/3:
import numpy as np
import tensorflow as tf
import random as rn

# The below is necessary in Python 3.2.3 onwards to
# have reproducible behavior for certain hash-based operations.
# See these references for further details:
# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED
# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926

import os
os.environ['PYTHONHASHSEED'] = '0'

# The below is necessary for starting Numpy generated random numbers
# in a well-defined initial state.

np.random.seed(42)

# The below is necessary for starting core Python generated random numbers
# in a well-defined state.

rn.seed(12345)

# Force TensorFlow to use single thread.
# Multiple threads are a potential source of
# non-reproducible results.
# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res

session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

from keras import backend as K

# The below tf.set_random_seed() will make random number generation
# in the TensorFlow backend have a well-defined initial state.
# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed

tf.set_random_seed(1234)

sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)
69/4:
TRAIN_DIR = '../../ubuntu/.kaggle/competitions/dogs-vs-cats/train/'
TEST_DIR = '../../ubuntu/.kaggle/competitions/dogs-vs-cats/test/'
69/5:
# import shutil
 
# def move(file, src="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/", dest="/home/ubuntu/.kaggle/competitions/dogs-vs-cats/train/"):
#     if file[0] == 'd':
#         shutil.move(src+file, dest+'dogs')
#     else:
#          shutil.move(src+file, dest+'cats')
69/6:
# for file in tqdm(files_train):
#     move(file)
69/7:
img = plt.imread(f'{TRAIN_DIR}dogs/{os.listdir(TRAIN_DIR+"dogs/")[0]}')
plt.imshow(img);
69/8:
ROWS = 64
COLS = 64
CHANNELS = 3

train_dogs = os.listdir(TRAIN_DIR+'dogs')
train_cats = os.listdir(TRAIN_DIR+'cats')
# train_images = train_dogs.extend(train_cats)

img_width, img_height = (128, 128)
epochs = 10
batch_size = 128

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)
69/9: Mobile_model = MobileNet(include_top=False, input_shape=input_shape)
71/1: import numpy as np
71/2:
!pip3 install tqdm
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
from google.colab import files
from tqdm import *
courses = pd.read_csv('courses_all.csv')
career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
71/3:
!pip3 install tqdm
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('courses_all.csv')
career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
71/4:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('courses_all.csv')
career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
71/5:
indx = 0
cnt = -1
for header_course in tqdm(courses[str(2)][:300]):
    cnt += 1
    try:
        num = int(np.ceil(int(courses[str(1)][cnt])/10))
    except:
        pass
#     print(f'num = {num}')
    for page_num in tqdm(range(num)):
        link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
#         print(link)
        page = rq.get(link)
        course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://engineering.careers360.com' + course_link.a['href']
#             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None
            
            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass
            
            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None
            
            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
71/6:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('courses_all.csv')
71/7:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[str(2)][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[str(1)][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://engineering.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_{start}-{end}.csv')
71/8:
def threader():
  t1 = Thread(target=scrapper, args = (1,3))
  t2 = Thread(target=scrapper, args = (4, 6))
  t1.start()
  t2.start()
  t1.join()
  t2.join()
  print('done')
71/9:
def threader():
    t1 = Thread(target=scrapper, args = (0,600))
    t2 = Thread(target=scrapper, args = (1500, 2000))
    t3 = Thread(target=scrapper, args = (2000,2500))
    t4 = Thread(target=scrapper, args = (2500,3000))
    t1.start()
    t2.start()
    t3.start()
    t4.start()
    t1.join()
    t2.join()
    t3.join()
    t4.join()
    print('done')
71/10: threader()
71/11:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
from threading import Thread
#from google.colab import files
from tqdm import *
courses = pd.read_csv('courses_all.csv')
71/12:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[str(2)][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[str(1)][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://engineering.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_{start}-{end}.csv')
71/13:
def threader():
    t1 = Thread(target=scrapper, args = (0,600))
    t2 = Thread(target=scrapper, args = (1500, 2000))
    t3 = Thread(target=scrapper, args = (2000,2500))
    t4 = Thread(target=scrapper, args = (2500,3000))
    t1.start()
    t2.start()
    t3.start()
    t4.start()
    t1.join()
    t2.join()
    t3.join()
    t4.join()
    print('done')
71/14: threader()
72/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
from threading import Thread
#from google.colab import files
from tqdm import *
courses = pd.read_csv('courses_all.csv')
72/2:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[str(2)][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[str(1)][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://engineering.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_{start}-{end}.csv')
72/3: courses.shape
72/4:
def threader():
    t1 = Thread(target=scrapper, args = (3000,3600))
    t2 = Thread(target=scrapper, args = (3600, 4000))
    t3 = Thread(target=scrapper, args = (4000,4500))
    t4 = Thread(target=scrapper, args = (4500,5000))
    t5 = Thread(target=scrapper, args = (5000,5500))
    t6 = Thread(target=scrapper, args = (5500,5900))
    t7 = Thread(target=scrapper, args = (5900,6371))
    t1.start()
    t2.start()
    t3.start()
    t4.start()
    t5.start()
    t6.start()
    t7.start()
    t1.join()
    t2.join()
    t3.join()
    t4.join()
    t5.join()
    t6.join()
    t7.join()
    print('done')
72/5: threader()
73/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('courses_all.csv')
career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
73/2: eng_list = []
73/3:
for i in tqdm(range(82)):
    page = rq.get('https://medicine.careers360.com/colleges/list-of-medicine-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals[16:]:
        eng_list.append(val.a['href'])
73/4: len(eng_list)
73/5: eng_list[1:10]
73/6:
for i in tqdm(range(82)):
    page = rq.get('https://medicine.careers360.com/colleges/list-of-medicine-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        eng_list.append(val.a['href'])
73/7: eng_list = []
73/8:
page = rq.get('https://medicine.careers360.com/colleges/list-of-medicine-colleges-in-India?page='+str(0))
soup = bs(page.content,'html.parser')
vals = soup.find_all('div',class_='title')
vals
73/9:
page = rq.get('https://medicine.careers360.com/colleges/list-of-medicine-colleges-in-India?page='+str(1))
soup = bs(page.content,'html.parser')
vals = soup.find_all('div',class_='title')
vals
73/10:
for i in tqdm(range(82)):
    page = rq.get('https://medicine.careers360.com/colleges/list-of-medicine-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
73/11: len(eng_list)
73/12: courses = pd.DataFrame(columns=[0,1,2])
73/13:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://engineering.careers360.com'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
71/15: career360_courses.shape
74/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from threading import Thread
from tqdm import *
courses = pd.read_csv('courses_all.csv')
career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
74/2:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from threading import Thread
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
74/3: eng_list = []
74/4:
page = rq.get('https://pharmacy.careers360.com/colleges/list-of-pharmacy-colleges-in-India?page='+str(i))
soup = bs(page.content,'html.parser')
vals = soup.find_all('div',class_='title')
vals
74/5:
page = rq.get('https://pharmacy.careers360.com/colleges/list-of-pharmacy-colleges-in-India?page='+str(0))
soup = bs(page.content,'html.parser')
vals = soup.find_all('div',class_='title')
vals
74/6:
for i in tqdm(range(43)):
    page = rq.get('https://pharmacy.careers360.com/colleges/list-of-pharmacy-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
74/7: len(eng_list)
74/8: courses = pd.DataFrame(columns=[0,1,2])
71/16:
def threader():
    t1 = Thread(target=scrapper, args = (0,150))
    t2 = Thread(target=scrapper, args = (150, 300))
    t3 = Thread(target=scrapper, args = (300,450))
    t4 = Thread(target=scrapper, args = (450,600))
    t1.start()
    t2.start()
    t3.start()
    t4.start()
    t1.join()
    t2.join()
    t3.join()
    t4.join()
    print('done')
71/17: threader()
72/6:
def threader():
    t1 = Thread(target=scrapper, args = (3000,3100))
    t2 = Thread(target=scrapper, args = (3100,3300))
    t3 = Thread(target=scrapper, args = (4000,4200))
    t4 = Thread(target=scrapper, args = (3300,3600))
    t5 = Thread(target=scrapper, args = (4200,4400))
    t6 = Thread(target=scrapper, args = (4400,4500))
    t1.start()
    t2.start()
    t3.start()
    t4.start()
    t5.start()
    t6.start()
    t1.join()
    t2.join()
    t3.join()
    t4.join()
    t5.join()
    t6.join()
    print('done')
72/7: threader()
74/9: courses = pd.DataFrame(columns=[0,1,2])
74/10:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://engineering.careers360.com'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
73/14: courses.head()
73/15: courses[2][0]
73/16:
string = courses[2][0]
string[:30]
73/17:
string = courses[2][0]
string[:49]
73/18:
string = courses[2][0]
string[:40]
73/19:
string = courses[2][0]
string[:43]
73/20:
string = courses[2][0]
string[:39]
73/21:
string = courses[2][0]
string[:36]
73/22:
string = courses[2][0]
string[:35]
73/23:
string = courses[2][0]
string[:35] = 'https://medicine.careers360.com/'
string
73/24:
string = courses[2][0]
string[35:]
73/25:
string = courses[2][0]
string2 = string[35:] + 'https://medicine.careers360.com/'
string2
73/26:
string = courses[2][0]
string2 = 'https://medicine.careers360.com/' string[35:] + 
string2
73/27:
string = courses[2][0]
string2 = 'https://medicine.careers360.com/' + string[35:]
string2
73/28:
tcourses = courses
def subs(string):
    string2 = 'https://medicine.careers360.com/' + string[35:]
    return string2
tcourses[2].apply(subs)
tcourses.head()
73/29:
tcourses = courses
def subs(string):
    string2 = 'https://medicine.careers360.com/' + string[35:]
    return string2
tcourses[2].apply(subs)
73/30:
tcourses = courses
def subs(string):
    string2 = 'https://medicine.careers360.com/' + string[35:]
    return string2
tcourses[2] = tcourses[2].apply(subs)
tcourses.head()
73/31: tcourses[2][0]
73/32:
def subs(string):
    string2 = 'https://medicine.careers360.com/' + string[35:]
    return string2
courses[2] = courses[2].apply(subs)
courses.head()
73/33: courses[2][0]
73/34:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://engineering.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_medical({start}-{end}).csv')
73/35:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(19):
        thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
        threads.append(thread)
        thread.start()
    thread = Thread(target=scrapper, args = (19*numer,siz)
    threads.append(thread)
    thread.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/36:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(19):
        thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
        threads.append(thread)
        thread.start()
    threadd = Thread(target=scrapper, args = (19*numer,siz)
    threads.append(threadd)
    threadd.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/37:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz)
            threads.append(thread)
            thread.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/38:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz)
            thread.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/39:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz)
                            
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/40:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz)
            
                    
    print('done')
73/41:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz)
73/42:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz)
73/43:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz)
73/44:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz)
            threads.append(thread)
            thread.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/45:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    
                    
    print('done')
73/46:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
                           
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/47:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19                     
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/48:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:                  
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/49:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()               
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/50:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/51: threader()
71/18:
def threader():
    t1 = Thread(target=scrapper, args = (300,350))
    t2 = Thread(target=scrapper, args = (350, 400))
    t3 = Thread(target=scrapper, args = (400,450))
    t1.start()
    t2.start()
    t3.start()
    t1.join()
    t2.join()
    t3.join()
    print('done')
71/19: threader()
73/52:
import time
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
        time.sleep(200)
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/53: threader()
73/54: threader()
73/55: courses.to_csv('medical_courses.csv')
73/56:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
73/57: threader()
74/11:
tcourses = courses
def subs(string):
    string2 = 'https://pharmacy.careers360.com/' + string[35:]
    return string2
subs(tcourses[2][0])
74/12:
tcourses = courses
def subs(string):
    string2 = 'https://pharmacy.careers360.com/' + string[35:]
    return string2
subs(tcourses[2][1])
74/13:
def subs(string):
    string2 = 'https://pharmacy.careers360.com/' + string[35:]
    return string2
courses[2] = courses[2].apply(subs)
courses.head()
74/14: courses.to_csv('pharmacy_courses.csv')
74/15: courses.to_csv('pharmacy_courses.csv')
74/16: courses.shape
73/58: scrapper(1,4)
73/59: scrapper(1,2)
73/60:
from threading import Thread
def scrapper(start, end):
    print('kunal')
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        print(header_course)
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://engineering.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_medical({start}-{end}).csv')
73/61: scrapper(1,2)
74/17: courses.head()
74/18: courses[2][2]
74/19:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://engineering.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_pharmacy({start}-{end}).csv')
74/20:
siz = len(eng_list)
numer = siz/20
prev = 0
def threader():
    threads = []
    for i in range(19):
        thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
        threads.append(thread)
        thread.start()
    thread = Thread(target=scrapper, args = (19*numer,siz))
    threads.append(thread)
    thread.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
74/21: threader()
74/22:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(19):
        thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
        threads.append(thread)
        thread.start()
    thread = Thread(target=scrapper, args = (19*numer,siz))
    threads.append(thread)
    thread.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
74/23: threader()
72/8:
def threader():
    t3 = Thread(target=scrapper, args = (4000,4100))
    t4 = Thread(target=scrapper, args = (4300,4400))
    t5 = Thread(target=scrapper, args = (4100,4200))
    t6 = Thread(target=scrapper, args = (4200,4300))
    t3.start()
    t4.start()
    t5.start()
    t6.start()
    t3.join()
    t4.join()
    t5.join()
    t6.join()
    print('done')
72/9: threader()
73/62:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
73/63:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
73/64: eng_list = []
73/65:
courses = pd.read_csv('medical_courses.csv')
courses.head()
72/10:
def threader():
    t3 = Thread(target=scrapper, args = (4000,4100))
#     t4 = Thread(target=scrapper, args = (4300,4400))
#     t5 = Thread(target=scrapper, args = (4100,4200))
#     t6 = Thread(target=scrapper, args = (4200,4300))
     t3.start()
#     t4.start()
#     t5.start()
#     t6.start()
     t3.join()
#     t4.join()
#     t5.join()
#     t6.join()
    print('done')
72/11:
def threader():
    t3 = Thread(target=scrapper, args = (4000,4100))
#     t4 = Thread(target=scrapper, args = (4300,4400))
#     t5 = Thread(target=scrapper, args = (4100,4200))
#     t6 = Thread(target=scrapper, args = (4200,4300))
    t3.start()
#     t4.start()
#     t5.start()
#     t6.start()
    t3.join()
#     t4.join()
#     t5.join()
#     t6.join()
    print('done')
72/12: threader()
72/13: scrapper(4000,4100)
72/14:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[str(2)][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[str(1)][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://engineering.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_{start}-{end}.csv')
72/15: scrapper(4000,4100)
72/16: scrapper(4000,4100)
74/24:
def threader():
    t3 = Thread(target=scrapper, args = (0,100))
    t4 = Thread(target=scrapper, args = (100,200))
    t5 = Thread(target=scrapper, args = (200,300))
    t6 = Thread(target=scrapper, args = (300,400))
    t3.start()
    t4.start()
    t5.start()
    t6.start()
    t3.join()
    t4.join()
    t5.join()
    t6.join()
    print('done')
74/25: threader()
73/66:
idx = 0
for link in tqdm(courses[0]):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://medicine.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
73/67:
idx = 0
for link in tqdm(courses['0']):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://medicine.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
74/26: courses[0]
74/27: courses.head()
74/28: courses[2][0]
74/29: courses.head()
74/30: courses[2][1]
74/31: courses.head()
73/68: courses.head()
73/69: courses.head()
73/70:
idx = 0
for link in tqdm(courses['0'][:10]):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        tcourses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://medicine.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
73/71: tcourses.head()
73/72:
idx = 0
for link in tqdm(courses['0']):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://medicine.careers360.com'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
74/32: scrapper(0,2)
74/33:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        print(header_course)
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://engineering.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_pharmacy({start}-{end}).csv')
74/34: scrapper(0,2)
74/35: scrapper(0,3)
73/73: courses.head()
74/36:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        print(header_course)
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            pass
        print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://engineering.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_pharmacy({start}-{end}).csv')
74/37: scrapper(0,3)
74/38:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        print(header_course)
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://engineering.careers360.com' + course_link.a['href']
            print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_pharmacy({start}-{end}).csv')
74/39: scrapper(0,3)
74/40:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        print(header_course)
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://pharmacy.careers360.com/' + course_link.a['href']
            print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_pharmacy({start}-{end}).csv')
74/41: scrapper(0,3)
74/42:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        print(header_course)
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://pharmacy.careers360.com/' + course_link.a['href']
            print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_pharmacy({start}-{end}).csv')
74/43:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(19):
        thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
        threads.append(thread)
        thread.start()
    thread = Thread(target=scrapper, args = (19*numer,siz))
    threads.append(thread)
    thread.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
74/44: threader()
73/74: courses.head()
76/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from threading import Thread
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
76/2: courses = pd.read_csv('pharmacy_courses.csv')
76/3:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        print(header_course)
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://pharmacy.careers360.com/' + course_link.a['href']
            print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_pharmacy({start}-{end}).csv')
76/4:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(19):
        thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
        threads.append(thread)
        thread.start()
    thread = Thread(target=scrapper, args = (19*numer,siz))
    threads.append(thread)
    thread.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
76/5:
siz = len(courses.shape[0])
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(19):
        thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
        threads.append(thread)
        thread.start()
    thread = Thread(target=scrapper, args = (19*numer,siz))
    threads.append(thread)
    thread.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
76/6:
siz = (courses.shape[0])
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(19):
        thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
        threads.append(thread)
        thread.start()
    thread = Thread(target=scrapper, args = (19*numer,siz))
    threads.append(thread)
    thread.start()                
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
76/7: threader()
76/8:
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        print(header_course)
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://pharmacy.careers360.com/' + course_link.a['href']
            print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_pharmacy({start}-{end}).csv')
76/9: threader()
77/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
77/2: pd.read_csv('medical_courses.csv')
77/3: courses1 = pd.read_csv('medical_courses.csv')
77/4: courses = pd.DataFrame(columns=[0,1,2])
77/5:
idx = 0
for link in tqdm(courses1['0']):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://medicine.careers360.com'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
78/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
78/2: eng_list = []
78/3:
for i in tqdm(range(213)):
    page = rq.get('https://bschool.careers360.com/colleges/list-of-mba-colleges-in-india?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
79/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
79/2: eng_list = []
79/3:
for i in tqdm(range(213)):
    page = rq.get('https://bschool.careers360.com/colleges/list-of-mba-colleges-in-india?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
79/4: eng_list = []
78/4: courses = pd.DataFrame(columns=[0,1,2])
78/5:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
79/5:
for i in tqdm(range(34)):
    page = rq.get('https://law.careers360.com/colleges/list-of-law-colleges-in-india?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
77/6: courses.to_csv('medical_courses.csv')
77/7:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://medicine.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_medical({start}-{end}).csv')
77/8:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
80/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
80/2: eng_list = []
80/3:
for i in tqdm(range(34)):
    page = rq.get('https://law.careers360.com/colleges/list-of-law-colleges-in-india?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
80/4: courses = pd.DataFrame(columns=[0,1,2])
80/5:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://law.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
77/9: courses.head()
77/10:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            pass
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://medicine.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_medical({start}-{end}).csv')
77/11:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
77/12: threader()
77/13:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://medicine.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_medical({start}-{end}).csv')
77/14: threader()
82/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
82/2: eng_list = []
82/3:
for i in tqdm(range(34)):
    page = rq.get('https://law.careers360.com/colleges/list-of-law-colleges-in-india?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
82/4: courses = pd.DataFrame(columns=[0,1,2])
82/5:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://law.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
84/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
84/2: eng_list = []
84/3:
for i in tqdm(range(213)):
    page = rq.get('https://bschool.careers360.com/colleges/list-of-mba-colleges-in-india?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
87/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
87/2: eng_list = []
87/3:
for i in tqdm(range(34)):
    page = rq.get('https://law.careers360.com/colleges/list-of-law-colleges-in-india?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
87/4: courses = pd.DataFrame(columns=[0,1,2])
87/5:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://law.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
89/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
89/2: courses1 = pd.read_csv('medical_courses.csv')
89/3: courses = pd.read_csv('medical_courses.csv')
89/4: courses.head()
89/5:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://medicine.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_medical({start}-{end}).csv')
89/6:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
89/7:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://medicine.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_medical({start}-{end}).csv')
89/8:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
89/9: threader()
88/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
88/2: eng_list = []
88/3:
for i in tqdm(range(14)):
    page = rq.get('https://design.careers360.com/colleges/list-of-animation-fashion-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
88/4: courses = pd.DataFrame(columns=[0,1,2])
88/5:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://design.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
90/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
90/2: eng_list = []
90/3:
for i in tqdm(range(14)):
    page = rq.get('https://design.careers360.com/colleges/list-of-animation-fashion-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
90/4:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
90/5:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
90/6: eng_list = []
90/7:
for i in tqdm(range(20)):
    page = rq.get('https://media.careers360.com/colleges/list-of-media-journalism-colleges-in-India?page=20"/colleges/list-of-animation-fashion-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
90/8: courses = pd.DataFrame(columns=[0,1,2])
90/9:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://media.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
90/10: courses.to_csv('media_courses.csv')
90/11:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://media.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_law({start}-{end}).csv')
90/12:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
90/13: len(eng_list)
90/14:
for i in tqdm(range(20)):
    page = rq.get('https://media.careers360.com/colleges/list-of-media-journalism-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
90/15: courses = pd.DataFrame(columns=[0,1,2])
90/16:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://media.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
91/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
91/2: eng_list = []
91/3:
for i in tqdm(range(213)):
    page = rq.get('https://bschool.careers360.com/colleges/list-of-mba-colleges-in-india?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
88/6: courses.to_csv('design_courses.csv')
88/7:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://law.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_law({start}-{end}).csv')
88/8:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
88/9: threader()
88/10: courses = pd.read_csv('design_courses.csv')
88/11:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://law.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_law({start}-{end}).csv')
88/12:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
88/13: threader()
88/14:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://law.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_law({start}-{end}).csv')
88/15:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
88/16: threader()
89/10: scrapper(0-122)
89/11: scrapper(0,122)
89/12: scrapper(0,122)
91/4: courses = pd.DataFrame(columns=[0,1,2])
91/5:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
89/13: indx
89/14: scrapper(244,488)
87/6:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://law.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_law({start}-{end}).csv')
87/7:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
87/8: threader()
92/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
93/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
93/2: courses = pd.read_csv('medical_courses.csv')
93/3:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://medicine.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_medical({start}-{end}).csv')
93/4:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
93/5: scrapper(244,488)
92/2: scrapper(0,21)
92/3:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
92/4: courses = pd.read_csv('design_courses.csv')
92/5:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://law.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_law({start}-{end}).csv')
92/6:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://law.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_law({start}-{end}).csv')
92/7:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
92/8:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
92/9: scrapper(0,21)
94/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
94/2: eng_list = []
94/3:
for i in tqdm(range(213)):
    page = rq.get('https://bschool.careers360.com/colleges/list-of-mba-colleges-in-india?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
92/10:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://design.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_law({start}-{end}).csv')
92/11:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
92/12: threader()
92/13: scrapper(200,220)
92/14: scrapper(200,220)
96/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
96/2: eng_list = []
96/3:
for i in tqdm(range(213)):
    page = rq.get('https://bschool.careers360.com/colleges/list-of-mba-colleges-in-india?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
97/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
97/2: eng_list = []
97/3:
for i in tqdm(range(20)):
    page = rq.get('https://media.careers360.com/colleges/list-of-media-journalism-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
97/4: courses = pd.DataFrame(columns=[0,1,2])
97/5:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://media.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        pass
96/4:
for i in tqdm(range(213)):
    try:
        page = rq.get('https://bschool.careers360.com/colleges/list-of-mba-colleges-in-india?page='+str(i))
    except:
        print(f'error in {i}')
        pass
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
96/5: len(eng_list)
96/6: set(eng_list)
96/7: len(set(eng_list))
96/8: list(set(eng_list))
96/9: len(list(set(eng_list)))
96/10: eng_list = list(set(eng_list))
96/11: eng_list.shape
96/12: len(eng_list)
96/13:
for i in tqdm({73,75,101,170}):
    try:
        page = rq.get('https://bschool.careers360.com/colleges/list-of-mba-colleges-in-india?page='+str(i))
    except:
        print(f'error in {i}')
        pass
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
96/14: len(eng_list)
96/15: len(list(set(eng_list)))
96/16: len(eng_list)
96/17:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {link}')
        pass
97/6: courses.to_csv('media_courses.csv')
97/7:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://media.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_law({start}-{end}).csv')
97/8:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
97/9: threader()
97/10: scrapper(0,29)
98/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
98/2: eng_list = []
98/3:
for i in tqdm(range(213)):
    try:
        page = rq.get('https://bschool.careers360.com/colleges/list-of-mba-colleges-in-india?page='+str(i))
    except:
        print(f'error in {i}')
        pass
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
98/4: (pd.DataFrame(eng_list)).to_csv('mang_list.csv')
98/5: courses = pd.DataFrame(columns=[0,1,2])
98/6:
idx = 0
for link in tqdm(eng_list[:10]):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {link}')
        pass
98/7: courses.head()
98/8:
idx = 0
for link in tqdm(eng_list[:100]):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {link}')
        pass
98/9: courses.shape
98/10: courses.head()
98/11:
idx = 100
for link in tqdm(eng_list[100:]):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {link}')
        pass
99/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
99/2: eng_list = []
99/3:
for i in tqdm(range(34)):
    page = rq.get('https://law.careers360.com/colleges/list-of-law-colleges-in-india?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
99/4: pd.DataFrame(eng_list).to_csv('mang_list.csv')
99/5: courses = pd.DataFrame(columns=[0,1,2])
99/6:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://law.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {idx} and {link}')
        pass
99/7: courses.shape
99/8: courses.to_csv('law_courses.csv')
99/9:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://law.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_law({start}-{end}).csv')
99/10:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
99/11: threader()
99/12: scrapper(50,100)
100/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
100/2: eng_list = []
100/3:
for i in tqdm(range(19)):
    page = rq.get('https://hospitality.careers360.com/colleges/list-of-hospitality-tourism-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
100/4: pd.DataFrame(eng_list).to_csv('hotel_list.csv')
100/5: courses = pd.DataFrame(columns=[0,1,2])
100/6:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://hospitality.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {idx} and {link}')
        pass
102/1: courses = pd.read_csv('medical_courses.csv')
102/2:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
102/3: courses = pd.read_csv('medical_courses.csv')
100/7: courses.to_csv('hotel_courses.csv')
100/8:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://hospitality.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_hotel({start}-{end}).csv')
100/9:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
100/10: threader()
103/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
103/2: courses = pd.read_csv('medical_courses.csv')
103/3: courses.head()
103/4: scrapper(0,122)
103/5:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://medicine.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_medical({start}-{end}).csv')
103/6: scrapper(0,122)
104/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
104/2: eng_list = list(pd.read_csv('mang_list.csv'))
104/3: courses = pd.DataFrame(columns=[0,1,2])
104/4:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {link}')
        pass
104/5: eng_list
104/6: pd.read_csv('mang_list.csv')
104/7: (pd.read_csv('mang_list.csv')[0])
104/8: (pd.read_csv('mang_list.csv')['0'])
104/9: list(pd.read_csv('mang_list.csv')['0'])
104/10: eng_list = list(pd.read_csv('mang_list.csv')['0'])
104/11: courses = pd.DataFrame(columns=[0,1,2])
104/12:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {link}')
        pass
105/1: courses = pd.read_csv('hotel_course.csv')
105/2:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
105/3: courses = pd.read_csv('hotel_course.csv')
105/4: courses = pd.read_csv('hotel_courses.csv')
105/5:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://hospitality.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_hotel({start}-{end}).csv')
105/6:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
105/7:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
105/8: threader()
105/9:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://hospitality.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_hotel({start}-{end}).csv')
105/10:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
105/11: threader()
106/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
106/2: eng_list = []
103/7: scrapper(244,488)
106/3:
for i in tqdm(range(139)):
    page = rq.get('https://it.careers360.com/colleges/list-of-BCA-MCA-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
107/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
107/2: courses = pd.read_csv('medical_courses.csv')
107/3: courses.head()
109/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
107/4:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
107/5: scrapper(244,488)
107/6:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://medicine.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_medical({start}-{end}).csv')
107/7: scrapper(0,122)
108/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
108/2: eng_list = []
108/3:
for i in tqdm(range(139)):
    page = rq.get('https://it.careers360.com/colleges/list-of-BCA-MCA-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
109/2:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
109/3: eng_list = list(pd.read_csv('mang_list.csv')['0'])
109/4:
for i in tqdm(range(213)):
    try:
        page = rq.get('https://bschool.careers360.com/colleges/list-of-mba-colleges-in-india?page='+str(i))
    except:
        print(f'error in {i}')
        pass
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
111/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
111/2: eng_list = list(pd.read_csv('mang_list.csv')['0'])
111/3:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
111/4: eng_list = list(pd.read_csv('mang_list.csv')['0'])
111/5: courses = pd.DataFrame(columns=[0,1,2])
111/6: eng_list
111/7:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {link}')
        pass
112/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
112/2: courses = pd.read_csv('medical_courses.csv')
112/3:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://medicine.careers360.com' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_medical({start}-{end}).csv')
112/4:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
112/5: scrapper(244,488)
113/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
113/2: eng_list = []
113/3:
for i in tqdm(range(139)):
    page = rq.get('https://it.careers360.com/colleges/list-of-BCA-MCA-colleges-in-India?page='+str(i))
    soup = bs(page.content,'html.parser')
    vals = soup.find_all('div',class_='title')
    for val in vals:
        try:
            eng_list.append(val.a['href'])
        except:
            pass
113/4: pd.DataFrame(eng_list).to_csv('it_list.csv')
113/5:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://it.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {idx} and {link}')
        pass
112/6: scrapper(732,976)
114/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
114/2: eng_list = []
114/3:
eng_list = list(pd.read_csv('it_list.csv'))
eng_list
114/4:
eng_list = list(pd.read_csv('it_list.csv')[0])
eng_list
114/5:
eng_list = list(pd.read_csv('it_list.csv')['0'])
eng_list
114/6:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://it.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {idx} and {link}')
        pass
115/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
115/2:
eng_list = list(pd.read_csv('it_list.csv')['0'])
eng_list
115/3:
idx = 0
for link in tqdm(eng_list):
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://it.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {idx} and {link}')
        pass
116/1:
idx = 0
for link in tqdm(eng_list):
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://it.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
116/2:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
116/3:
eng_list = list(pd.read_csv('it_list.csv')['0'])
eng_list
116/4:
idx = 0
for link in tqdm(eng_list):
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://it.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
116/5: courses = pd.DataFrame(columns=[0,1,2])
116/6:
idx = 0
for link in tqdm(eng_list):
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://it.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
116/7: courses.head()
116/8: courses = pd.DataFrame(columns=[0,1,2])
116/9:
idx = 0
for link in tqdm(eng_list):
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://it.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
112/7: scrapper(1098,1220)
112/8: scrapper(1464,1830)
112/9: scrapper(2074,2318)
116/10: indx
116/11: idx
116/12: courses.tail()
116/13: idx
116/14: courses.tail()
116/15:
idx = 3154
for link in tqdm(eng_list[3154:]):
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://it.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
111/8: courses.to_csv('mang_courses.csv')
111/9:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
111/10:
siz = len(eng_list)
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
111/11: threader()
111/12: idxs = {6051,6049,6048,6047,6045,6038,6037,6036,6033,5961,5960,5959,5958,5957,5955,5954,5952,5951,5949,5948,5944,5942,2525}
111/13: idx
111/14: courses.shape
116/16: courses.to_csv('it_courses.csv')
111/15: ttcourses = courses
111/16:
ttcourses = courses
tcourses = courses
111/17: tcourses.iloc[318:637]
111/18: courses = tcourses.iloc[318:637]
111/19:
courses = tcourses.iloc[318:637]
courses
111/20:
courses = tcourses.iloc[318:637]
courses.index
111/21: courses.index = range(courses.shape[0])
111/22: courses.head
111/23: courses.head()
111/24: courses = tcourses.iloc[318:637]
111/25: courses.head()
111/26: courses = ttcourses
116/17:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://it.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_it({start}-{end}).csv')
116/18:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
116/19: threader()
116/20:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://it.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_it({start}-{end}).csv')
116/21:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
116/22: threader()
111/27: scrapper(318,636)
117/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('mang_courses.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
117/2: scrapper(1272,1908)
117/3:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('mang_courses.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
117/4: scrapper(1272,1908)
118/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('mang_courses.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
118/2: scrapper(2544,3816)
121/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('mang_courses.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
121/2:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('mang_courses.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
121/3: scrapper(2544,3816)
123/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
123/2:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
123/3: courses = pd.read_csv('mang_courses.csv')
123/4: scrapper(318,636)
123/5: threader()
123/6:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
123/7: scrapper(318,636)
122/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('mang_courses.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
122/2: scrapper(1272,1908)
125/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
125/2: courses = pd.read_csv('it_courses.csv')
125/3:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses[2][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses[1][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://it.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_it({start}-{end}).csv')
125/4:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
125/5: threader()
125/6:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://it.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_it({start}-{end}).csv')
125/7:
siz = courses.shape[0]
numer = int(siz/20)
prev = 0
def threader():
    threads = []
    for i in range(20):
        if i!=19:
            thread = Thread(target=scrapper, args = (i*numer,(i+1)*numer))
            threads.append(thread)
            thread.start()
        else:
            thread = Thread(target=scrapper, args = (19*numer,siz))
            threads.append(thread)
            thread.start()        
    
    for thread in threads:  # iterates over the threads
        thread.join()
                    
    print('done')
125/8: threader()
123/8: scrapper(4134,4452)
125/9: scrapper(207,621)
125/10:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
125/11: courses=pd.read_csv('it_courses.csv')
125/12:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://it.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_it({start}-{end}).csv')
125/13: scrapper(207,621)
121/4:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('mang_courses.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
121/5: scrapper(2544,3216)
127/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('mang_courses.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
127/2: scrapper(2544,3216)
128/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
128/2: courses = pd.read_csv('it_courses.csv')
128/3:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://it.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_it({start}-{end}).csv')
128/4: scrapper(207,621)
128/5: scrapper(828,1500)
129/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
129/2: courses = pd.read_csv('it_courses.csv')
129/3:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://it.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_it({start}-{end}).csv')
129/4: scrapper(2484,2691)
130/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
130/2: courses = pd.read_csv('it_courses.csv')
130/3: courses.shape
130/4:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://it.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_it({start}-{end}).csv')
130/5: scrapper(3105,3600)
131/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
131/2: eng_list = list(pd.read_csv('mang_list.csv')['0'])
131/3: courses = pd.read_csv('mang_courses.csv')
131/4:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
131/5: courses.shape
131/6: courses.tail()
131/7: courses.tail()
131/8:
idxs = {6051,6049,6048,6047,6045,6038,6037,6036,6033,5961,5960,5959,5958,5957,5955,5954,5952,5951,5949,5948,5944,5942,2525}
idx = 6340
for ids in idxs:
    link = eng_list[ids]
    try:
        page = rq.get(link)
        soup = bs(page.content,'html.parser')
        courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
        idx+=1
    except:
        print(f'error in {link}')
        pass
131/9: courses.shape
131/10: link
131/11: page = rq.get(link)
131/12:
page = rq.get(link)
page
131/13:
page = rq.get(link)
soup = bs(page.content,'html.parser')
soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:]
131/14:
idxs = {6051,6049,6048,6047,6045,6038,6037,6036,6033,5961,5960,5959,5958,5957,5955,5954,5952,5951,5949,5948,5944,5942,2525}
idx = 6340
for ids in idxs:
    link = eng_list[ids]
    page = rq.get(link)
    soup = bs(page.content,'html.parser')
    courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
    idx+=1
131/15: courses.shape
131/16:
page = rq.get(link)
soup = bs(page.content,'html.parser')
soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']
131/17:
page = rq.get(eng_list[6051])
soup = bs(page.content,'html.parser')
soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']
131/18: courses.tail()
131/19: courses.drop(('Unnamed:0'),axis=1)
131/20: courses.drop(('Unnamed:0 '),axis=1)
131/21: courses.index
131/22: courses.columns
131/23: courses.drop(('Unnamed: 0'),axis=1)
131/24: courses = courses.drop(('Unnamed: 0'),axis=1)
131/25:
idxs = {6051,6049,6048,6047,6045,6038,6037,6036,6033,5961,5960,5959,5958,5957,5955,5954,5952,5951,5949,5948,5944,5942,2525}
idx = 6340
for ids in idxs:
    link = eng_list[ids]
    page = rq.get(link)
    soup = bs(page.content,'html.parser')
    courses.loc[idx]=[link,soup.find_all('h4',class_='blockSubHeading')[0].get_text()[-2:],'https://bschool.careers360.com/'+soup.find_all('ul',class_='list-tabs')[0].find_all('li')[1].a['href']]
    idx+=1
131/26: courses.shape
131/27: scrapper(6340,6363)
127/3: scrapper(3216,3816)
130/6: scrapper(3600,4145)
132/1:
from bs4 import BeautifulSoup as b
import requests as r
import pandas as pd
import numpy as np
import re
132/2: links = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
132/3: links = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
132/4:
res = r.get('https://medicine.careers360.com/colleges/madras-medical-college-chennai')
soup = b(res.content,'html.parser')
132/5: soup = soup.find_all('div',attrs={"class": "infoQuestion"})
132/6:
lists = soup.find_all('ul',class_='clg-info')
lsts = []
for lst in lists:
  for li in lst.findAll('li'):
    lsts.append(li)
lsts
132/7:
res = r.get('https://medicine.careers360.com/Medicine/Vardhman-Mahavir-Medical-College')
soup = b(res.content,'html.parser')
lists = soup.find_all('ul',class_='clg-info')
l1 = soup.find_all('ul',class_='clg-info')[0]
lsts = []
for li in l1.findAll('li'):
  lsts.append(li.get_text())
lsts
132/8: soup.find('div', class='contentBlockSec')
132/9: soup.find('div', attrs={class:'contentBlockSec'})
132/10: soup.find('div', attrs={class_:'contentBlockSec'})
132/11: soup.find('div', class_='contentBlockSec')
132/12:
facility = soup.find('div', id_ = 'facility')
soup.find('div', class_='contentBlockSec')
132/13:
print(soup.find('div', id_ = 'facility'))
soup.find('div', class_='contentBlockSec')
132/14:
print(soup.find('div', id = 'facility'))
soup.find('div', class_='contentBlockSec')
132/15:
facility = soup.find('div', id = 'facility')
facility.find('div', class_='contentBlockSec')
132/16:
facility = soup.find('div', id = 'facility')
facility.find('div', class_='contentBlockSec').get_text()
132/17:
facility = soup.find('div', id = 'facility')
facility.find('div', class_='contentBlockSec').get_text().split()
132/18:
facility = soup.find('div', id = 'facility')
facilities = facility.find('div', class_='contentBlockSec').get_text()
re.findall('[A-Z][^A-Z]*', facilities)
132/19:
facility = soup.find('div', id = 'facility')
facilities = facility.find('div', class_='contentBlockSec').get_text()
re.findall('^ [A-Z][^A-Z]*', facilities)
132/20:
facility = soup.find('div', id = 'facility')
facilities = facility.find('div', class_='contentBlockSec').get_text()
re.findall('^\ [A-Z][^A-Z]*', facilities)
132/21:
facility = soup.find('div', id = 'facility')
facilities = facility.find('div', class_='contentBlockSec').get_text()
re.findall(' [A-Z][^A-Z]*', facilities)
132/22:
facility = soup.find('div', id = 'facility')
facilities = facility.find('div', class_='contentBlockSec')['li']
# re.findall(' [A-Z][^A-Z]*', facilities)
132/23:
facility = soup.find('div', id = 'facility')
facilities = facility.find('div', class_='contentBlockSec')['ul']
# re.findall(' [A-Z][^A-Z]*', facilities)
132/24:
facility = soup.find('div', id = 'facility')
facilities = facility.find('div', class_='contentBlockSec')
# re.findall(' [A-Z][^A-Z]*', facilities)
132/25:
facility = soup.find('div', id = 'facility')
facility.find('div', class_='contentBlockSec')
# re.findall(' [A-Z][^A-Z]*', facilities)
132/26:
soup.find('div', id = 'facilitylist')
# re.findall(' [A-Z][^A-Z]*', facilities)
132/27:
soup.find('div', class_ = 'facilitylist')
# re.findall(' [A-Z][^A-Z]*', facilities)
132/28:
soup.find('div', class_ = 'facilitylist')['ul']
# re.findall(' [A-Z][^A-Z]*', facilities)
132/29:
soup.find('div', class_ = 'facilitylist').gettext()
# re.findall(' [A-Z][^A-Z]*', facilities)
132/30:
soup.find('div', class_ = 'facilitylist').get_text()
# re.findall(' [A-Z][^A-Z]*', facilities)
132/31:
soup.find('div', class_ = 'facilitylist')
# re.findall(' [A-Z][^A-Z]*', facilities)
132/32:
facility_list = soup.find('div', class_ = 'facilitylist')
facility_list.find('ul')
# re.findall(' [A-Z][^A-Z]*', facilities)
132/33:
facility_list = soup.find('div', class_ = 'facilitylist')
facility_list.find('ul')
for li in facility_list.find_all('li'):
    print(li.get_text())
# re.findall(' [A-Z][^A-Z]*', facilities)
132/34:
facilities = []
facility_list = soup.find('div', class_ = 'facilitylist')
facility_list.find('ul')
for li in facility_list.find_all('li'):
    facilities.append(li)
facilities
# re.findall(' [A-Z][^A-Z]*', facilities)
132/35:
facilities = []
facility_list = soup.find('div', class_ = 'facilitylist')
facility_list.find('ul')
for li in facility_list.find_all('li'):
    facilities.append(li.get_text())
facilities
# re.findall(' [A-Z][^A-Z]*', facilities)
132/36: soup.find_all('id', attrs={id = 'mCSB_1_container'})
132/37: soup.find_all('id="mCSB_1_container"')
132/38: soup.find_all(id="mCSB_1_container")
132/39: soup.find_all(class_='college-admission-table')
132/40: soup.find_all(class_='college-admission-table')['table']
132/41: soup.find_all(class_='college-admission-table')[0]
132/42: soup.find_all(class_='college-admission-table')[0]['table']
132/43: soup.find_all(class_='college-admission-table')[0]
132/44: dict(soup.find_all(class_='college-admission-table')[0])
132/45: soup.find_all(class_='college-admission-table')[0]
132/46: soup.find_all(class_='college-admission-table')[0].table
132/47: soup.find_all(class_='college-admission-table')[0].table.tbody
132/48: soup.find_all(class_='college-admission-table')[0].table.tbody[0]
132/49: soup.find_all(class_='college-admission-table')[0].table.tbody
132/50:
ex = soup.find_all(class_='college-admission-table')[0].table.tbody
exams = []
for tr in ex.find_all('tr'):
    exams.append(tr.get_text())
132/51:
ex = soup.find_all(class_='college-admission-table')[0].table.tbody
exams = []
for tr in ex.find_all('tr'):
    exams.append(tr.get_text())
exams
132/52:
ex = soup.find_all(class_='college-admission-table')[0].table.tbody
exams = []
for tr in ex.find_all('tr'):
    exams.append(tr.td.get_text())
exams
132/53:
res = r.get('https://engineering.careers360.com/colleges/indian-institute-of-technology-madras')
soup = b(res.content,'html.parser')
lists = soup.find_all('ul',class_='clg-info')
l1 = soup.find_all('ul',class_='clg-info')[0]
lsts = []
for li in l1.findAll('li'):
  lsts.append(li.get_text())
lsts
132/54:
facilities = []
facility_list = soup.find('div', class_ = 'facilitylist')
facility_list.find('ul')
for li in facility_list.find_all('li'):
    facilities.append(li.get_text())
facilities
# re.findall(' [A-Z][^A-Z]*', facilities)
132/55:
ex = soup.find_all(class_='college-admission-table')[0].table.tbody
exams = []
for tr in ex.find_all('tr'):
    exams.append(tr.td.get_text())
exams
132/56:
res = r.get('https://medicine.careers360.com/colleges/jamia-millia-islamia-new-delhi')
soup = b(res.content,'html.parser')
lists = soup.find_all('ul',class_='clg-info')
lsts = []
l1 = soup.find_all('ul',class_='clg-info')[0]
lsts = []
for li in l1.findAll('li'):
  lsts.append(li.get_text())
lsts
#soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
132/57:
from bs4 import BeautifulSoup as b
import requests as r
import pandas as pd
import numpy as np
import re
133/1:
from bs4 import BeautifulSoup as b
import requests as r
import pandas as pd
import numpy as np
import re
133/2: links = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
133/3: data = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams'])
133/4:
for i in range(10):
    link = links.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[idx] = [idx, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams]
    idx+=1
133/5:
for i in range(10):
    link = links.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [idx, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams]
133/6:
for i in range(10):
    link = links.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams]
133/7: data.shape
133/8: data.head()
133/9:
for i in range(10):
    link = links.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, *facilities, *exams]
133/10:
for i in range(10):
    link = links.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, str(facilities)[1:-1], str(exams)[1:-1]]
133/11: data.shape
133/12: data.head()
133/13:
def format_faci(l):
    return re.subn("'", "", str(l)[1:-1])
133/14: format_faci(['a', 'b', 'c'])
133/15: type(format_faci(['a', 'b', 'c']))
133/16:
def format_faci(l):
    return re.subn("'", "", str(l)[1:-1])[0]
133/17: type(format_faci(['a', 'b', 'c']))
133/18: format_faci(['a', 'b', 'c'])
133/19:
for i in range(10):
    link = links.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, format_faci(facilities), format_faci(exams)]
133/20: data.shape
133/21: data.head()
133/22:
from sklearn.preprocessing import LabelBinarizer as lb
lb = lb()
lb.fit(data['facilities'])
133/23:
from sklearn.preprocessing import LabelBinarizer as lb
lb = lb()
lb.fit(data['facilities'])
lb.classes
133/24:
from sklearn.preprocessing import LabelBinarizer as lb
lb = lb()
lb.fit(data['facilities'])
lb.classes_
133/25:
from collections import Counter
data2 = data['facilities'].apply(Counter)
pd.DataFrame.from_records(data3).fillna(value=0)
133/26:
from collections import Counter
data2 = data['facilities'].apply(Counter)
pd.DataFrame.from_records(data2).fillna(value=0)
133/27:
def format_faci(l):
    return str(l)[1:-1]
133/28:
for i in range(10):
    link = links.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, format_faci(facilities), format_faci(exams)]
133/29: data.shape
133/30: data.head()
133/31:
from collections import Counter
data2 = data['facilities'].apply(Counter)
pd.DataFrame.from_records(data2).fillna(value=0)
133/32:
def format_faci(l):
    return '"'+str(l)[1:-1]+'"'
133/33:
for i in range(10):
    link = links.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, format_faci(facilities), format_faci(exams)]
133/34: data.shape
133/35: data.head()
133/36:
from collections import Counter
data2 = data['facilities'].apply(Counter)
pd.DataFrame.from_records(data2).fillna(value=0)
133/37:
def format_faci(l):
    return l
133/38:
for i in range(5):
    link = links.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, format_faci(facilities), format_faci(exams)]
133/39: data.shape
133/40: data.head()
133/41:
from collections import Counter
data2 = data['facilities'].apply(Counter)
pd.DataFrame.from_records(data2).fillna(value=0)
133/42:
from collections import Counter
data2 = data['facilities'].apply(Counter)
pd.DataFrame.from_records(data2).fillna(value=0)
data2.columns
133/43:
from collections import Counter
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3.columns
133/44:
from collections import Counter
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
list(filter(lambda x: len(x)>1, data3.columns))
133/45:
from collections import Counter
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3.iloc(list(filter(lambda x: len(x)>1, data3.columns))).head()
133/46:
from collections import Counter
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3.iloc(list(filter(lambda x: len(x)>1, data3.columns)))
133/47:
from collections import Counter
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3.iloc(list(filter(lambda x: len(x)>1, data3.columns)))
data3.head()
133/48:
from collections import Counter
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3.iloc(list(filter(lambda x: len(x)>1, data3.columns)))
data3
133/49:
from collections import Counter
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = pd.DataFrame(data3.iloc(list(filter(lambda x: len(x)>1, data3.columns))))
data3
133/50:
from collections import Counter
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))]
data3.head()
133/51:
from collections import Counter
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))]
data3.astype(int).head()
133/52:
medical = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
eng = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
133/53:
for i in range(5):
    link =  eng.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, format_faci(facilities), format_faci(exams)]
133/54: data.shape
133/55: data.head()
133/56:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
133/57:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
133/58:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data3.head()
133/59:
medical = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
eng = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
133/60:
medical = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
eng = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
law = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
133/61:
for i in range(3):
    link =  law.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, format_faci(facilities), format_faci(exams)]
133/62: data.shape
133/63: data.head()
133/64:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data3.head()
133/65: data.head(3)
133/66: data.head(2)
133/67:
medical = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
eng = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
law = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
design = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
133/68:
for i in range(3):
    link =  design.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, format_faci(facilities), format_faci(exams)]
133/69: data.head(2)
133/70:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data3.head()
133/71: data.to_csv('data.csv')
133/72:
medical = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
eng = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
law = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
design = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
133/73:
medical = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
eng = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
law = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
design = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
it = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
133/74:
for i in range(3):
    link =  it.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, format_faci(facilities), format_faci(exams)]
133/75: data.head(2)
133/76:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data3.head()
133/77: data.to_csv('data.csv')
133/78:
medical = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
eng = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
law = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
design = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
it = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
hotel = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
pharmacy = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
management = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
media = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
133/79:
for i in range(3):
    link =  media.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, format_faci(facilities), format_faci(exams)]
133/80: data.head(2)
133/81:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data3.head()
133/82:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data3 = pd.merge(data, data3)
data3.head()
133/83:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data3 = pd.merge(data, data3, on='Id')
data3.head()
133/84:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data3 = pd.merge(data, data3, on='id')
data3.head()
133/85:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
133/86:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.join(data)
data3.head()
133/87:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.join(data)
133/88:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data.join(data3)
133/89:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data.join(data3).shape()
133/90:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data.join(data3).shape
133/91:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data.join(data3)
133/92:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data = data.join(data3)
133/93: data.head(2)
133/94: data.columns
133/95:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data = data.join(data3)
data.head()
133/96: data = defaultdict()
133/97:
from bs4 import BeautifulSoup as b
import requests as r
import pandas as pd
import numpy as np
import re
from collections import Counter, defaultdict
from threading import Thread
133/98: data = defaultdict()
133/99: data['eng'] = e
133/100: data['eng'] = 4
133/101: data
133/102:
data = defaultdict()
threads = defaultdict()
for stream in streams:
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in streams:
    threads[stream].start()
for stream in streams:
    threads[stream].join()
133/103:
def scrapper(stream):for i in range(3):
    link =  media.iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')
    
    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None
    
    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None
    
    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())
      
    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None
      
    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))
    
    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True
    
#     print(not chk)
    if (not chk):
      estb = None
   
    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None
      
    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None
    
    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None
          
#     try:
#         estb = int(l2[0].get_text()[7:])
#     except:
#         estb = None
#     try:
#         ownership = l2[1].get_text()[12:]
#     except:
#         ownership = None
#     try:
#         approv = l2[2].get_text()[14:]
#     except:
#         approv = None  
    
    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      
        
    l3 = soup.find_all('ul',class_='clg-info')[1].li
    
    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None
        
    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None
    
    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
      
    data.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams]
134/1:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
import requests as r
import pandas as pd
import numpy as np
import re
134/2:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
134/3:
def scrapper(stream):
    global data
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
    for i in np.arange(streams[stream].shape[0]):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
134/4:
data = defaultdict()
threads = defaultdict()
for stream in streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream))
for stream in streams.keys():
    threads[stream].start()
for stream in streams.keys():
    threads[stream].join()
134/5:
data = defaultdict()
threads = defaultdict()
for stream in streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in streams.keys():
    threads[stream].start()
for stream in streams.keys():
    threads[stream].join()
134/6:
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
134/7:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
import requests as r
import pandas as pd
import numpy as np
import re
134/8:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
134/9:
def scrapper(stream):
    global data
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
    for i in np.arange(streams[stream].shape[0]):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

        data = data.append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
134/10:
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
134/11:
def scrapper(stream):
    global data
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
    for i in np.arange(streams[stream].shape[0]):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

        data[stream] = data[stream].append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
134/12:
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
134/13:
%%pdb
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
134/14:
%pdb
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
134/15:
def scrapper(stream):
    global data
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
    for i in np.arange(streams[stream].shape[0]):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

        data[stream] = data[stream].append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        print(data3.columns)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
134/16:
data2 = data['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data = data.join(data3)
data.head()
135/1:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
import requests as r
import pandas as pd
import numpy as np
import re
135/2:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
135/3:
def scrapper(stream):
    global data
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
    for i in np.arange(streams[stream].shape[0]):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

        data[stream] = data[stream].append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        print(data3.columns)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
135/4:
%pdb
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
135/5:
def scrapper(stream):
    global data
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
    for i in np.arange(streams[stream].shape[0]):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

        try:
            data[stream] = data[stream].append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
            data2 = data[stream]['facilities'].apply(Counter)
            data3 = pd.DataFrame.from_records(data2).fillna(value=0)
            data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
            data[stream] = data[stream].join(data3)
        except:
            print(data3.columns)
135/6:
%pdb
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
135/7:
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
135/8:
def scrapper(stream):
    global data
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
    for i in np.arange(streams[stream].shape[0]):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

        try:
            print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
            data[stream] = data[stream].append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
            data2 = data[stream]['facilities'].apply(Counter)
            data3 = pd.DataFrame.from_records(data2).fillna(value=0)
            data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
            data[stream] = data[stream].join(data3)
        except:
            print(data3.columns)
135/9:
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
135/10:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
for i in np.arange(streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    try:
        print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream] = data[stream].append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
    except:
        print(data3.columns)
135/11: data
135/12: data['eng']
135/13:
data = defaultdict()
# threads = defaultdict()
# for stream in ['eng']:#streams.keys():
#     threads[stream] = Thread(target=scrapper, args = (stream,))
# for stream in ['eng']:#streams.keys():
#     threads[stream].start()
# for stream in ['eng']:#streams.keys():
#     threads[stream].join()
135/14: data['eng']
135/15:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
for i in np.arange(streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    try:
        print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream] = data[stream].append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
    except:
        print(data3.columns)
135/16: data['eng']
135/17:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
for i in np.arange(streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream] = data[stream].append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    try:
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
    except:
        data[stream]
137/1:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
import requests as r
import pandas as pd
import numpy as np
import re
137/2:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
137/3:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
for i in np.arange(18):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream] = data[stream].append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    try:
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
    except:
        data[stream]
137/4:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
for i in np.arange(18):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream] = data[stream].append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    try:
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
    except:
        data[stream]
137/5: data['eng']
137/6:
data = defaultdict()
# threads = defaultdict()
# for stream in ['eng']:#streams.keys():
#     threads[stream] = Thread(target=scrapper, args = (stream,))
# for stream in ['eng']:#streams.keys():
#     threads[stream].start()
# for stream in ['eng']:#streams.keys():
#     threads[stream].join()
137/7:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
for i in np.arange(18):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream] = data[stream].append([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    try:
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
    except:
        data[stream]
137/8: data['eng']
137/9:
data = defaultdict()
# threads = defaultdict()
# for stream in ['eng']:#streams.keys():
#     threads[stream] = Thread(target=scrapper, args = (stream,))
# for stream in ['eng']:#streams.keys():
#     threads[stream].start()
# for stream in ['eng']:#streams.keys():
#     threads[stream].join()
137/10:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
for i in np.arange(18):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream] = pd.concat(data[stream],[i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    try:
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
    except:
        data[stream]
137/11: data['eng']
137/12:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
for i in np.arange(18):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream] = pd.concat([data[stream],[i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]], axis=0)
    try:
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
    except:
        data[stream]
137/13:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
for i in np.arange(18):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
    try:
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
    except:
        data[stream]
137/14:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(18):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
    try:
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
    except:
        data[stream]
137/15: data['eng']
137/16:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(18):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
    try:
        data2 = data[stream]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream].join(data3)
    except:
        print('\n\n\n')
        data[stream]
137/17:
data2 = data[stream]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/18:
data2 = data[stream][:10]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/19:
data2 = data[stream][10:15]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/20:
data2 = data[stream][12:15]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/21:
data2 = data[stream][14:15]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/22:
data2 = data[stream][12:14]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/23:
data2 = data[stream][10:14]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/24:
data2 = data[stream][10:11]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/25:
data2 = data[stream][0:9]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/26:
data2 = data[stream][0:10]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/27:
data2 = data[stream][0:11]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/28:
data2 = data[stream][0:12]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/29:
data2 = data[stream][0:13]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/30:
data2 = data[stream][0:14]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/31:
data2 = data[stream][0:15]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/32:
data2 = data[stream][0:45]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/33:
data2 = data[stream][0:20]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/34:
data2 = data[stream][0:18]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/35: data['eng'].head()
138/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
138/2: courses = pd.read_csv('it_courses.csv')
138/3:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://it.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_it({start}-{end}).csv')
138/4: scrapper(3105,3600)
137/36:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(25):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
try:
    data2 = data[stream]['facilities'].apply(Counter)
    data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data[stream] = data[stream].join(data3)
except:
    print('\n\n\n')
    data[stream]
141/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
141/2: courses = pd.read_csv('it_courses.csv')
141/3:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://it.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_it({start}-{end}).csv')
141/4: scrapper(828,1500)
139/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
139/2: courses = pd.read_csv('it_courses.csv')
139/3:
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://it.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_it({start}-{end}).csv')
137/37: data.head()
137/38: data[stream].head()
139/4: scrapper(3600,4145)
137/39:
data2 = data[stream][0:18]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
137/40:
data2 = data[stream][0:25]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3 = pd.merge(data, data3, on='id')
data3.head()
142/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
courses = pd.read_csv('mang_courses.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
from threading import Thread
def scrapper(start, end):
    career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
    indx = 0
    cnt = start - 1
    for header_course in tqdm(courses['2'][start:end]):
        cnt += 1
        try:
            num = int(np.ceil(int(courses['1'][cnt])/10))
        except:
            continue
    #     print(f'num = {num}')
        for page_num in tqdm(range(num)):
            link = header_course + '?levelId=all&courseNid=all-all&page='+str(page_num)
    #         print(link)
            page = rq.get(link)
            course_soup = bs(page.content, 'html.parser')
        for course_link in course_soup.find_all('span',class_='readmore'):
            course_link = 'https://bschool.careers360.com/' + course_link.a['href']
            #             print(f'course_link = {course_link}')
            page1 = rq.get(course_link)
            soup = bs(page1.content,'html.parser')
            career360_courses.loc[indx]=None

            try:
                career360_courses.loc[indx]['Course'] = soup.find_all('h2',class_='block-title')[0].get_text()
                career360_courses.loc[indx]['College']=soup.find_all('h1',class_='titleNameCol')[0].get_text()
                career360_courses.loc[indx]['City'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[0].get_text()
                career360_courses.loc[indx]['State'] = soup.find_all('ul',class_='clg-info')[0].find_all('li')[0].find_all('a')[1].get_text()
            except:
                pass

            try:
                career360_courses.loc[indx]['Eligiblity'] = soup.find_all('span',class_='more-eligibility')[0].get_text()[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Eligiblity'] = soup.find_all('div',class_='default-elig')[0].get_text()
                except:
                    career360_courses.loc[indx]['Eligiblity'] = None

            try:
                career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('span',class_='moreCourseDetails')[0].p.get_text())[:-11]
            except:
                try:
                    career360_courses.loc[indx]['Course_Detail'] = (soup.find_all('div',class_='default-course-details')[0].get_text())
                except:
                    career360_courses.loc[indx]['Course_Detail'] = None

            for row in soup.find_all('div',class_='coursesPageLableInnerSec'):
                career360_courses.loc[indx][row.strong.get_text()] = row.p.get_text()
            indx+=1
    career360_courses.to_csv(f'career360_scrap_dm_mang({start}-{end}).csv')
142/2: scrapper(3216,3816)
137/41:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(25):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
try:
    
    data2 = data[stream][0:25]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data3.head()
# data3 = pd.merge(data, data3, on='id')
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
#     data[stream] = data[stream].join(data3)
except:
    print('\n\n\n')
    data[stream]
137/42:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(25):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
try:
    
    data2 = data[stream][0:25]['facilities'].apply(Counter)
    data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data3.head()
# data3 = pd.merge(data, data3, on='id')
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
#     data[stream] = data[stream].join(data3)
except:
    print('\n\n\n')
    data[stream]
143/1:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
import requests as r
import pandas as pd
import numpy as np
import re
143/2:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
142/3: scrapper(3216,3500)
139/5: scrapper(3600,4145)
141/5: scrapper(828,1500)
143/3:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
import requests as r
import pandas as pd
import numpy as np
import re
143/4:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
144/1:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
import requests as r
import pandas as pd
import numpy as np
import re
144/2:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
144/3:
data = defaultdict()
# threads = defaultdict()
# for stream in ['eng']:#streams.keys():
#     threads[stream] = Thread(target=scrapper, args = (stream,))
# for stream in ['eng']:#streams.keys():
#     threads[stream].start()
# for stream in ['eng']:#streams.keys():
#     threads[stream].join()
144/4:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(25):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

    print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
try:
    
    data2 = data[stream][0:25]['facilities'].apply(Counter)
    data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data3.head()
# data3 = pd.merge(data, data3, on='id')
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
#     data[stream] = data[stream].join(data3)
except:
    print('\n\n\n')
    data[stream]
144/5: data['eng'].head()
144/6:     data3 = pd.merge(data, data3, on='id')
144/7: data3 = pd.merge(data, data3, on='id')
144/8: data3 = pd.merge(data[stream], data3, on='id')
144/9: data[stream].join(data3)
144/10:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(25):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
try:
    
    data2 = data[stream][0:25]['facilities'].apply(Counter)
    data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data3 = pd.merge(data, data3, on='id')
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
     data[stream] = data[stream].join(data3)
except:
    print('\n\n\n')
    data[stream]
144/11:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(25):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
try:
    
    data2 = data[stream][0:25]['facilities'].apply(Counter)
    data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data3 = pd.merge(data, data3, on='id')
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data[stream] = data[stream].join(data3)
except:
    print('\n\n\n')
    data[stream]
142/4: scrapper(3500,3816)
144/12: data['eng'].head()
144/13:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(25):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
try:
    
    data2 = data[stream][0:25]['facilities'].apply(Counter)
    data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data3 = pd.merge(data, data3, on='id')
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data[stream] = data[stream].join(data3)
except:
    print('\n\n\n')
    data[stream]
144/14:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(25):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
try:
    
    data2 = data[stream][:25]['facilities'].apply(Counter)
    data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data[stream] = data[stream][:25].join(data3)
except:
    print('\n\n\n')
    data[stream]
145/1:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
import requests as r
import pandas as pd
import numpy as np
import re
145/2:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
145/3:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(25):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
try:
    
    data2 = data[stream][:25]['facilities'].apply(Counter)
    data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data[stream] = data[stream][:25].join(data3)
except:
    print('\n\n\n')
    data[stream]
145/4: data[stream].join(data3)
145/5:
data = defaultdict()
# threads = defaultdict()
# for stream in ['eng']:#streams.keys():
#     threads[stream] = Thread(target=scrapper, args = (stream,))
# for stream in ['eng']:#streams.keys():
#     threads[stream].start()
# for stream in ['eng']:#streams.keys():
#     threads[stream].join()
145/6:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(25):#streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
try:
    
    data2 = data[stream][:25]['facilities'].apply(Counter)
    data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data[stream] = data[stream][:25].join(data3)
except:
    print('\n\n\n')
    data[stream]
145/7: data[stream].join(data3)
145/8: data[stream]
145/9:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

data2 = data[stream][:streams[stream].shape[0]]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data[stream] = data[stream][:streams[stream].shape[0]].join(data3)
145/10:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
from tqdm import tqdm
import requests as r
import pandas as pd
import numpy as np
import re
145/11:
stream = 'eng'
global data
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(streams[stream].shape[0]):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

try:
    data2 = data[stream][:streams[stream].shape[0]]['facilities'].apply(Counter)
    data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    # data3.head()
    #     data2 = data[stream]['facilities'].apply(Counter)
    #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data[stream] = data[stream][:streams[stream].shape[0]].join(data3)
except:
    print(f'{stream} not completed correctly')
145/12:
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
145/13:
def scrapper(stream):
    global data
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
    for i in np.arange(10):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

    try:
    #     ''':streams[stream].shape[0]]'''
        data2 = data[stream][10]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        # data3.head()
        #     data2 = data[stream]['facilities'].apply(Counter)
        #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream][:10].join(data3)
    except:
        print(f'{stream} not completed correctly')
145/14:
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
145/15:
%debug
def scrapper(stream):
    global data
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
    for i in np.arange(10):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

    try:
    #     ''':streams[stream].shape[0]]'''
        data2 = data[stream][10]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        # data3.head()
        #     data2 = data[stream]['facilities'].apply(Counter)
        #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream][:10].join(data3)
    except:
        print(f'{stream} not completed correctly')
145/16:
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
145/17:
data = defaultdict()
threads = defaultdict()
for stream in ['eng']:#streams.keys():
    threads[stream] = Thread(target=scrapper, args = (stream,))
for stream in ['eng']:#streams.keys():
    threads[stream].start()
for stream in ['eng']:#streams.keys():
    threads[stream].join()
145/18: data
145/19: data['eng']
145/20: data['medical']
145/21: data.keys()
145/22:
data = defaultdict()
global data
for stream in streams:
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
    for i in np.arange(10):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

    try:
    #     ''':streams[stream].shape[0]]'''
        data2 = data[stream][10]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        # data3.head()
        #     data2 = data[stream]['facilities'].apply(Counter)
        #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream][:10].join(data3)
    except:
        print(f'{stream} not completed correctly')
145/23:
data = defaultdict()
for stream in streams:
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
    for i in np.arange(10):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

    try:
    #     ''':streams[stream].shape[0]]'''
        data2 = data[stream][10]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        # data3.head()
        #     data2 = data[stream]['facilities'].apply(Counter)
        #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream][:10].join(data3)
    except:
        print(f'{stream} not completed correctly')
145/24: data['eng'].head()
145/25: data
145/26:
data = defaultdict(pd.Dataframe)
for stream in streams:
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
    for i in np.arange(10):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

    try:
    #     ''':streams[stream].shape[0]]'''
        data2 = data[stream][10]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        # data3.head()
        #     data2 = data[stream]['facilities'].apply(Counter)
        #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream][:10].join(data3)
    except:
        print(f'{stream} not completed correctly')
145/27:
data = defaultdict(pd.DataFrame)
for stream in streams:
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
    for i in np.arange(10):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

    try:
    #     ''':streams[stream].shape[0]]'''
        data2 = data[stream][10]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        # data3.head()
        #     data2 = data[stream]['facilities'].apply(Counter)
        #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream][:10].join(data3)
    except:
        print(f'{stream} not completed correctly')
145/28:
data = defaultdict(pd.DataFrame)
for stream in streams:
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]), dtype=None)
    for i in np.arange(10):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

    try:
    #     ''':streams[stream].shape[0]]'''
        data2 = data[stream][10]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        # data3.head()
        #     data2 = data[stream]['facilities'].apply(Counter)
        #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream][:10].join(data3)
    except:
        print(f'{stream} not completed correctly')
145/29:
data = defaultdict(pd.DataFrame)
for stream in streams:
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
    for i in np.arange(10):
        try:
            link =  streams[stream].iloc[i,1]
            try:
                res = r.get(link)
            except:
                res = None
            soup = b(res.content, 'html.parser')

            try:
                also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
            except:
                also_as = None

            try:
                name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
            except:
                name = None

            l1 = soup.find_all('ul',class_='clg-info')[0]
            lsts = []
            for li in l1.findAll('li'):
              lsts.append(li.get_text())

            try:
                city, state = lsts[0].split(',')
            except:
                city, state = None,None

            chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
        #     for lst in lsts[1:]:
        #       print(re.sub(chks[0],'',lst))

            estb = None
            ownership = None
            approved_by = None
            affilated_to = None
            chk = False
            for lst in lsts[1:]:
              if chks[0] in lst:
                estb = re.sub(chks[0],'',lst)
        #         print(estb)
                chk = True

        #     print(not chk)
            if (not chk):
              estb = None

            chk = False
            for lst in lsts[1:]:
              if chks[1] in lst:
                ownership = re.sub(chks[1],'',lst)
        #         print(ownership)
                chk = True
            if(not chk):
              ownership = None

            chk = False
            for lst in lsts[1:]:
              if chks[2] in lst:
                approved_by = re.sub(chks[2],'',lst)
                chk = True
            if not chk:
              approved_by = None

            chk = False
            for lst in lsts[1:]:
              if chks[3] in lst:
                affilated_to = re.sub(chks[3],'',lst)
                chk = True
            if not chk:
              affilated_to = None

            try:
                add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
            except:
                add = None      

            l3 = soup.find_all('ul',class_='clg-info')[1].li

            try:
                website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
            except:
                website = None
            try:
                contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
            except:
                contact = None

            try:
                rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
            except:
                rank = None   
            try:
                score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
            except:
                score = None
            try:
                facilities = []
                facility_list = soup.find('div', class_ = 'facilitylist')
                facility_list.find('ul')
                for li in facility_list.find_all('li'):
                    facilities.append(li.get_text())
            except:
                None

            try:
                ex = soup.find_all(class_='college-admission-table')[0].table.tbody
                exams = []
                for tr in ex.find_all('tr'):
                    exams.append(tr.td.get_text())
            except:
                None

        #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
            data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
        except:
            pass
    try:
    #     ''':streams[stream].shape[0]]'''
        data2 = data[stream][10]['facilities'].apply(Counter)
        data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        # data3.head()
        #     data2 = data[stream]['facilities'].apply(Counter)
        #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
        #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
        data[stream] = data[stream][:10].join(data3)
    except:
        print(f'{stream} not completed correctly')
145/30:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    try:
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
    except:
        pass
try:
#     ''':streams[stream].shape[0]]'''
    data2 = data[stream][10]['facilities'].apply(Counter)
    data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    # data3.head()
    #     data2 = data[stream]['facilities'].apply(Counter)
    #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
    #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
    data[stream] = data[stream][:10].join(data3)
except:
    print(f'{stream} not completed correctly')
145/31:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    try:
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
          lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
          if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
    #         print(estb)
            chk = True

    #     print(not chk)
        if (not chk):
          estb = None

        chk = False
        for lst in lsts[1:]:
          if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
    #         print(ownership)
            chk = True
        if(not chk):
          ownership = None

        chk = False
        for lst in lsts[1:]:
          if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
        if not chk:
          approved_by = None

        chk = False
        for lst in lsts[1:]:
          if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
        if not chk:
          affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]
    except:
        pass
#     ''':streams[stream].shape[0]]'''
data2 = data[stream][10]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data[stream] = data[stream][:10].join(data3)
# except:
#     print(f'{stream} not completed correctly')
145/32:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
data2 = data[stream][10]['facilities'].apply(Counter)
data3 = pd.DataFrame.from_records(data2).fillna(value=0)
data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data3.head()
#     data2 = data[stream]['facilities'].apply(Counter)
#     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
#     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
data[stream] = data[stream][:10].join(data3)
# except:
#     print(f'{stream} not completed correctly')
145/33:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
from tqdm import tqdm
import requests as r
import pandas as pd
import numpy as np
import re
145/34:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
145/35: data = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams'])
145/36:
res = r.get('https://medicine.careers360.com/colleges/madras-medical-college-chennai')
soup = b(res.content,'html.parser')
145/37:
res = r.get('https://engineering.careers360.com/colleges/indian-institute-of-technology-madras')
soup = b(res.content,'html.parser')
lists = soup.find_all('ul',class_='clg-info')
l1 = soup.find_all('ul',class_='clg-info')[0]
lsts = []
for li in l1.findAll('li'):
  lsts.append(li.get_text())
lsts
145/38:
facilities = []
facility_list = soup.find('div', class_ = 'facilitylist')
facility_list.find('ul')
for li in facility_list.find_all('li'):
    facilities.append(li.get_text())
facilities
# re.findall(' [A-Z][^A-Z]*', facilities)
145/39:
ex = soup.find_all(class_='college-admission-table')[0].table.tbody
exams = []
for tr in ex.find_all('tr'):
    exams.append(tr.td.get_text())
exams
145/40:
res = r.get('https://medicine.careers360.com/colleges/jamia-millia-islamia-new-delhi')
soup = b(res.content,'html.parser')
lists = soup.find_all('ul',class_='clg-info')
lsts = []
l1 = soup.find_all('ul',class_='clg-info')[0]
lsts = []
for li in l1.findAll('li'):
  lsts.append(li.get_text())
lsts
#soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
145/41:
l1 = soup.find_all('ul',class_='clg-info')[0].li
l2 = l1.find_next_siblings('li')
estb = int(l2[0].get_text()[7:])
estb
145/42: data[stream].head()
145/43:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
145/44:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
# data2 = data[stream][10]['facilities'].apply(Counter)
# data3 = pd.DataFrame.from_records(data2).fillna(value=0)
# data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# # data3.head()
# #     data2 = data[stream]['facilities'].apply(Counter)
# #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
# #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data[stream] = data[stream][:10].join(data3)
# # except:
# #     print(f'{stream} not completed correctly')
145/45: streams[stream].head()
145/46:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'])
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
# data2 = data[stream][10]['facilities'].apply(Counter)
# data3 = pd.DataFrame.from_records(data2).fillna(value=0)
# data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# # data3.head()
# #     data2 = data[stream]['facilities'].apply(Counter)
# #     data3 = pd.DataFrame.from_records(data2).fillna(value=0)
# #     data3 = data3[list(filter(lambda x: len(x)>1, data3.columns))].astype(int)
# data[stream] = data[stream][:10].join(data3)
# # except:
# #     print(f'{stream} not completed correctly')
145/47:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
145/48:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    print(link)
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
      lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
      if chks[0] in lst:
        estb = re.sub(chks[0],'',lst)
#         print(estb)
        chk = True

#     print(not chk)
    if (not chk):
      estb = None

    chk = False
    for lst in lsts[1:]:
      if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
#         print(ownership)
        chk = True
    if(not chk):
      ownership = None

    chk = False
    for lst in lsts[1:]:
      if chks[2] in lst:
        approved_by = re.sub(chks[2],'',lst)
        chk = True
    if not chk:
      approved_by = None

    chk = False
    for lst in lsts[1:]:
      if chks[3] in lst:
        affilated_to = re.sub(chks[3],'',lst)
        chk = True
    if not chk:
      affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
145/49:
res = r.get('https://medicine.careers360.com/colleges/jamia-millia-islamia-new-delhi')
soup = b(res.content,'html.parser')
lists = soup.find_all('ul',class_='clg-info')
lsts = []
l1 = soup.find_all('ul',class_='clg-info')[0]
lsts = []
for li in l1.findAll('li'):
  lsts.append(li.get_text())
lsts
#soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
145/50:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
    if chks[1] in lst:
        ownership = re.sub(chks[1],'',lst)
        #         print(ownership)
        chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
145/51:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
        if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
            #         print(ownership)
            chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
145/52:
stream = 'eng'
data1 = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
        if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
            #         print(ownership)
            chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data1.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
145/53: soup
145/54: data1
145/55: str(soup)
145/56:
stream = 'eng'
data1 = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in range(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
        if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
            #         print(ownership)
            chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data1.loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
145/57: data1
145/58: re.findall(r'Total Faculty In College: \d+',str(soup))
145/59:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
        if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
            #         print(ownership)
            chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
145/60:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
145/61: re.findall(r'Total Faculty In College: \d+',str(soup))
145/62:
res = r.get('https://engineering.careers360.com/colleges/indian-institute-of-technology-madras')
soup = b(res.content,'html.parser')
lists = soup.find_all('ul',class_='clg-info')
l1 = soup.find_all('ul',class_='clg-info')[0]
lsts = []
for li in l1.findAll('li'):
  lsts.append(li.get_text())
lsts
145/63: re.findall(r'Total Faculty In College: \d+',str(soup))
145/64: defaultdict()
145/65: temp = defaultdict()
145/66: re.findall(r'\d+',re.findall(r'Total Faculty In College: \d+',str(soup)))
145/67: temp[pd.DataFrame()]
145/68: re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))
145/69: temp['eng'] = pd.DataFrame()
145/70: temp
145/71: int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup)))))
145/72: int(re.find(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup)))))
145/73: temp['eng'].columns = [1,2,3]
145/74: int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))[0])
145/75: temp['eng'] = pd.DataFrame(columns = [1,2,3])
145/76: temp['eng']
146/1:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
from tqdm import tqdm
import requests as r
import pandas as pd
import numpy as np
import re
146/2:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
146/3: temp = defaultdict()
146/4: temp['eng'] = pd.DataFrame(columns = [1,2,3])
146/5: temp['eng']
146/6: temp['eng'] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
146/7:
temp = defaultdict()
stream = 'eng'
146/8: temp['eng'] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
146/9: temp['eng']
146/10:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
data = defaultdict()
146/11:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
        if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
            #         print(ownership)
            chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
146/12: data['eng']
145/77:
res = r.get('https://engineering.careers360.com/colleges/indian-institute-of-technology-madras')
soup = b(res.content,'html.parser')
lists = soup.find_all('ul',class_='clg-info')
l1 = soup.find_all('ul',class_='clg-info')[0]
lsts = []
for li in l1.findAll('li'):
  lsts.append(li.get_text())
lsts
145/78: social = soup.find_all('div', class_='social-icon-left')
145/79:
social = soup.find_all('div', class_='social-icon-left')
social
145/80:
social = soup.find_all('div', class_='social-icon-left')
social.find('ul', class_= 'blockInfoIco')
145/81:
social = soup.find_all('div', class_='social-icon-left')
social.find_all('ul', class_= 'blockInfoIco')
145/82:
social = soup.find_all('div', class_='social-icon-left')
social.find_all('ul', class_= 'blockInfoIco')
145/83:
social = soup.find_all('div', class_='social-icon-left')
social.type
145/84:
social = soup.find_all('div', class_='social-icon-left')
type(social)
145/85: social = soup.find_all('div', class_='social-icon-left')[0]
145/86:
social = soup.find_all('div', class_='social-icon-left')[0]
social
145/87:
social = soup.find_all('div', class_='social-icon-left')[0]
social.find_all('ul', class_ = "blockInfoIco")
145/88:
social = soup.find_all('div', class_='social-icon-left')[0]
social.find_all('ul', class_ = "blockInfoIco")[0]
145/89:
social = soup.find_all('div', class_='social-icon-left')[0]
social.find_all('ul', class_ = "blockInfoIco")[0]
social.find_all('li')
145/90:
social = soup.find_all('div', class_='social-icon-left')[0]
social.find_all('li')
145/91:
social = soup.find_all('div', class_='social-icon-left')[0]
social.find_all('li')[0]
145/92:
social = soup.find_all('div', class_='social-icon-left')[0]
social.find_all('li')[0].a
145/93:
social = soup.find_all('div', class_='social-icon-left')[0]
social.find_all('li')[0].find_all('a')
145/94:
social = soup.find_all('div', class_='social-icon-left')[0]
social.find_all('li').find_all('a')
145/95:
social = soup.find_all('div', class_='social-icon-left')[0]
social.find_all('li').a
145/96:
social = soup.find_all('div', class_='social-icon-left')[0]
social.find_all('li')
145/97:
social = soup.find_all('div', class_='social-icon-left')[0]
social.find_all('li')
li.a
145/98:
social = soup.find_all('div', class_='social-icon-left')[0]
social = social.find_all('li')
145/99:
social = soup.find_all('div', class_='social-icon-left')[0]
social = social.find_all('li')
social
145/100:
social = soup.find_all('div', class_='social-icon-left')[0]
social = social.find_all('li')
social[0]
145/101:
social = soup.find_all('div', class_='social-icon-left')[0]
social = social.find_all('li')
social[0].a
145/102:
social = soup.find_all('div', class_='social-icon-left')[0]
social = social.find_all('li')
social[0].a.href
145/103:
social = soup.find_all('div', class_='social-icon-left')[0]
social = social.find_all('li')
social[0].a
145/104:
social = soup.find_all('div', class_='social-icon-left')[0]
social = social.find_all('li')
social[0].a.get_text()
145/105:
social = soup.find_all('div', class_='social-icon-left')[0]
social = social.find_all('li')
social[0].a
145/106:
social = soup.find_all('div', class_='social-icon-left')[0]
social = social.find_all('li')
social[0].a.href
145/107:
social = soup.find_all('div', class_='social-icon-left')[0]
social = social.find_all('li')
social[0].a['href']
145/108:
urls = []
for li in social.find_all('li'):
    urls.append(li.a['href'])
urls
145/109:
urls = []
for li in social.find_all('li'):
    print(li)
    urls.append(li.a['href'])
urls
145/110:
urls = []
for li in social.find_all('li'):
    print(li)
#     urls.append(li.a['href'])
# urls
145/111:
social = soup.find_all('div', class_='social-icon-left')[0]
# social = social.find_all('li')
social[0].a['href']
145/112:
urls = []
for li in social.find_all('li'):
    print(li)
#     urls.append(li.a['href'])
# urls
145/113:
urls = []
for li in social.find_all('li'):
    print(li)
    urls.append(li.a['href'])
urls
145/114:
urls = defaultdict()
for li in social.find_all('li'):
    urls.li.a['class'] = urls.li.a['href']
urls
145/115:
urls = defaultdict()
for li in social.find_all('li'):
    li.a['class'] = urls.li.a['href']
urls
145/116:
urls = defaultdict()
for li in social.find_all('li'):
    li.a['class'] = li.a['href']
urls
145/117:
urls = defaultdict()
for li in social.find_all('li'):
    urls.append(li.a['href'])
urls
145/118:
urls = [defaultdict()]
for li in social.find_all('li'):
    urls.append(li.a['href'])
urls
145/119:
urls = []
for li in social.find_all('li'):
    urls.append(li.a['href'])
urls
145/120:
urls = []
for li in social.find_all('li'):
    urls.append(li.a['class'])
urls
145/121:
urls = []
for li in social.find_all('li'):
    urls.append(li.a['href'])
urls
145/122:
urls = []
for li in social.find_all('li'):
    urls.append(li.a['class'])
urls
145/123:
urls = []
for li in social.find_all('li'):
    urls.append(li.a['href'])
urls
145/124: re.search('face', urls[0])
145/125: str(re.search('face', urls[0])[0])
145/126: str(re.search('.*face.*', urls[0])[0])
145/127: str(re.search('.*face.*', urls[1])[0])
145/128: str(re.search('.*face.*', urls[0])[0])
147/1:
import numpy as np
import pandas as pd
import requests as rq
from bs4 import BeautifulSoup as bs
#from google.colab import files
from tqdm import *
#courses = pd.read_csv('courses_all.csv')
#career360_courses = pd.DataFrame(columns=['College','City','State','Course','Duration','Study mode','Approved intake','Course level','Course Fee','Approval','Eligiblity','Course_Detail'])
145/129: str(re.search('.*face.*', urls)[0])
146/13:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
        if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
            #         print(ownership)
            chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
        
        try:
        faculty = int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))[0])
    except:
        faculty =  None
        
    social = soup.find_all('div', class_='social-icon-left')[0]
    urls = []
    for li in social.find_all('li'):
        urls.append(li.a['href'])
    face = list(filter(lambda x: 'face' in x, urls))
    twit = list(filter(lambda x: 'twit' in x, urls))
    you = list(filter(lambda x: 'youtube' in x, urls))
    wiki = list(filter(lambda x: 'wiki' in x, urls))
    face, twit, you, wiki = list(map(lambda x: x if len(x) > 0 else None), [face, twit, you, wiki])

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
146/14:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
        if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
            #         print(ownership)
            chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
        
        try:
            faculty = int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))[0])
        except:
            faculty =  None
        
    social = soup.find_all('div', class_='social-icon-left')[0]
    urls = []
    for li in social.find_all('li'):
        urls.append(li.a['href'])
    face = list(filter(lambda x: 'face' in x, urls))
    twit = list(filter(lambda x: 'twit' in x, urls))
    you = list(filter(lambda x: 'youtube' in x, urls))
    wiki = list(filter(lambda x: 'wiki' in x, urls))
    face, twit, you, wiki = list(map(lambda x: x if len(x) > 0 else None), [face, twit, you, wiki])

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
146/15:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
        if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
            #         print(ownership)
            chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
        
        try:
            faculty = int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))[0])
        except:
            faculty =  None
        
    social = soup.find_all('div', class_='social-icon-left')[0]
    urls = []
    for li in social.find_all('li'):
        urls.append(li.a['href'])
    face = list(filter(lambda x: 'face' in x, urls))
    twit = list(filter(lambda x: 'twit' in x, urls))
    you = list(filter(lambda x: 'youtube' in x, urls))
    wiki = list(filter(lambda x: 'wiki' in x, urls))
    face, twit, you, wiki = list(map(lambda x: x if len(x) > 0 else None, [face, twit, you, wiki]))

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream]

#     ''':streams[stream].shape[0]]'''
146/16: data['eng']
146/17:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream', 'faculty', 'facebook', 'twitter','youtube', 'wiki'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
        if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
            #         print(ownership)
            chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
        
        try:
            faculty = int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))[0])
        except:
            faculty =  None
        
    social = soup.find_all('div', class_='social-icon-left')[0]
    urls = []
    for li in social.find_all('li'):
        urls.append(li.a['href'])
    face = list(filter(lambda x: 'face' in x, urls))
    twit = list(filter(lambda x: 'twit' in x, urls))
    you = list(filter(lambda x: 'youtube' in x, urls))
    wiki = list(filter(lambda x: 'wiki' in x, urls))
    face, twit, you, wiki = list(map(lambda x: x if len(x) > 0 else None, [face, twit, you, wiki]))

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream, faculty, face, twit, you, wiki]

#     ''':streams[stream].shape[0]]'''
146/18:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream', 'faculty', 'facebook', 'twitter','youtube', 'wiki'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
        if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
            #         print(ownership)
            chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
        
    try:
        faculty = int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))[0])
    except:
        faculty =  None
        
    social = soup.find_all('div', class_='social-icon-left')[0]
    urls = []
    for li in social.find_all('li'):
        urls.append(li.a['href'])
    face = list(filter(lambda x: 'face' in x, urls))
    twit = list(filter(lambda x: 'twit' in x, urls))
    you = list(filter(lambda x: 'youtube' in x, urls))
    wiki = list(filter(lambda x: 'wiki' in x, urls))
    face, twit, you, wiki = list(map(lambda x: x if len(x) > 0 else None, [face, twit, you, wiki]))

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream, faculty, face, twit, you, wiki]

#     ''':streams[stream].shape[0]]'''
146/19: data['eng']
146/20:
for stream in streams.keys():
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream', 'faculty', 'facebook', 'twitter','youtube', 'wiki'], index=range(streams[stream].shape[0]))
    for i in np.arange(3):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
            lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
            if chks[0] in lst:
                estb = re.sub(chks[0],'',lst)
                #         print(estb)
                chk = True

    #     print(not chk)
        if (not chk):
            estb = None

        chk = False
        for lst in lsts[1:]:
            if chks[1] in lst:
                ownership = re.sub(chks[1],'',lst)
                #         print(ownership)
                chk = True
        if(not chk):
            ownership = None

        chk = False
        for lst in lsts[1:]:
            if chks[2] in lst:
                approved_by = re.sub(chks[2],'',lst)
                chk = True
        if not chk:
            approved_by = None

        chk = False
        for lst in lsts[1:]:
            if chks[3] in lst:
                affilated_to = re.sub(chks[3],'',lst)
                chk = True
        if not chk:
            affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

        try:
            faculty = int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))[0])
        except:
            faculty =  None

        social = soup.find_all('div', class_='social-icon-left')[0]
        urls = []
        for li in social.find_all('li'):
            urls.append(li.a['href'])
        face = list(filter(lambda x: 'face' in x, urls))
        twit = list(filter(lambda x: 'twit' in x, urls))
        you = list(filter(lambda x: 'youtube' in x, urls))
        wiki = list(filter(lambda x: 'wiki' in x, urls))
        face, twit, you, wiki = list(map(lambda x: x[0] if len(x) > 0 else None, [face, twit, you, wiki]))

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream, faculty, face, twit, you, wiki]

#     ''':streams[stream].shape[0]]'''
146/21: data['eng']
146/22:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
from tqdm import tqdm
import requests as r
import pandas as pd
import numpy as np
import re
146/23: data['medical']
146/24:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
data = defaultdict()
146/25:
stream = 'eng'
data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream', 'faculty', 'facebook', 'twitter','youtube', 'wiki'], index=range(streams[stream].shape[0]))
for i in np.arange(10):
    link =  streams[stream].iloc[i,1]
    try:
        res = r.get(link)
    except:
        res = None
    soup = b(res.content, 'html.parser')

    try:
        also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
    except:
        also_as = None

    try:
        name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
    except:
        name = None

    l1 = soup.find_all('ul',class_='clg-info')[0]
    lsts = []
    for li in l1.findAll('li'):
        lsts.append(li.get_text())

    try:
        city, state = lsts[0].split(',')
    except:
        city, state = None,None

    chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
#     for lst in lsts[1:]:
#       print(re.sub(chks[0],'',lst))

    estb = None
    ownership = None
    approved_by = None
    affilated_to = None
    chk = False
    for lst in lsts[1:]:
        if chks[0] in lst:
            estb = re.sub(chks[0],'',lst)
            #         print(estb)
            chk = True

#     print(not chk)
    if (not chk):
        estb = None

    chk = False
    for lst in lsts[1:]:
        if chks[1] in lst:
            ownership = re.sub(chks[1],'',lst)
            #         print(ownership)
            chk = True
    if(not chk):
        ownership = None

    chk = False
    for lst in lsts[1:]:
        if chks[2] in lst:
            approved_by = re.sub(chks[2],'',lst)
            chk = True
    if not chk:
        approved_by = None

    chk = False
    for lst in lsts[1:]:
        if chks[3] in lst:
            affilated_to = re.sub(chks[3],'',lst)
            chk = True
    if not chk:
        affilated_to = None

    try:
        add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
    except:
        add = None      

    l3 = soup.find_all('ul',class_='clg-info')[1].li

    try:
        website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
    except:
        website = None
    try:
        contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
    except:
        contact = None

    try:
        rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
    except:
        rank = None   
    try:
        score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
    except:
        score = None
    try:
        facilities = []
        facility_list = soup.find('div', class_ = 'facilitylist')
        facility_list.find('ul')
        for li in facility_list.find_all('li'):
            facilities.append(li.get_text())
    except:
        None

    try:
        ex = soup.find_all(class_='college-admission-table')[0].table.tbody
        exams = []
        for tr in ex.find_all('tr'):
            exams.append(tr.td.get_text())
    except:
        None
        
    try:
        faculty = int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))[0])
    except:
        faculty =  None
        
    social = soup.find_all('div', class_='social-icon-left')[0]
    urls = []
    for li in social.find_all('li'):
        urls.append(li.a['href'])
    face = list(filter(lambda x: 'face' in x, urls))
    twit = list(filter(lambda x: 'twit' in x, urls))
    you = list(filter(lambda x: 'youtube' in x, urls))
    wiki = list(filter(lambda x: 'wiki' in x, urls))
    face, twit, you, wiki = list(map(lambda x: x if len(x) > 0 else None, [face, twit, you, wiki]))

#     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
    data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream, faculty, face, twit, you, wiki]

#     ''':streams[stream].shape[0]]'''
146/26: data['eng']
146/27:
for stream in streams.keys():
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream', 'faculty', 'facebook', 'twitter','youtube', 'wiki'], index=range(streams[stream].shape[0]))
    for i in np.arange(streams[stream].shape[0]):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
            lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
            if chks[0] in lst:
                estb = re.sub(chks[0],'',lst)
                #         print(estb)
                chk = True

    #     print(not chk)
        if (not chk):
            estb = None

        chk = False
        for lst in lsts[1:]:
            if chks[1] in lst:
                ownership = re.sub(chks[1],'',lst)
                #         print(ownership)
                chk = True
        if(not chk):
            ownership = None

        chk = False
        for lst in lsts[1:]:
            if chks[2] in lst:
                approved_by = re.sub(chks[2],'',lst)
                chk = True
        if not chk:
            approved_by = None

        chk = False
        for lst in lsts[1:]:
            if chks[3] in lst:
                affilated_to = re.sub(chks[3],'',lst)
                chk = True
        if not chk:
            affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

        try:
            faculty = int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))[0])
        except:
            faculty =  None

        social = soup.find_all('div', class_='social-icon-left')[0]
        urls = []
        for li in social.find_all('li'):
            urls.append(li.a['href'])
        face = list(filter(lambda x: 'face' in x, urls))
        twit = list(filter(lambda x: 'twit' in x, urls))
        you = list(filter(lambda x: 'youtube' in x, urls))
        wiki = list(filter(lambda x: 'wiki' in x, urls))
        face, twit, you, wiki = list(map(lambda x: x[0] if len(x) > 0 else None, [face, twit, you, wiki]))

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream, faculty, face, twit, you, wiki]

#     ''':streams[stream].shape[0]]'''
146/28:
for stream in streams.keys():
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream', 'faculty', 'facebook', 'twitter','youtube', 'wiki'], index=range(streams[stream].shape[0]))
    for i in tqdm(np.arange(streams[stream].shape[0])):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
            continue
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
            lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
            if chks[0] in lst:
                estb = re.sub(chks[0],'',lst)
                #         print(estb)
                chk = True

    #     print(not chk)
        if (not chk):
            estb = None

        chk = False
        for lst in lsts[1:]:
            if chks[1] in lst:
                ownership = re.sub(chks[1],'',lst)
                #         print(ownership)
                chk = True
        if(not chk):
            ownership = None

        chk = False
        for lst in lsts[1:]:
            if chks[2] in lst:
                approved_by = re.sub(chks[2],'',lst)
                chk = True
        if not chk:
            approved_by = None

        chk = False
        for lst in lsts[1:]:
            if chks[3] in lst:
                affilated_to = re.sub(chks[3],'',lst)
                chk = True
        if not chk:
            affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

        try:
            faculty = int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))[0])
        except:
            faculty =  None

        social = soup.find_all('div', class_='social-icon-left')[0]
        urls = []
        for li in social.find_all('li'):
            urls.append(li.a['href'])
        face = list(filter(lambda x: 'face' in x, urls))
        twit = list(filter(lambda x: 'twit' in x, urls))
        you = list(filter(lambda x: 'youtube' in x, urls))
        wiki = list(filter(lambda x: 'wiki' in x, urls))
        face, twit, you, wiki = list(map(lambda x: x[0] if len(x) > 0 else None, [face, twit, you, wiki]))

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream, faculty, face, twit, you, wiki]

#     ''':streams[stream].shape[0]]'''
149/1:
from collections import Counter, defaultdict
from bs4 import BeautifulSoup as b
from threading import Thread
from tqdm import tqdm
import requests as r
import pandas as pd
import numpy as np
import re
149/2:
streams = defaultdict()
streams['medical'] = pd.read_csv('../Kunal_mereexams/medical/medical_courses.csv')
streams['eng'] = pd.read_csv('../Kunal_mereexams/engineering/courses_all.csv')
streams['law'] = pd.read_csv('../Kunal_mereexams/law/law_courses.csv')
streams['design'] = pd.read_csv('../Kunal_mereexams/design/design_courses.csv')
streams['it'] = pd.read_csv('../Kunal_mereexams/it_colleges/it_list.csv')
streams['hotel'] = pd.read_csv('../Kunal_mereexams/hotel_manag/hotel_list.csv')
streams['pharmacy'] = pd.read_csv('../Kunal_mereexams/pharmacy/pharmacy_courses.csv')
streams['management'] = pd.read_csv('../Kunal_mereexams/management/mang_list.csv')
streams['media'] = pd.read_csv('../Kunal_mereexams/media_journalism/media_courses.csv')
data = defaultdict()
149/3:
for stream in streams.keys():
    data[stream] = pd.DataFrame(columns=['Id', 'Name','known_as', 'Estd','State', 'City', 'Address', 'ownership','Approved_by','Affilate_to','contact', 'Rank','website', 'score', 'facilities', 'exams', 'stream', 'faculty', 'facebook', 'twitter','youtube', 'wiki'], index=range(streams[stream].shape[0]))
    for i in tqdm(range(streams[stream].shape[0])):
        link =  streams[stream].iloc[i,1]
        try:
            res = r.get(link)
        except:
            res = None
            continue
        soup = b(res.content, 'html.parser')

        try:
            also_as = soup.find_all('div',class_='infoQuestion')[0].get_text()[15:]
        except:
            also_as = None

        try:
            name = soup.find_all('h1',class_='titleNameCol')[0].get_text()
        except:
            name = None

        l1 = soup.find_all('ul',class_='clg-info')[0]
        lsts = []
        for li in l1.findAll('li'):
            lsts.append(li.get_text())

        try:
            city, state = lsts[0].split(',')
        except:
            city, state = None,None

        chks = ['Estd : ','Ownership : ','Approved By : ','Affiliated To : ',]
    #     for lst in lsts[1:]:
    #       print(re.sub(chks[0],'',lst))

        estb = None
        ownership = None
        approved_by = None
        affilated_to = None
        chk = False
        for lst in lsts[1:]:
            if chks[0] in lst:
                estb = re.sub(chks[0],'',lst)
                #         print(estb)
                chk = True

    #     print(not chk)
        if (not chk):
            estb = None

        chk = False
        for lst in lsts[1:]:
            if chks[1] in lst:
                ownership = re.sub(chks[1],'',lst)
                #         print(ownership)
                chk = True
        if(not chk):
            ownership = None

        chk = False
        for lst in lsts[1:]:
            if chks[2] in lst:
                approved_by = re.sub(chks[2],'',lst)
                chk = True
        if not chk:
            approved_by = None

        chk = False
        for lst in lsts[1:]:
            if chks[3] in lst:
                affilated_to = re.sub(chks[3],'',lst)
                chk = True
        if not chk:
            affilated_to = None

        try:
            add = soup.find_all('div',class_='mapInfoColInner')[0].find('br').next_sibling
        except:
            add = None      

        l3 = soup.find_all('ul',class_='clg-info')[1].li

        try:
            website = l3 = soup.find_all('ul',class_='clg-info')[1].li.get_text()
        except:
            website = None
        try:
            contact = l3 = soup.find_all('ul',class_='clg-info')[1].li.next_sibling.next_sibling.get_text()[10:]
        except:
            contact = None

        try:
            rank = int(str(soup.find_all('div', class_='countBlockRayco')[0].span)[6:7])
        except:
            rank = None   
        try:
            score = float(soup.find_all('div', class_='countBlockRayco')[1].span.get_text())
        except:
            score = None
        try:
            facilities = []
            facility_list = soup.find('div', class_ = 'facilitylist')
            facility_list.find('ul')
            for li in facility_list.find_all('li'):
                facilities.append(li.get_text())
        except:
            None

        try:
            ex = soup.find_all(class_='college-admission-table')[0].table.tbody
            exams = []
            for tr in ex.find_all('tr'):
                exams.append(tr.td.get_text())
        except:
            None

        try:
            faculty = int(re.findall(r'\d+',str(re.findall(r'Total Faculty In College: \d+',str(soup))))[0])
        except:
            faculty =  None

        social = soup.find_all('div', class_='social-icon-left')[0]
        urls = []
        for li in social.find_all('li'):
            urls.append(li.a['href'])
        face = list(filter(lambda x: 'face' in x, urls))
        twit = list(filter(lambda x: 'twit' in x, urls))
        you = list(filter(lambda x: 'youtube' in x, urls))
        wiki = list(filter(lambda x: 'wiki' in x, urls))
        face, twit, you, wiki = list(map(lambda x: x[0] if len(x) > 0 else None, [face, twit, you, wiki]))

    #     print([i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream])
        data[stream].loc[i] = [i, name, also_as, estb, state, city, add, ownership, approved_by, affilated_to, contact, rank, website, score, facilities, exams, stream, faculty, face, twit, you, wiki]

#     ''':streams[stream].shape[0]]'''
152/1:
import pandas as pd
import numpy as np
152/2: lst = ['iitd','Hostel','Vellore','Tamil Nadu','fees','mba','private','finance','ggsipu']
152/3: len(lst)
152/4: arr = []
152/5:
import pandas as pd
import numpy as np
from itertools import combinations
152/6: lst = ['','iitd','Hostel','Vellore','Tamil Nadu','fees','mba','private','finance','ggsipu']
152/7: lst = ['iitd','Hostel','Vellore','Tamil Nadu','fees','mba','private','finance','ggsipu']
152/8: combination??
152/9:
import pandas as pd
import numpy as np
from itertools import combinations
152/10: combinations??
152/11: combinations([1,2,3])
152/12: combinations([1,2,3], (4,5))
152/13: combinations([1,2,3], 4,5)
152/14:
import pandas as pd
import numpy as np
from itertools import combinations as cb
153/1:
import pandas as pd
import numpy as np
from itertools import combinations as cb
153/2: lst = ['iitd','Hostel','Vellore','Tamil Nadu','fees','mba','private','finance','ggsipu']
153/3:
arr = []
for i in range(len(lst)):
    arr.extend(cb(lst, i))
153/4: arr
154/1:
import pandas as pd
import numpy as np
from itertools import combinations as cb
154/2: lst = ['iitd','Hostel','Vellore','Tamil Nadu','fees','mba','private','finance','ggsipu']
154/3:
arr = []
for i in range(1,len(lst)):
    arr.extend(cb(lst, i))
154/4: arr
154/5: *('iitd', 'Hostel').join(' ')
154/6: ('iitd', 'Hostel').join(' ')
154/7: list(('iitd', 'Hostel')).join(' ')
154/8: ('iitd', 'Hostel').join(' ')
154/9:
a =['d', 'd']
a.join(' ')
154/10: ' '.join(('iitd', 'Hostel'))
154/11: sentences = list(map(lambda x: ' '.join(x), arr))
154/12: sentences
155/1:
import pandas as pd
import numpy as np
from itertools import combinations as cb
155/2: lst = ['iitd','Hostel','Vellore','Tamil Nadu','fees','mba','private','finance','ggsipu']
155/3:
arr = []
for i in range(1,len(lst)):
    arr.extend(cb(lst, i))
155/4: sentences = list(map(lambda x: ' '.join(x), arr))
155/5: sentences
159/1:
import pandas as pd
import re
import dask.dataframe as dd
from dask.multiprocessing import get
159/2:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|and|&', '', name, flags = re.IGNORECASE)[0]
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]))
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
        
    return list(set(names))

syn_gen("L.K.KHOT ARTS COLLEGE, HEBBAL")
159/3:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|and|&', '', name, flags = re.IGNORECASE)[0]
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
        
    return list(set(names))

syn_gen("L.K.KHOT ARTS COLLEGE, HEBBAL")
159/4:
data = pd.read_csv('../all_colleges_mereexams.csv').astype(str)
ddata = dd.from_pandas(data['name'], npartitions=2)
ddata.map_partitions(lambda df: df.apply(syn_gen), meta=pd.DataFrame).compute(get=get)
159/5:
data = pd.read_csv('../all_institutions.csv').astype(str)
ddata = dd.from_pandas(data['name'], npartitions=2)
ddata.map_partitions(lambda df: df.apply(syn_gen), meta=pd.DataFrame).compute(get=get)
159/6:
data = pd.read_csv('../all_institutions.csv').astype(str)
ddata = dd.from_pandas(data['name'], npartitions=2)
data['col_synonyms'] = ddata.map_partitions(lambda df: df.apply(syn_gen), meta=pd.DataFrame).compute(get=get)
159/7: data.to_csv('../col_synonym_data.csv')
159/8:
import pandas as pd
import re
import dask.dataframe as dd
from dask.multiprocessing import get
159/9:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|and|&', '', name, flags = re.IGNORECASE)[0]
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
        
    return list(set(names))

syn_gen("L.K.KHOT ARTS COLLEGE, HEBBAL")
159/10: data.columns
159/11: data['Affiliated_to_University'].head(50)
159/12: data['Affiliated_to_University'].unique
159/13: data['Affiliated_to_University'].unique()
159/14:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|and|&', '', name, flags = re.IGNORECASE)[0]
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
        
    return list(set(names))

syn_gen("Sankalchand Patel University, Visnagar")
159/15:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
        
    return list(set(names))

syn_gen("Sankalchand Patel University, Visnagar")
159/16: data['T.M. Bhagalpur University, Bhagalpur'].unique()
159/17:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
        
    return list(set(names))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
159/18: data['Affiliated_to_University'].unique()
159/19:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    pattern = '\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?'
    print(pattern.findall(name)[0])
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(set(names))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
159/20:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    pattern = re.compile('\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?')
    print(pattern.findall(name)[0])
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(set(names))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
159/21:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    pattern = re.compile('\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?')
    inst_type = pattern.findall(name)[0]
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    names.append(names[-1]+' '+inst_type)
    
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(set(names))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
159/22:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    pattern = re.compile('\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?')
    inst_type = pattern.findall(name)[0]
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
        names.append(names[-1]+' '+inst_type)
    
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(set(names))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
159/23:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    pattern = re.compile('\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?')
    inst_type = pattern.findall(name)[0]
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
        names.append(names[-1]+inst_type)
    
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(set(names))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
159/24:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in univ_names:
    univ_name_synonyms.append([univ_name].extend(syn_gen(univ_name)))
159/25:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in univ_names:
    univ_name_synonyms.append([name].extend(syn_gen(name)))
159/26:
%debug
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in univ_names:
    univ_name_synonyms.append([name].extend(syn_gen(name)))
162/1:
import pandas as pd
import re
import dask.dataframe as dd
from dask.multiprocessing import get
162/2:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    pattern = re.compile('\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?|\sres\S*\sfound\S*\s?')
    inst_type = pattern.findall(name)[0]
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
        names.append(names[-1]+inst_type)
    
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(set(names))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
162/3:
import pandas as pd
import re
import dask.dataframe as dd
from dask.multiprocessing import get
from tqdm import tqdm
162/4:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    univ_name_synonyms.append([name].extend(syn_gen(name)))
162/5:
data = pd.read_csv('../all_institutions.csv').astype(str)
# ddata = dd.from_pandas(data['name'], npartitions=2)
# data['col_synonyms'] = ddata.map_partitions(lambda df: df.apply(syn_gen), meta=pd.DataFrame).compute(get=get)
162/6:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    univ_name_synonyms.append([name].extend(syn_gen(name)))
162/7:
%debug
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    univ_name_synonyms.append([name].extend(syn_gen(name)))
163/1:
import pandas as pd
import re
import dask.dataframe as dd
from dask.multiprocessing import get
from tqdm import tqdm
163/2:
def syn_gen(col_name):
    if col_name == 'nan':
        return ['nan']
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    pattern = re.compile('\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?|\sres\S*\sfound\S*\s?')
    inst_type = pattern.findall(name)[0]
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
        names.append(names[-1]+inst_type)
    
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(set(names))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
163/3:
data = pd.read_csv('../all_institutions.csv').astype(str)
# ddata = dd.from_pandas(data['name'], npartitions=2)
# data['col_synonyms'] = ddata.map_partitions(lambda df: df.apply(syn_gen), meta=pd.DataFrame).compute(get=get)
163/4:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    univ_name_synonyms.append([name].extend(syn_gen(name)))
163/5:
def syn_gen(col_name):
    if col_name == 'nan':
        return ['nan']
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
        pattern = re.compile('\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?|\sres\S*\sfound\S*\s?')
        inst_type = pattern.findall(name)[0]
        names.append(names[-1]+inst_type)
    
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(set(names))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
163/6:
def syn_gen(col_name):
    if col_name == 'nan':
        return ['nan']
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
        
        try:
            pattern = re.compile('\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?|\sres\S*\sfound\S*\s?')
            inst_type = pattern.findall(name)[0]
            names.append(names[-1]+inst_type)
        except:
            pass
        
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(set(names))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
163/7:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    univ_name_synonyms.append([name].extend(syn_gen(name)))
163/8: univ_names
163/9: univ_name_synonyms
163/10:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    print([name].extend(syn_gen(name)))
163/11:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    print([name])
163/12:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    print([name].extend())
163/13:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    print([name].extend(syn_gen(name)))
163/14:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    print([name],syn_gen(name))
163/15:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    print(name,'\t',syn_gen(name))
163/16:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    print(name,syn_gen(name), sep = '\t', end='\n')
163/17:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    print(name,syn_gen(name), sep = '\t', end='\n\n')
163/18:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|and|&', '', name, flags = re.IGNORECASE)[0]
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
        
    return list(set(names))

syn_gen("VVS'S ARTS/COMM COLLEGE,  BIJAPUR
")
163/19:
def syn_gen(col_name):
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|and|&', '', name, flags = re.IGNORECASE)[0]
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
        
    return list(set(names))

syn_gen("VVS'S ARTS/COMM COLLEGE,  BIJAPUR")
163/20:
def syn_gen(col_name):
    if col_name == 'nan':
        return ['nan']
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
        
        try:
            pattern = re.compile('\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?|\sres\S*\sfound\S*\s?')
            inst_type = pattern.findall(name)[0]
            names.append(names[-1]+inst_type)
        except:
            pass
        
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(filter(None,set(names)))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
169/1:
import pandas as pd
import re
import dask.dataframe as dd
from dask.multiprocessing import get
from tqdm import tqdm
169/2:
def syn_gen(col_name):
    if col_name == 'nan':
        return ['nan']
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
        
        try:
            pattern = re.compile('\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?|\sres\S*\sfound\S*\s?')
            inst_type = pattern.findall(name)[0]
            names.append(names[-1]+inst_type)
        except:
            pass
        
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(filter(None,set(names)))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
169/3:
data = pd.read_csv('../all_institutions.csv').astype(str)
# ddata = dd.from_pandas(data['name'], npartitions=2)
# data['col_synonyms'] = ddata.map_partitions(lambda df: df.apply(syn_gen), meta=pd.DataFrame).compute(get=get)
169/4:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    univ_name_synonyms.append(name,syn_gen(name), sep = '\t', end='\n\n')
169/5:
univ_name_synonyms = []
univ_names = data['Affiliated_to_University'].unique()
for name in tqdm(univ_names):
    univ_name_synonyms.append([name].append(syn_gen(name)))
169/6: univ_name_synonyms
169/7: from csv import writer
169/8:
from csv import writer
univ_names = open('univ_names', 'wb+')
writer = writer(univ_names)
univ_name_synonyms = []
univ_names = list(data['Affiliated_to_University'].unique())
for name in tqdm(univ_names):
#     writer.writerow([name].extend(syn_gen(name)))
    univ_name_synonyms.append([name].append(syn_gen(name)))
169/9: univ_name_synonyms
169/10:
from csv import writer
univ_names = open('univ_names', 'wb+')
writer = writer(univ_names)
univ_name_synonyms = []
univ_names = list(data['Affiliated_to_University'].unique())
for name in tqdm(univ_names):
#     writer.writerow([name].extend(syn_gen(name)))
    univ_name_synonyms.append([name].extend(syn_gen(name)))
169/11: univ_name_synonyms
169/12:
from csv import writer
univ_names = open('univ_names', 'wb+')
writer = writer(univ_names)
univ_name_synonyms = []
univ_names = list(data['Affiliated_to_University'].unique())
for name in tqdm(univ_names):
#     writer.writerow([name].extend(syn_gen(name)))
    univ_name_synonyms.append([name]+syn_gen(name))
169/13: univ_name_synonyms
169/14: writer.writerow(univ_name_synonyms)
169/15:
from csv import writer
univ_names = open('univ_names', 'w+')
writer = writer(univ_names)
writer.writerow(univ_name_synonyms)
169/16: pd.read_csv('univ_names')
169/17:
from csv import writer
univ_names = open('univ_names', 'w+')
writer = writer(univ_names)
writer.writerows(univ_name_synonyms)
169/18: pd.read_csv('univ_names')
166/1: ls
169/19:
def rm_brk(s):
    return re.sub('\(.*\)','',str(s))
clgs = pd.DataFrame(data.name)
169/20: names = pd.DataFrame(clgs['name'].apply(syn_gen))
169/21: names.iloc[0]
169/22: names.iloc[0]['name']
169/23: names.iloc[:]['name']
169/24: names.name.values.tolist()
169/25: names
169/26: pd.DataFrame(names.name.values.tolist(), index= names.index)
169/27: a = pd.DataFrame(names.name.values.tolist(), index= names.index)
169/28:
names[list(range(25))] = pd.DataFrame(names.name.values.tolist(), index= names.index)
names.head()
169/29: names = a
169/30: names
169/31:
univ_name_synonyms = []
univ_names = list(data['Affiliated_to_University'].unique())
for name in tqdm(univ_names):
    univ_name_synonyms.append([name]+syn_gen(name))
169/32: pd.DataFrame(univ_name_synonyms)
169/33:
def syn_gen(col_name):
    if col_name == 'nan':
        return ['nan']
    col_name = re.sub('\(.*\)','',col_name)
    col_name = col_name.lower()
    name = col_name.split(',')[0]
    names = []
    names.append(name)                                           # As given
    
    name = re.subn('of|\sand|&', '', name, flags = re.IGNORECASE)[0]
    
    name = re.subn('\s+', ' ', name)[0]                          # Removing 'of' and 'and'
    names.append(name)
    names.append(''.join([word[0] for word in name.split()]).upper())
    
    if "'" in col_name:    
        names.append(''.join([word[0] for word in filter(None,re.split('\W+', name))]).upper())
    if col_name.count('.') > 1:
        names.append(''.join(re.split(r'\.\s?', col_name)[:-1]).upper())
        
        try:
            pattern = re.compile('\s?univ\S*\s?|\s?coll\S*\s?|\s?inst\S*\s?|\sres\S*\sfound\S*\s?')
            inst_type = pattern.findall(name)[0]
            names.append(names[-1]+inst_type)
        except:
            pass
        
    if ',' in col_name:
        location_full = [name+re.findall(',.*', col_name)[0][1:] for name in names]
        try:
            location_short = [name+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)]
            location_short.extend([name+'-'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+'/'+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            location_short.extend([name+' '+col_name.split(',')[-1].split()[0][0].upper() for name in filter(lambda x: x.isupper(), names)])
            names.extend(location_short)
        except:
            pass
        names.extend(location_full)
    return list(filter(None,set(names)))

syn_gen("T.M. Bhagalpur University, Bhagalpur")
169/34:
univ_name_synonyms = []
univ_names = list(data['Affiliated_to_University'].unique())
for name in tqdm(univ_names):
    univ_name_synonyms.append([name]+syn_gen(name))
169/35: len(list(data['Affiliated_to_University'].unique()))
169/36: names = pd.DataFrame(univ_name_synonyms)
169/37: names.head()
169/38: names[:,0]
169/39: names.iloc[:,0]
169/40: names.iloc[:,0] = list(data['Affiliated_to_University'].unique())
169/41: names.head()
169/42: names = pd.DataFrame(univ_name_synonyms)
169/43: names.head()
169/44: names.to_csv('univ_names.csv')
169/45: names[0].to_csv()
169/46: names.iloc[:,0].to_csv()
169/47: names.iloc[0].to_csv()
169/48:
def comma(s):
    if str(s) != str(None):
        return '"'+s+'"'
    return None
169/49: comma('hello')
169/50:
import csv
names.to_csv('univ_names.csv', index = None, quoting=csv.QUOTE_NONE)
169/51:
import csv
names.to_csv('univ_names.csv', index = None, quoting=csv.QUOTE_NONE, escapechar='\\')
169/52:
import csv
names.to_csv('univ_names.csv', index = None, quoting=csv.QUOTE_NONE, escapechar=' ')
166/2: clgs.head()
166/3: data = pd.read_csv('CollegeName_entity.csv')
166/4:
import pandas as pd
import numpy as np
import re
import dask.dataframe as dd
from dask.multiprocessing import get
from tqdm import tqdm
166/5: data = pd.read_csv('CollegeName_entity.csv')
166/6: pd.to_csv(data.iloc[:,15000],header=None,index=None)
166/7: data.iloc[:,15000].to_csv('CollgeName1',header=None,index=None)
166/8: data.shape
166/9: data.iloc[:,15000]
166/10: data.iloc[:15000].to_csv('CollgeName1',header=None,index=None)
166/11: data.iloc[15000:].to_csv('CollegeName2',header=None,index=None)
174/1: import numpy as np
174/2: import pandas as pd
174/3: data = pd.read_csv('all_institutions_course.csv')
174/4: data.head()
174/5: data['discipline'].unique()
174/6: len(data['discipline'].unique())
174/7: len(data['discipline_group'].unique())
174/8: len(data['discipline_group_category'].unique())
174/9: def brack(st):
174/10: def brak(st):
174/11: discipline = {}for
174/12: discipline = {}
174/13: for discipline in data.iterrows():
174/14: for disciplin in data.iterrows():
178/1:
import pandas as pd
import re
import dask.dataframe as dd
from dask.multiprocessing import get
from tqdm import tqdm
178/2:
def syn_specialization(spec):
    spec = re.sub('(', '[', spec)
    return = re.sub(')', ']', spec)
178/3:
def syn_specialization(spec):
    spec = re.sub('(', '[', spec)
    return re.sub(')', ']', spec)
178/4: syn_specialization('hel(sl)ls')
178/5:
def syn_specialization(spec):
    spec = re.sub('\(', '\[', spec)
    return re.sub('\)', '\]', spec)
178/6: syn_specialization('hel(sl)ls')
178/7:
def syn_specialization(spec):
    spec = re.sub('\(', '[', spec)
    return re.sub('\)', ']', spec)
178/8: syn_specialization('hel(sl)ls')
178/9:
def syn_specialization(spec):
    specs = []
    spec = re.sub('\)', ']', s[ec)
    specs.append(re.sub('\(', '[', spec))
    specs.append(re.sub('\(', '-', spec))
    specs.append(re.sub('\(', ' ', spec))
    return specs
syn_specialization('hel(sl)ls')
178/10:
def syn_specialization(spec):
    specs = []
    spec = re.sub('\)', ']', spec)
    specs.append(re.sub('\(', '[', spec))
    specs.append(re.sub('\(', '-', spec))
    specs.append(re.sub('\(', ' ', spec))
    return specs
syn_specialization('hel(sl)ls')
178/11:
def syn_specialization(spec):
    specs = []
    specy = re.sub('\)', ']', spec)
    spec = re.sub('\)', '', spec)
    specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\(', '-', spec))
    specs.append(re.sub('\(', ' ', spec))
    return specs
syn_specialization('hel(sl)ls')
178/12:
data = pd.read_csv('../all_institutions_course_updated.csv').astype(str)
# ddata = dd.from_pandas(data['name'], npartitions=2)
# data['col_synonyms'] = ddata.map_partitions(lambda df: df.apply(syn_gen), meta=pd.DataFrame).compute(get=get)
178/13: data.columns
178/14: data.head()
178/15:
def change_brk(s):
    s = str(s)
    s.replace('(','[')
    s.replace(')',']')
178/16:
def change_brk(s):
    s = str(s)
    s = s.replace('(','[')
    return s.replace(')',']')
178/17: change_brk('kunal(be)')
178/18: change_brk('kunal')
178/19:
def syn_specialization(spec):
    specs = []
    specy = re.sub('\)', ']', spec)
    spec = re.sub('\)', '', spec)
    specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\(', '-', spec))
    specs.append(re.sub('\(', ' ', spec))
    return specs
syn_specialization('hel[sl]ls')
178/20:
def syn_specialization(spec):
    specs = []
    specy = re.sub('\)', ']', spec)
    spec = re.sub('\)', '', spec)
    specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\(', '-', spec))
    specs.append(re.sub('\(', ' ', spec))
    return specs
syn_specialization('hel(sl)ls')
178/21:
def syn_specialization(spec):
    specs = []
    specy = re.sub('\)', ']', spec)
    spec = re.sub('\)', '', spec)
    specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\(', '-', spec))
    specs.append(re.sub('\(', ' ', spec))
    return specs
syn_specialization('helslls')
178/22:
def syn_specialization(spec):
    specs = []
    specy = re.sub('\)', ']', spec)
    spec = re.sub('\)', '', spec)
    specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\(', '-', spec))
    specs.append(re.sub('\(', ' ', spec))
    return specs
syn_specialization('hel(sll)s')
178/23:
def syn_specialization(spec):
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
178/24: data[['discipline_group','discipline']]
178/25: data[['discipline','discipline_group']]
178/26: data[['discipline','discipline_group']].apply(change_brk)
178/27: data[['discipline','discipline_group']]=data[['discipline','discipline_group']].apply(change_brk)
178/28: data[['discipline']]=data[['discipline']].apply(change_brk)
178/29: data['discipline']=data['discipline'].apply(change_brk)
178/30: data['discipline_group'] = data['discipline_group'].apply(change_brk)
178/31: data.head()
178/32: data['discipline'].isnull()
178/33: sum(data['discipline'].isnull())
178/34: sum(data['discipline_group'].isnull())
178/35: sum(data['discipline_group']=='nan')
178/36: sum(data['discipline']=='nan')
178/37: data[data['discipline']=='nan']
178/38: data[data['discipline']=='nan']['discipline']  = data[data['discipline']=='nan']['discipline_group']
178/39: data['discipline'].str.len
178/40: data[data['discipline'].str.len()<2]
178/41: data[data['discipline'].str.len()<2].shape
178/42: data[data['discipline'].str.len()<2]
178/43: data[data['discipline'].str.len()<2]['discipline'] = data[data['discipline'].str.len()<2]['discipline_group']
178/44: data[data['discipline'].str.len()<3]
178/45: data[data['discipline'].str.len()<2]
178/46: data.loc[data['discipline'].str.len()<2]['discipline'] = data.loc[data['discipline'].str.len()<2]['discipline_group']
178/47: data.loc[data['discipline'].str.len()<2,'discipline']
178/48: data.loc[data['discipline'].str.len()<2,'discipline'] = data.loc[data['discipline'].str.len()<2,'discipline_group']
178/49: data.loc[data['discipline']=='nan','discipline']  = data.loc[data['discipline']=='nan','discipline_group']
178/50: data[data['discipline'].str.len()<2]
178/51: data[data['discipline'].str.len()<3]
178/52: data['Unnamed: 0']
178/53: data.drop('Unnamed: 0',axis=1)
184/1: data.head()
184/2:
import pandas as pd
import re
import dask.dataframe as dd
from dask.multiprocessing import get
from tqdm import tqdm
184/3:
def syn_specialization(spec):
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
184/4:
def change_brk(s):
    s = str(s)
    s = s.replace('(','[')
    return s.replace(')',']')
184/5: change_brk('kunal')
184/6:
data = pd.read_csv('../all_institutions_course_updated.csv').astype(str)
# ddata = dd.from_pandas(data['name'], npartitions=2)
# data['col_synonyms'] = ddata.map_partitions(lambda df: df.apply(syn_gen), meta=pd.DataFrame).compute(get=get)
184/7: data['discipline']=data['discipline'].apply(change_brk)
184/8: data['discipline_group'] = data['discipline_group'].apply(change_brk)
184/9: data.head()
184/10: data.columns
184/11: data.loc[data['discipline']=='nan','discipline']  = data.loc[data['discipline']=='nan','discipline_group']
184/12: data.loc[data['discipline'].str.len()<2,'discipline'] = data.loc[data['discipline'].str.len()<2,'discipline_group']
184/13: data.head()
184/14: data.drop('Unnamed: 0',axis=1)
184/15: data = data.drop('Unnamed: 0',axis=1)
184/16: data.to_csv('all_institution_courses_cleaned.csv')
179/1:
import numpy as np
import pandas as pd
179/2: data = pd.read_csv('all_institution_courses_cleaned.csv')
179/3: data.head()
179/4: data.head()
179/5: data['discipline'].unique()
179/6: (data['discipline'].unique()).len
179/7: (data['discipline'].unique()).length()
179/8: len(data['discipline'].unique())
179/9: len(data['discipline_group'].unique())
179/10:
def syn_specialization(spec):
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
179/11:
import numpy as np
import pandas as pd
import re
179/12:
def syn_specialization(spec):
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
179/13: cdata = pd.concat([data['discipline'],data['discipline_group']],axis = 1)
179/14: cdata.head()
179/15: cdata = pd.concat([data['discipline'],data['discipline_group']],axis = 0)
179/16: cdata.head()
179/17: cdata.shape
179/18: cdata = pd.concat([data['discipline'].unique(),data['discipline_group'].unique()],axis = 0)
179/19: cdata = data['discipline'].unique()+data['discipline_group'].unique()
187/1:
import numpy as np
import pandas as pd
import re
187/2:
import numpy as np
import pandas as pd
187/3:
import numpy as np
import pandas as p
import re
187/4: data = pd.read_csv('all_institution_courses_cleaned.csv')
187/5: data.head()
187/6: list(data.discipline)+list(data.discipline_group)
187/7: data.discipline.isnull()
187/8: sum(data.discipline.isnull())
187/9: sum(data.discipline_group.isnull())
187/10: sum(data.discipline_group=='nan')
187/11: sum(data.discipline_group==nan)
187/12: sum(data.discipline_group==np.nan)
187/13: sum(data.discipline_group=='N.A.')
187/14: sum(data.discipline=='N.A.')
187/15:
data.drop('Unnamed: 0')
sum(data.discipline=='N.A.')
187/16:
data.drop('Unnamed: 0',axis=1)
sum(data.discipline=='N.A.')
187/17:
data = data.drop('Unnamed: 0',axis=1)
sum(data.discipline=='N.A.')
187/18:
data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='N.A.')
187/19: data.to_csv('all_institution_courses_cleaned.csv')
187/20: list(data.discipline)+list(data.discipline_group)
187/21: pd.DataFrame(list(data.discipline)+list(data.discipline_group))
187/22: data = pd.DataFrame(list(data.discipline)+list(data.discipline_group))[0].unique()
187/23: data
187/24: data = pd.DataFrame(data)
187/25: data.head()
187/26:
def syn_specialization(spec):
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
187/27:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
187/28:
data[1] = data[0]
data.head()
187/29:
data[1] = data[0]
data[2] = data[1]
data.head()
187/30: data.shape
187/31: data[2] = data[2].apply(syn_specialization)
187/32: data.head()
187/33:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    if '[' in spec:
        specs.append(re.sub('\[', '-', spec))
        specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
187/34:
data[1] = data[0]
data[2] = data[1]
data.head()
187/35: data[2] = data[2].apply(syn_specialization)
187/36: data.head()
187/37: data.tail()
187/38: data[0]
187/39: data.iloc[0]
187/40: data.iloc[0][2][1]
187/41:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
187/42:
data[1] = data[0]
data[2] = data[1]
data.head()
187/43: data[2] = data[2].apply(syn_specialization)
187/44: data.iloc[0][2][1]
187/45: data.head()
187/46:
def ret_1(lst):
    lst = list(lst)
    return lst[1]
187/47: data[3] = data[2].apply(ret_0)
187/48:
def ret_0(lst):
    lst = list(lst)
    return lst[0]
187/49: data[3] = data[2].apply(ret_0)
187/50: data.head()
187/51: data[2] = data[2].apply(ret_1)
187/52: data.head()
187/53:
def ad_comma(st):
    st = str(st)
    return '"' + st + '"'
187/54: data.apply(ad_comma)
187/55: data1 = data.apply(ad_comma)
187/56: data1.head()
187/57: data1
187/58:
for i in range(4):
    data[i] = data[i].apply(ad_comma)
187/59: data.head()
187/60: data.to_csv('entity_specs.csv',header=None,index=None)
187/61:
import numpy as np
import pandas as p
import re
187/62:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
187/63: data = pd.read_csv('all_institution_courses_cleaned.csv')
187/64: data.head()
185/1: pd
185/2:
import pandas as pd
import re
import dask.dataframe as dd
from dask.multiprocessing import get
from tqdm import tqdm
185/3: data = pd.read_csv('all_institution_courses_cleaned.csv')
185/4: data.head()
185/5: data['discipline'].str.lower()
188/1: data.head()
188/2:
import numpy as np
import pandas as p
import re
188/3:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
188/4: data = pd.read_csv('all_institution_courses_cleaned.csv')
188/5: data.head()
188/6:
import numpy as np
import pandas as pd
import re
188/7:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
188/8: data = pd.read_csv('all_institution_courses_cleaned.csv')
188/9: data.head()
188/10: data = data.drop('Unnamed: 0',axis=1)
188/11:
data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='N.A.')
188/12: data['discipline']=data['discipline'].str.lower()
188/13:
import numpy as np
import pandas as pd
import re
188/14:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
188/15: data = pd.read_csv('all_institution_courses_cleaned.csv')
188/16: data.head()
188/17: data = data.drop('Unnamed: 0',axis=1)
188/18:
data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='N.A.')
188/19: data['discipline_lower']=data['discipline'].str.lower()
188/20: len(data['discipline_group'].str.lower().unique())
188/21: len(data['discipline_group'].unique())
188/22: data['discipline_group_lower'] = data['discipline_group'].str.lower()
188/23: data.to_csv('all_institution_courses_cleaned_lowered.csv')
188/24: combined_spec = pd.DataFrame(list(data.discipline_lower)+list(data.discipline_group_lower))[0].unique()
188/25: combined_spec.head()
188/26: combined_spec = pd.DataFrame(combined_spec)
188/27:
combined_spec[1] = combined_spec[0]
combined_spec[2] = combined_spec[1]
combined_spec.head()
188/28: combined_spec.shape
188/29: combined_spec[2] = combined_spec[2].apply(syn_specialization)
188/30:
def ret_0(lst):
    lst = list(lst)
    return lst[0]
188/31:
def ret_1(lst):
    lst = list(lst)
    return lst[1]
188/32: combined_spec[3] = combined_spec[2].apply(ret_0)
188/33: combined_spec[2] = combined_spec[2].apply(ret_1)
188/34: combined_spec.head()
188/35:
def ad_comma(st):
    st = str(st)
    return '"' + st + '"'
188/36:
for i in range(4):
    combined_spec[i] = combined_spec[i].apply(ad_comma)
188/37: data.head()
188/38: combined_spec.head()
188/39: combined_spec.to_csv('entity_specs.csv',header=None,index=None)
190/1:
import pandas as pd
import re
import dask.dataframe as dd
from dask.multiprocessing import get
from tqdm import tqdm
190/2:
def syn_specialization(spec):
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
191/1: data = pd.read_csv('all_institution_courses_cleaned.csv')
191/2: data.head()
191/3:
import numpy as np
import pandas as pd
import re
191/4:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
191/5: data = pd.read_csv('all_institution_courses_cleaned.csv')
191/6: data.head()
191/7: data = data.drop('Unnamed: 0',axis=1)
191/8: data = pd.read_csv('all_institution_courses_cleaned_lowered.csv')
191/9: data.head()
191/10: data = pd.read_csv('all_institution_courses_cleaned.csv')
191/11: data.head()
191/12: data = data.drop('Unnamed: 0',axis=1)
191/13:
data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='N.A.')
191/14: data.to_csv('all_institution_courses_cleaned_lowered.csv')
191/15:
import numpy as np
import pandas as pd
import re
191/16:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
191/17: data = pd.read_csv('all_institution_courses_cleaned.csv')
191/18: data.head()
191/19: data = data.drop('Unnamed: 0',axis=1)
191/20:
data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='N.A.')
191/21: data['discipline_lower']=data['discipline'].str.lower()
191/22: data['discipline_group_lower'] = data['discipline_group'].str.lower()
191/23: data.to_csv('all_institution_courses_cleaned_lowered.csv')
191/24:
import numpy as np
import pandas as pd
import re
191/25:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
191/26:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def add_brk(spec):
    spec = str(spec)
    if '[' in spec and ']' not in spec:
        return spec + ']'
191/27: data = pd.read_csv('all_institution_courses_cleaned.csv')
191/28:
data['discipline'] = data['discipline'].apply(add_brk)
data.head()
191/29:
import numpy as np
import pandas as pd
import re
191/30:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs
syn_specialization('hel[sll]s')
191/31:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
191/32: data = pd.read_csv('all_institution_courses_cleaned.csv')
191/33: data.head()
191/34:
data['discipline'] = data.discipline.apply(ad_brk)
data.head()
191/35: data = data.drop('Unnamed: 0',axis=1)
191/36:
data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='N.A.')
191/37: data['discipline_lower']=data['discipline'].str.lower()
191/38: data['discipline_group_lower'] = data['discipline_group'].str.lower()
191/39: data.to_csv('all_institution_courses_cleaned_lowered.csv')
191/40: combined_spec = pd.DataFrame(list(data.discipline_lower)+list(data.discipline_group_lower))[0].unique()
191/41: combined_spec = pd.DataFrame(combined_spec)
191/42:
combined_spec[1] = combined_spec[0]
combined_spec[2] = combined_spec[1]
combined_spec.head()
191/43: combined_spec.shape
191/44: data.head()
191/45: data.shape
191/46: combined_spec.shape
191/47: len(list(data.discipline_lower)+list(data.discipline_group_lower))
191/48: len(pd.DataFrame(list(data.discipline_lower)+list(data.discipline_group_lower))[0].unique())
191/49: data = pd.read_csv('all_institution_courses_cleaned.csv')
191/50:
# data['discipline'] = data.discipline.apply(ad_brk)
data.head()
191/51:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
ad_brk('kunal[kd')
191/52:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
ad_brk('kunal]kd')
191/53: data = pd.read_csv('all_institution_courses_cleaned.csv')
191/54:
# data['discipline'] = data.discipline.apply(ad_brk)
data.head()
191/55:
import numpy as np
import pandas as pd
import re
191/56:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
ad_brk('kunal]kd')
191/57: data = pd.read_csv('all_institution_courses_cleaned.csv')
191/58:
# data['discipline'] = data.discipline.apply(ad_brk)
data.head()
191/59: data = data.drop('Unnamed: 0',axis=1)
191/60:
data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='N.A.')
191/61: data['discipline_lower']=data['discipline'].str.lower()
191/62: data['discipline_group_lower'] = data['discipline_group'].str.lower()
192/1: data = pd.read_csv('all_institution_courses_cleaned.csv')
192/2:
data['discipline'] = data.discipline.apply(ad_brk)
data.head()
192/3:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
ad_brk('kunal]kd')
192/4: data = pd.read_csv('all_institution_courses_cleaned.csv')
192/5:
import numpy as np
import pandas as pd
import re
192/6:
import numpy as np
import pandas as pd
import re
192/7:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
ad_brk('kunal]kd')
192/8: data = pd.read_csv('all_institution_courses_cleaned.csv')
192/9:
data['discipline'] = data.discipline.apply(ad_brk)
data.head()
192/10: data = data.drop('Unnamed: 0',axis=1)
193/1: import np as np
192/11: data.head()
192/12: len(data.discipline.unique())
192/13:
import numpy as np
import pandas as pd
import re
192/14:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
ad_brk('kunal]kd')
192/15: data = pd.read_csv('all_institution_courses_cleaned.csv')
192/16: data.shape
192/17: data['discipline']
192/18: data['discipline'].unique()
192/19: len(data['discipline'].unique())
192/20:
data['discipline'] = data.discipline.apply(ad_brk)
data.head()
192/21: len(data['discipline'].unique())
192/22:
import numpy as np
import pandas as pd
import re
192/23:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
ad_brk('kunal]kd')
192/24: data = pd.read_csv('all_institution_courses_cleaned.csv')
192/25: len(data['discipline'].unique())
192/26:
data['discipline'] = data['discipline'].apply(ad_brk)
data.head()
192/27: len(data['discipline'].unique())
192/28: data['discipline'].unique()
192/29:
import numpy as np
import pandas as pd
import re
192/30:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
ad_brk('kunal]kd')
192/31: data = pd.read_csv('all_institution_courses_cleaned.csv')
192/32: len(data['discipline'].unique())
192/33:
data['discipline'] = data['discipline'].apply(ad_brk)
data.head()
192/34: len(data['discipline'].unique())
192/35:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
    else return st
ad_brk('kunal]kd')
192/36:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
    else: return st
ad_brk('kunal]kd')
192/37: data = pd.read_csv('all_institution_courses_cleaned.csv')
192/38: len(data['discipline'].unique())
192/39:
data['discipline'] = data['discipline'].apply(ad_brk)
data.head()
192/40: len(data['discipline'].unique())
192/41:
data['discipline'] = data['discipline'].apply(ad_brk)
data.head()
192/42: data = data.drop('Unnamed: 0',axis=1)
192/43:
data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='N.A.')
192/44: len(data.discipline.unique())
192/45: data['discipline_lower']=data['discipline'].str.lower()
192/46: data['discipline_group_lower'] = data['discipline_group'].str.lower()
192/47: data.to_csv('all_institution_courses_lowered(new).csv')
192/48: data.shape
192/49: len(pd.DataFrame(list(data.discipline_lower)+list(data.discipline_group_lower))[0].unique())
192/50: combined_spec = (pd.DataFrame(list(data.discipline_lower)+list(data.discipline_group_lower))[0].unique())
192/51: combined_spec = pd.DataFrame(combined_spec)
192/52:
combined_spec[1] = combined_spec[0]
combined_spec[2] = combined_spec[1]
combined_spec.head()
192/53: combined_spec.shape
192/54: combined_spec[2] = combined_spec[2].apply(syn_specialization)
192/55:
def ret_0(lst):
    lst = list(lst)
    return lst[0]
192/56:
def ret_1(lst):
    lst = list(lst)
    return lst[1]
192/57: combined_spec[3] = combined_spec[2].apply(ret_0)
192/58: combined_spec[2] = combined_spec[2].apply(ret_1)
192/59: combined_spec.head()
192/60:
def ad_comma(st):
    st = str(st)
    return '"' + st + '"'
192/61:
for i in range(4):
    combined_spec[i] = combined_spec[i].apply(ad_comma)
192/62: combined_spec.head()
192/63: combined_spec.to_csv('entity_specs.csv',header=None,index=None)
194/1: import pandas as pd
194/2: import numpy as np
194/3: data  = pd.read_csv('MereExamsBot/all_institutions.csv')
194/4: data.columns
194/5: data.type.unique
194/6: data.type.unique()
194/7: data.loc[:, "Affiliated_to_University"]
194/8: data.loc[:, "Affiliated_to_University"].unique()
194/9: data.loc[:, "Affiliated_to_University":].head()
194/10: data.loc[:, "Affiliated_to_University":].columns
195/1:
import numpy as np
import pandas as pd
import re
195/2:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']',' ')
    else: return st
ad_brk('kunal]kd')
195/3: data = pd.read_csv('../all_institutions_course_updated-Copy1.csv')
195/4: data.head()
195/5: data.drop('Unnamed: 0')
195/6: data.drop('Unnamed: 0',axis=1)
195/7: data = data.drop('Unnamed: 0',axis=1)
195/8: data.head()
195/9: data.discipline.str.count(')')
195/10: data.discipline.str.count(")")
195/11: data.discipline.str.count("a")
195/12: data.discipline.str.count("()")
195/13: data.discipline.str.count(r'\()')
195/14: data.discipline.str.count(r'\(')
195/15: data.discipline.str.count(r'\(').max
195/16: data.discipline.str.count(r'\(').max()
195/17: data.discipline.str.count(r'\(')==10.0
195/18: data[data.discipline.str.count(r'\(')==10.0]
195/19: data[data.discipline.str.count(r'\(')==10.0].discipline
195/20: data[data.discipline.str.count(r'\(')==10.0]
195/21: data[data.discipline.str.count(r'\(')>2].shape
195/22: data[data.discipline.str.count(r'\)')>2].shape
195/23: data[data.discipline.str.count(r'\)')>2 and data.discipline.str.count(r'\(')<2].shape
195/24: data[np.logical_and(data.discipline.str.count(r'\)')>2 and data.discipline.str.count(r'\(')<2)].shape
195/25: data[np.logical_and(data.discipline.str.count(r'\)')>2 ,data.discipline.str.count(r'\(')<2)].shape
195/26: data[data.discipline.str.count(r'\(')<2].shape
195/27: data[data.discipline.str.count(r'\(')>2].shape
195/28: data[data.discipline.str.count(r'\(')>2].shape
195/29: data[np.logical_and(data.discipline.str.count(r'\)')>2 ,data.discipline.str.count(r'\(')>2)].shape
195/30: data[np.logical_and(data.discipline.str.count(r'\)')>2)].shape
195/31: data[(data.discipline.str.count(r'\)')>2)].shape
195/32: data[data.discipline.str.count(r'\[')>2].shape
195/33: data[(data.discipline.str.count(r'\]')>2)].shape
195/34: data[(data.discipline.str.count(r'\<')>0)].shape
195/35: data[(data.discipline.str.count(r'\>')>0)].shape
195/36: data[(data.discipline.str.count(r'\[]')>0)].shape
195/37: data[(data.discipline.str.count(r'\[')>0)].shape
195/38: data[(data.discipline.str.count(r'\]')>0)].shape
195/39: data[(data.discipline.str.count(r'\{')>0)].shape
195/40: data[(data.discipline.str.count(r'\}')>0)].shape
195/41: data[(data.discipline.str.count(r'\:')>0)].shape
195/42: data.drop(data.discipline.str.count(r'\(')>2,axis = 1)
195/43: data.drop(data.loc[data.discipline.str.count(r'\(')>2],axis = 0)
195/44: data.drop(data.discipline.str.count(r'\(')>2)
195/45: data.drop(data.discipline.str.count(r'\(')>2).shape
195/46: data.shape
195/47: (data.discipline.str.count(r'\(')>2)
195/48: data.drop(data[data.discipline.str.count(r'\(')>2].index)
195/49: data = data.drop(data[data.discipline.str.count(r'\(')>2].index)
195/50: data.drop(data[data.discipline.str.count(r'\)')>2].index)
195/51: data = data.drop(data[data.discipline.str.count(r'\)')>2].index)
195/52: data = data.drop(data[data.discipline.str.count(r'\<')>0].index)
195/53: data = data.drop(data[data.discipline.str.count(r'\>')>2].index)
195/54: data = data.drop(data[data.discipline.str.count(r'\:')>2].index)
195/55: data = data.drop(data[data.discipline.str.count(r'\>')>0].index)
195/56: data = data.drop(data[data.discipline.str.count(r'\:')>0].index)
195/57: data.shape
195/58: data[data.discipline=='nan'].shape
195/59: data[data.discipline=='N.A.'].shape
195/60: data[data.discipline.isnull()].shape
195/61: data.loc[data['discipline'].isnull(),'discipline']  = data.loc[data['discipline'].isnull(),'discipline_group']
195/62: data.loc[data['discipline']=='N.A.','discipline']  = data.loc[data['discipline']=='N.A.','discipline_group']
195/63: data[data['discipline'].str.len()<2].shape()
195/64: data[data['discipline'].str.len()<2].shape
195/65: data[data['discipline'].str.len()<1].shape
195/66: data[data['discipline'].str.len()<2]
195/67: list(data.discipline_lower)+list(data.discipline_group_lower)
195/68: data.head()
195/69: len(data.discipline.unique())
195/70: data['discipline_lower']=data['discipline'].str.lower()
195/71: data['discipline_group_lower'] = data['discipline_group'].str.lower()
195/72: data.shape
195/73:
import numpy as np
import pandas as pd
import re
195/74:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
ad_brk('kunal]kd')
195/75: data = pd.read_csv('../all_institutions_course_updated-Copy1.csv')
195/76: data.head()
195/77: data = data.drop(data[data.discipline.str.count(r'\(')>2].index)
197/1:
import numpy as np
import pandas as pd
import re
197/2:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
ad_brk('kunal]kd')
197/3: data = pd.read_csv('../all_institutions_course_updated-Copy1.csv')
197/4: data.head()
197/5: data = data.drop(data[data.discipline.str.count(r'\(')>2].index)
197/6: data = data.drop(data[data.discipline.str.count(r'\)')>2].index)
197/7: data = data.drop(data[data.discipline.str.count(r'\<')>0].index)
197/8: data = data.drop(data[data.discipline.str.count(r'\>')>0].index)
197/9: data = data.drop(data[data.discipline.str.count(r'\:')>0].index)
197/10: data.loc[data['discipline'].isnull(),'discipline']  = data.loc[data['discipline'].isnull(),'discipline_group']
197/11: data.loc[data['discipline']=='N.A.','discipline']  = data.loc[data['discipline']=='N.A.','discipline_group']
197/12: data[data['discipline'].str.len()<2]['dispcipline'] = data[data['discipline'].str.len()<2]['dispcipline']
197/13: data.shape
197/14: len(data['discipline'].unique())
197/15: data[data['discipline'].str.len()<2]['discipline'] = data[data['discipline'].str.len()<2]['dispcipline']
197/16: data[data['discipline'].str.len()<2]['discipline'] = data[data['discipline'].str.len()<2]['discipline']
197/17: data[data['discipline'].str.len()<2]['discipline'] = data[data['discipline'].str.len()<2]['discipline']
197/18: data.shape
197/19: len(data['discipline'].unique())
197/20:
data['discipline'] = data['discipline'].apply(ad_brk)
data.head()
197/21: data = data.drop('Unnamed: 0',axis=1)
197/22:
data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='N.A.')
197/23: len(data.discipline.unique())
197/24: data['discipline_lower']=data['discipline'].str.lower()
197/25: data['discipline_group_lower'] = data['discipline_group'].str.lower()
197/26: data.to_csv('all_institution_courses_lowered(new).csv')
197/27: data.shape
197/28: combined_spec = (pd.DataFrame(list(data.discipline_lower)+list(data.discipline_group_lower))[0].unique())
197/29: combined_spec = pd.DataFrame(combined_spec)
197/30: combined_spec
197/31: list(data.discipline_lower)+list(data.discipline_group_lower)
197/32: combined_spec[1]
197/33: combined_spec[0]
197/34: combined_spec[1]
197/35: combined_spec[2]
197/36: combined_spec
197/37:
# combined_spec[1] = combined_spec[0]
# combined_spec[2] = combined_spec[1]
combined_spec.head()
197/38: combined_spec.shape
197/39: syn = combined_spec.apply(syn_specialization)
197/40: syn
197/41: syn.head()
197/42: syn = combined_spec.apply(syn_specialization, axis=1)
197/43: syn
197/44: syn.head()
197/45: pd.read_csv('entity_specs.csv')
197/46: pd.DataFrame(syn).head()
197/47:
def ret_0(lst):
    lst = list(lst)
    return lst[0]
197/48: pd.DataFrame(syn).apply(ret_0).head()
197/49: pd.DataFrame(syn).apply(ret_0, axis=1).head()
197/50:
def syn_specialization(spec):
    spec = spec[0]
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
ad_brk('kunal]kd')
197/51: syn = combined_spec.apply(syn_specialization, axis=1)
197/52: pd.DataFrame(syn).head()
197/53:
def syn_specialization(spec):
    spec = spec[0]
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    if '[' in spec:
        specs.append(re.sub('\[', '-', spec))
        specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
ad_brk('kunal]kd')
197/54: syn = combined_spec.apply(syn_specialization, axis=1)
197/55: pd.DataFrame(syn).head()
197/56:
def syn_specialization(spec):
    if '[' in spec or ']' in spec:
        spec = spec[0]
        specs = []
        specy = re.sub('\)', ']', spec)
        spec = re.sub('\]', '', spec)
        specs.append(re.sub('\(', '[', specy))
    #     if '[' in spec:
        specs.append(re.sub('\[', '-', spec))
        specs.append(re.sub('\[', ' ', spec))
        return specs
    else:
        return [spec]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
ad_brk('kunal_goyal(kg)')
197/57:
def syn_specialization(spec):
    if any(x in spec for x in ['[',']','(', ')'])):
        spec = spec[0]
        specs = []
        specy = re.sub('\)', ']', spec)
        spec = re.sub('\]', '', spec)
        specs.append(re.sub('\(', '[', specy))
    #     if '[' in spec:
        specs.append(re.sub('\[', '-', spec))
        specs.append(re.sub('\[', ' ', spec))
        return specs
    else:
        return [spec]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
ad_brk('kunal_goyal(kg)')
197/58:
def syn_specialization(spec):
    if any(x in spec for x in ['[',']','(', ')']):
        spec = spec[0]
        specs = []
        specy = re.sub('\)', ']', spec)
        spec = re.sub('\]', '', spec)
        specs.append(re.sub('\(', '[', specy))
    #     if '[' in spec:
        specs.append(re.sub('\[', '-', spec))
        specs.append(re.sub('\[', ' ', spec))
        return specs
    else:
        return [spec]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
ad_brk('kunal_goyal(kg)')
197/59:
def syn_specialization(spec):
    if any(x in spec for x in ['[',']','(', ')']):
        spec = spec[0]
        specs = []
        specy = re.sub('\)', ']', spec)
        spec = re.sub('\]', '', spec)
        specs.append(re.sub('\(', '[', specy))
    #     if '[' in spec:
        specs.append(re.sub('\[', '-', spec))
        specs.append(re.sub('\[', ' ', spec))
        return specs
    else:
        return [spec]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kunal_goyal(kg)')
197/60:
def syn_specialization(spec):
    if any(x in spec for x in ['[',']','(', ')']):
        specs = []
        specy = re.sub('\)', ']', spec)
        spec = re.sub('\]', '', spec)
        specs.append(re.sub('\(', '[', specy))
    #     if '[' in spec:
        specs.append(re.sub('\[', '-', spec))
        specs.append(re.sub('\[', ' ', spec))
        return specs
    else:
        return [spec]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kunal_goyal(kg)')
197/61:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
ad_brk('kunal]kd')
197/62:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return specs

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kunal]kd')
197/63:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs))

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kunal]kd')
197/64: syn = combined_spec.apply(syn_specialization, axis=1)
197/65:
def syn_specialization(spec):
    spec = spec[0]
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs))

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kunal]kd')
197/66:
def syn_specialization(spec):
    spec = spec[0]
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs))

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
ad_brk('kunal]kd')
197/67: syn = combined_spec.apply(syn_specialization, axis=1)
197/68: pd.DataFrame(syn).head()
197/69:
%debug
syn = combined_spec.apply(syn_specialization, axis=1)
198/1:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs))+spec

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specializaition('kunal]kd')
198/2:
import numpy as np
import pandas as pd
import re
198/3:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs))+spec

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specializaition('kunal]kd')
198/4:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs))+spec

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kunal]kd')
198/5:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs+spec))

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kunal]kd')
198/6: data = pd.read_csv('../all_institutions_course_updated-Copy1.csv')
198/7:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs.append(spec)))

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kunal]kd')
198/8:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs))

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kunal]kd')
198/9:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs))

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kun[al]kd')
198/10:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs.append(spec)))

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kun[al]kd')
198/11:
def syn_specialization(spec):
    spec = str(spec)
    print(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs.append(spec)))

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kun[al]kd')
198/12:
def syn_specialization(spec):
    spec = str(spec)
    print(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs + [spec]))

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kun[al]kd')
198/13:
def syn_specialization(spec):
    spec = str(spec)
    print(spec)
    specs = []
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    return list(set(specs + [spec]))

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kunalkd')
198/14: data = pd.read_csv('../all_institutions_course_updated-Copy1.csv')
198/15: data.head()
198/16: data = data.drop(data[data.discipline.str.count(r'\(')>2].index)
198/17: data = data.drop(data[data.discipline.str.count(r'\)')>2].index)
198/18: data = data.drop(data[data.discipline.str.count(r'\<')>0].index)
198/19: data = data.drop(data[data.discipline.str.count(r'\>')>0].index)
198/20: data = data.drop(data[data.discipline.str.count(r'\:')>0].index)
198/21: data.loc[data['discipline'].isnull(),'discipline']  = data.loc[data['discipline'].isnull(),'discipline_group']
198/22: data.loc[data['discipline']=='N.A.','discipline']  = data.loc[data['discipline']=='N.A.','discipline_group']
198/23: data[data['discipline'].str.len()<2]['discipline'] = data[data['discipline'].str.len()<2]['discipline']
198/24: data.loc[data['discipline'].str.len()<2,'discipline'] = data.loc[data['discipline'].str.len()<2,'discipline']
198/25: data.shape
198/26: len(data['discipline'].unique())
198/27:
data['discipline'] = data['discipline'].apply(ad_brk)
data.head()
198/28: data = data.drop('Unnamed: 0',axis=1)
198/29:
# data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='N.A.')
198/30:
# data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='nan')
198/31: len(data.discipline.unique())
198/32: data['discipline'].str.isdigit()
198/33: sum(data['discipline'].str.isdigit())
198/34: data[(data['discipline'].str.isdigit())]
198/35: data.loc[data['discipline'].str.len()<2,'discipline'] = data.loc[data['discipline'].str.len()<2,'discipline_group']
198/36: data.shape
198/37: len(data['discipline'].unique())
198/38:
# data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='nan')
198/39: data[(data['discipline'].str.isdigit())]
198/40: sum(data['discipline'].str.isdigit())
198/41: data[(data['discipline'].str.isdigit())]
198/42: data.loc[(data['discipline'].str.isdigit()),'discipline'] = data.loc[(data['discipline'].str.isdigit()),'discipline_group']
198/43: len(data.discipline.unique())
198/44: data
198/45: data['discipline_lower']=data['discipline'].str.lower()
198/46: data['discipline_group_lower'] = data['discipline_group'].str.lower()
198/47: data.shape
198/48: len(data.discipline_lower.unique())
198/49: data.to_csv('(final)all_institution_courses_lowered_cleaned.csv')
198/50: data.shape
198/51: combined_spec = (pd.DataFrame(list(data.discipline_lower)+list(data.discipline_group_lower))[0].unique())
198/52: combined_spec = pd.DataFrame(combined_spec)
198/53:
# combined_spec[1] = combined_spec[0]
# combined_spec[2] = combined_spec[1]
combined_spec.head()
198/54:
def syn_specialization(spec):
    spec = str(spec)
    print(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kunalkd')
198/55:
def syn_specialization(spec):
    spec = str(spec)
    print(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kun[al]kd')
198/56:
# combined_spec[1] = combined_spec[0]
# combined_spec[2] = combined_spec[1]
combined_spec.head()
198/57: combined_spec.shape
198/58: pd.DataFrame(syn).head()
198/59: combined_spec[2] = combined_spec[0].apply(syn_specialization)
198/60:
# combined_spec[1] = combined_spec[0]
# combined_spec[2] = combined_spec[1]
combined_spec.head()
198/61:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kun[al]kd')
198/62: combined_spec[2] = combined_spec[0].apply(syn_specialization)
198/63: combined_spec.shape
198/64:
# combined_spec[1] = combined_spec[0]
# combined_spec[2] = combined_spec[1]
combined_spec.head()
198/65:
def ret_0(lst):
    lst = list(lst)
    return lst[0]
198/66:
def ret_1(lst):
    lst = list(lst)
    return lst[1]
def ret_2(lst):
    lst = list(lst)
    return lst[2]
198/67: combined_spec[1] = combined_spec[0].apply(syn_specialization)
198/68:
# combined_spec[1] = combined_spec[0]
# combined_spec[2] = combined_spec[1]
combined_spec.head()
198/69: combined_spec = (pd.DataFrame(list(data.discipline_lower)+list(data.discipline_group_lower))[0].unique())
198/70: combined_spec = pd.DataFrame(combined_spec)
198/71:
# combined_spec[1] = combined_spec[0]
# combined_spec[2] = combined_spec[1]
combined_spec.head()
198/72: combined_spec.shape
198/73: combined_spec[1] = combined_spec[0].apply(syn_specialization)
198/74:
# combined_spec[1] = combined_spec[0]
# combined_spec[2] = combined_spec[1]
combined_spec.head()
198/75:
def ret_0(lst):
    lst = list(lst)
    return lst[0]
198/76:
def ret_1(lst):
    lst = list(lst)
    return lst[1]
def ret_2(lst):
    lst = list(lst)
    return lst[2]
198/77: combined_spec[3] = combined_spec[1].apply(ret_1)
198/78: combined_spec[2] = combined_spec[1].apply(ret_2)
198/79: combined_spec[1] = combined_spec[1].apply(ret_0)
198/80: combined_spec.head()
198/81: combined_spec
198/82:
def comma(s):
    if str(s) != str(None):
        return '"'+s+'"'
    return None
198/83:
for i in range(4):
    combined_spec[i] = combined_spec[i].apply(comma)
198/84: combined_spec.head()
198/85: combined_spec.to_csv('(final)entity_specs.csv',header=None,index=None)
198/86:
import numpy as np
import pandas as pd
import re
198/87:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st
syn_specialization('kun[al]kd')
198/88: data = pd.read_csv('../all_institutions_course_updated-Copy1.csv')
198/89:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('(','[')
    return st.replace(')',']')
con_brk('kunal(dkd)')
199/1:
import numpy as np
import pandas as pd
import re
199/2:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('(','[')
    return st.replace(')',']')
con_brk('kunal(dkd)')
199/3: data = pd.read_csv('../all_institutions_course_updated-Copy1.csv')
199/4: data.head()
199/5: data = data.drop(data[data.discipline.str.count(r'\(')>2].index)
199/6: data = data.drop(data[data.discipline.str.count(r'\)')>2].index)
199/7: data = data.drop(data[data.discipline.str.count(r'\<')>0].index)
199/8: data = data.drop(data[data.discipline.str.count(r'\>')>0].index)
199/9: data = data.drop(data[data.discipline.str.count(r'\:')>0].index)
199/10: data.loc[data['discipline'].isnull(),'discipline']  = data.loc[data['discipline'].isnull(),'discipline_group']
199/11: data.loc[data['discipline']=='N.A.','discipline']  = data.loc[data['discipline']=='N.A.','discipline_group']
199/12: data.loc[data['discipline'].str.len()<2,'discipline'] = data.loc[data['discipline'].str.len()<2,'discipline_group']
199/13: data.shape
199/14: data['discipline'].apply(con_brk)
199/15: data['discipline'] = data['discipline'].apply(con_brk)
199/16: data.shape
199/17: len(data['discipline'].unique())
199/18:
data['discipline'] = data['discipline'].apply(ad_brk)
data.head()
199/19: data = data.drop('Unnamed: 0',axis=1)
199/20:
# data.loc[data.discipline=='N.A.','discipline'] = data.loc[data.discipline=='N.A.','discipline_group']
sum(data.discipline=='nan')
199/21: data.loc[(data['discipline'].str.isdigit()),'discipline'] = data.loc[(data['discipline'].str.isdigit()),'discipline_group']
199/22: len(data.discipline_lower.unique())
199/23: len(data.discipline_lower.unique())
199/24: data['discipline_lower']=data['discipline'].str.lower()
199/25: data['discipline_group_lower'] = data['discipline_group'].str.lower()
199/26: data.to_csv('(final_final)all_institution_courses_lowered_cleaned.csv')
199/27: data.shape
199/28: combined_spec = (pd.DataFrame(list(data.discipline_lower)+list(data.discipline_group_lower))[0].unique())
199/29: combined_spec = pd.DataFrame(combined_spec)
199/30:
# combined_spec[1] = combined_spec[0]
# combined_spec[2] = combined_spec[1]
combined_spec.head()
199/31: combined_spec.shape
199/32: combined_spec[1] = combined_spec[0].apply(syn_specialization)
199/33:
def ret_0(lst):
    lst = list(lst)
    return lst[0]
199/34:
def ret_1(lst):
    lst = list(lst)
    return lst[1]
def ret_2(lst):
    lst = list(lst)
    return lst[2]
199/35: combined_spec[3] = combined_spec[1].apply(ret_1)
199/36: combined_spec[2] = combined_spec[1].apply(ret_2)
199/37: combined_spec[1] = combined_spec[1].apply(ret_0)
199/38: combined_spec
199/39:
def comma(s):
    if str(s) != str(None):
        return '"'+s+'"'
    return None
199/40:
for i in range(4):
    combined_spec[i] = combined_spec[i].apply(comma)
199/41: combined_spec.head()
199/42: combined_spec.to_csv('(final_final)entity_specs.csv',header=None,index=None)
199/43:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    spec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', spec))
    specs.append(re.sub('\[', ' ', spec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('(','[')
    return st.replace(')',']')
syn_specialization('diploma in dialysis technology [ddt]')
199/44:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('(','[')
    return st.replace(')',']')
syn_specialization('diploma in dialysis technology [ddt]')
199/45:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '-', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('(','[')
    return st.replace(')',']')
syn_specialization('diploma in dialysis technology [ddt]')
199/46:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '- ', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('(','[')
    return st.replace(')',']')
syn_specialization('diploma in dialysis technology [ddt]')
199/47:
import numpy as np
import pandas as pd
import re
199/48:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '- ', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('(','[')
    return st.replace(')',']')
syn_specialization('diploma in dialysis technology [ddt]')
199/49: data = pd.read_csv('../all_institutions_course_updated-Copy1.csv')
199/50: data.head()
215/1:
%matplotlib inline
%reload_ext autoreload
%autoreload 2
215/2:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
torch.cuda.set_device(3)
215/3:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
torch.cuda.set_device(3)
215/4:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
torch.cuda.set_device(3)
231/1:
import pandas as pd
import re
231/2:
own_data = pd.read_csv('../../data/CollgeName1_entity.csv')
dialogflow = pd.read_csv('../../dialogflow_college1')
231/3:
own_data = open('../../data/CollgeName1_entity.csv', 'r+')
dialogflow = pd.read_csv('../../dialogflow_college1', 'r+')
232/1: import pandas as pd
232/2: alias data import pandas as pd; import numpy as np; import re; import matplotlib.pyplot as plt
232/3: %store data
232/4: data
232/5: c = get_config()
233/1: who
233/2: import pandas as pd
233/3: who
234/1: who
234/2: pd.read_csv('..')
237/1: for i in range(10000):
237/2: for i in range(10000):
237/3: for i in range(10000):
237/4: for i in range(10000):
237/5: for i in range(1000):
237/6: for i in range(30): i=i-1
237/7: while True: pass
238/1: from time import time
238/2: from time import time as t
238/3: ini = t()
238/4: end = t()
238/5: end-ini
243/1:
%matplotlib inline
%reload_ext autoreload
%autoreload 2
243/2:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
torch.cuda.set_device(3)
243/3:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
# torch.cuda.set_device(3)
243/4:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
# torch.cuda.set_device(3)
243/5:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
# torch.cuda.set_device(3)
243/6:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
# torch.cuda.set_device(3)
243/7:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
# torch.cuda.set_device(3)
246/1:
%matplotlib inline
%reload_ext autoreload
%autoreload 2
246/2:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
torch.cuda.set_device(3)
246/3:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
torch.cuda.set_device(3)
247/1:
%matplotlib inline
%reload_ext autoreload
%autoreload 2
247/2:
from fastai.conv_learner import *
from fastai.dataset import *

from pathlib import Path
import json
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
torch.cuda.set_device(3)
247/3: !pip install torch
248/1: import tensorflow
249/1: pip install ipython
251/1: import cv2 as cv
251/2: cv.__version__
252/1: import keras
252/2: import cv2
252/3:
# importing required libraries

from keras.models import Sequential
from scipy.misc import imread
get_ipython().magic('matplotlib inline')
import matplotlib.pyplot as plt
import numpy as np
import keras
from keras.layers import Dense
import pandas as pd
252/4:
# importing required libraries

from keras.models import Sequential
from scipy.misc import imread
import matplotlib.pyplot as plt
import numpy as np
import keras
from keras.layers import Dense
import pandas as pd
252/5:
from keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
import numpy as np
from keras.applications.vgg16 import decode_predictions
252/6: model = VGG16(weights='imagenet', include_top=False)
252/7: !htop
252/8: !pkill jupyter
252/9: !htop
252/10: model.summary()
252/11: model.summary?
252/12: from keras import backend as K
256/1:
import numpy as np
import pandas as pd
import re
260/1:
import numpy as np
import pandas as pd
import re
260/2:
mereexams = pd.read_csv('../data/all_institutions_live.csv')
google = pd.read_csv('../data/all_institutions_live.csv')
260/3:
mereexams = pd.read_csv('../data/all_institutions_live.csv')
google = pd.read_csv('../data/all_institutions_live.csv')
260/4:
mereexams = pd.read_csv('../data/all_institutions_live.csv', error_bad_lines='ignore')
google = pd.read_csv('../data/all_institutions_live.csv')
260/5:
mereexams = pd.read_csv('../data/all_institutions_live.csv', error_bad_lines=0)
google = pd.read_csv('../data/all_institutions_live.csv')
260/6:
mereexams = pd.read_csv('../data/all_institutions_live.csv', error_bad_lines=1)
google = pd.read_csv('../data/all_institutions_live.csv')
260/7:
mereexams = pd.read_excel('../data/all_institutions_live.csv')
google = pd.read_csv('../data/all_institutions_live.csv')
260/8:
mereexams = pd.read_csv('../data/all_institutions_live.csv', encoding = "utf-8")
google = pd.read_csv('../data/all_institutions_live.csv')
260/9:
mereexams = pd.read_csv('../data/all_institutions_live.csv', encoding = "ISO-8859-1")
google = pd.read_csv('../data/all_institutions_live.csv')
260/10:
mereexams = pd.read_csv('../data/all_institutions_live.csv', encoding = "ISO-8859-1")
google = pd.read_csv('../data/all_institutions_live.csv', encoding = "ISO-8859-1")
260/11: mereexams.head()
260/12: mereexams['city_cleaned'] = mereexams['city'].copy()
260/13: mereexams.head()
260/14: mereexams = mereexams[['city', 'city_cleaned', 'cid', 'name']]
260/15: mereexams.head()
260/16: mereexams.city.unique()
260/17: len(mereexams.city.unique())
260/18: len(mereexams.city.str.lower().unique())
260/19: sum(mereexams.city.apply(lambda x: '[' in x))
260/20: mereexams.city.apply(lambda x: '[' in x)
262/1:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '- ', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('_','-')
    st = st.replace('(','[')
    return st.replace(')',']')
syn_specialization('diploma in dialysis technology dd]')
262/2:
import numpy as np
import pandas as pd
import re
262/3:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '- ', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('_','-')
    st = st.replace('(','[')
    return st.replace(')',']')
syn_specialization('diploma in dialysis technology dd]')
262/4:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '- ', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('_','-')
    st = st.replace('(','[')
    return st.replace(')',']')
syn_specialization('diploma in dialysis technology dd')
262/5:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '- ', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('_','-')
    st = st.replace('(','[')
    return st.replace(')',']')
syn_specialization('diploma in dialysis technology -  dd')
266/1:
import numpy as np
import pandas as pd
import re
266/2:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '- ', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = str(st)
    st = st.replace('_','-')
    st = st.replace('(','[')
    return st.replace(')',']')
syn_specialization('diploma in dialysis technology -  dd')
266/3: data = pd.read_csv('../data/all_institutions_course_ids_live_new.csv')
266/4: data = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1')
266/5: data.head()
266/6:
data['spec_clean'] = data['spec'].copy()
data = data.loc[:,['discipline', 'spec_clean', 'discipline_group']]
266/7:
data['spec_clean'] = data['discipline'].copy()
data = data.loc[:,['discipline', 'spec_clean', 'discipline_group']]
266/8: data.head()
266/9: data['discipline'].lower().unique().count()
266/10: data['discipline'].str.lower().unique().count()
266/11: data['discipline'].str.lower().unique().shape
266/12: data['discipline'].unique().shape
266/13:
data['spec'] = data['spec_clean']
data.drop('spec_clean', axis=1)
266/14: data.head()
266/15:
data['spec'] = data['spec_clean']
data = data.drop('spec_clean', axis=1)
266/16: data.head()
266/17: data['spec'] = data.spec.str.lower()
266/18: print(data.spec.unique())
266/19: print(list(data.spec.unique()))
266/20:
data['spec'] = data.spec.str.replace('^\s?comp.*ap\S+\s?', 'computer application', regex=True)
print(data['spec'].unique().shape)
print(list(data.spec.unique()))
266/21:
data.spec.str.replace('^\s?comp.*ap\S+\s?', 'computer application', regex=True)
# print(data['spec'].unique().shape)
# print(list(data.spec.unique()))
266/22:
data.spec.str.replace('$\s?comp.*ap\S+\s?', 'computer application', regex=True)
# print(data['spec'].unique().shape)
# print(list(data.spec.unique()))
266/23:
data.spec.str.replace('\s?comp.*ap\S+\s?', 'computer application', regex=True)
# print(data['spec'].unique().shape)
# print(list(data.spec.unique()))
266/24:
data.spec.str.replace('\s?comp.*ap\S+\s?', 'computer application')
# print(data['spec'].unique().shape)
# print(list(data.spec.unique()))
266/25: data.spec.unique()
266/26: list(data.spec.unique())
266/27:
data.spec.str.replace('$\s?comp.*ap\S+\s?', 'computer application')
# print(data['spec'].unique().shape)
# print(list(data.spec.unique()))
266/28:
data.spec.str.replace('\s?comp.*ap\S+\s?', 'computer application')
# print(data['spec'].unique().shape)
# print(list(data.spec.unique()))
266/29:
data.spec.str.replace('$\s?comp.*ap\S+\s?', 'computer application')
# print(data['spec'].unique().shape)
# print(list(data.spec.unique()))
266/30:
data.spec.str.replace('$\s?comp.*ap\S+\s?^', 'computer application')
# print(data['spec'].unique().shape)
# print(list(data.spec.unique()))
266/31:
data.spec.str.replace('\s?comp.*ap\S+\s?', 'computer application')
# print(data['spec'].unique().shape)
# print(list(data.spec.unique()))
266/32:
data.spec.str.replace('comp.*ap\S+\s?', 'computer application')
# print(data['spec'].unique().shape)
# print(list(data.spec.unique()))
266/33:
data.spec.str.replace('comp\S*\s*ap\S+\s?', 'computer application')
# print(data['spec'].unique().shape)
# print(list(data.spec.unique()))
266/34:
f = data.spec.str.replace('comp\S*\s*ap\S+\s?', 'computer application')
print(funique().shape)
# print(list(data.spec.unique()))
266/35:
f = data.spec.str.replace('comp\S*\s*ap\S+\s?', 'computer application')
print(f.unique().shape)
# print(list(data.spec.unique()))
266/36: pd.show_versions
266/37: pd.show_versions()
266/38: data[data.spec.str.contains('(')]['spec']
266/39: data[data.spec.str.contains('\(')]['spec']
266/40: data = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1').astype(str)
266/41:
data['spec_clean'] = data['discipline'].copy()
data = data.loc[:,['discipline', 'spec', 'discipline_group']]
266/42:
data['spec'] = data['spec_clean']
data = data.drop('spec_clean', axis=1)
266/43:
data['spec_clean'] = data['discipline'].copy()
data = data.loc[:,['discipline', 'spec', 'discipline_group']]
266/44:
data['spec'] = data['spec_clean']
data = data.drop('spec_clean', axis=1)
266/45: data.columns
266/46: data = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1').astype(str)
266/47:
data['spec'] = data['discipline'].copy()
data = data.loc[:,['discipline', 'spec', 'discipline_group']]
266/48: data.columns
266/49: data.head()
266/50: data['discipline'].unique().shape
266/51: data['discipline'].str.lower().unique().shape
266/52: data['spec'] = data.spec.str.lower()
266/53: data[data.spec.str.contains('\(')]['spec']
266/54: data[data.spec.str.contains('\(')][['spec','discipline']
266/55: data[data.spec.str.contains('\(')][['spec','discipline']]
266/56: data[data.spec.str.contains('\(')][['spec','discipline_group']]
266/57:
def remo_brace(s):
    re.split('\W+', 'Words, words, words.')
remo_brace(s)
266/58:
def remo_brace(s):
    re.split('\W+', 'Words, words, words.')
remo_brace(4)
266/59:
def remo_brace(s):
    return re.split('\W+', 'Words, words, words.')
remo_brace(4)
266/60:
def remo_brace(s):
    return re.split('\W+', s)
remo_brace('Words, words, words.')
266/61:
def remo_brace(s):
    return re.split('\W+', s)
remo_brace('Words, words, words. hele (mca)')
266/62:
def remo_brace(s):
    return re.split('\W+', s)
remo_brace('Words word ords hele (mca)')
266/63:
def remo_brace(s):
    return re.split('\W+ ', s)
remo_brace('Words word ords hele (mca)')
266/64:
def remo_brace(s):
    return re.split('\W+', s)
remo_brace('Words word ords hele (mca)')
266/65:
def remo_brace(s):
    return re.split('\W+ ', s)
remo_brace('Words word ords hele (mca)')
266/66:
def remo_brace(s):
    return re.split('(\W )+', s)
remo_brace('Words word ords hele (mca)')
266/67:
def remo_brace(s):
    return re.split('[\W\s]+', s)
remo_brace('Words word ords hele (mca)')
266/68:
def remo_brace(s):
    return re.split('[\(\)]+', s)
remo_brace('Words word ords hele (mca)')
266/69:
def remo_brace(s):
    s = s.sub('of|in','',s)
    return re.split('[\(\)]+', s)
remo_brace('Words word ords of  hele (mca)')
266/70:
def remo_brace(s):
    s = re.sub('of|in','',s)
    return re.split('[\(\)]+', s)
remo_brace('Words word ords of  hele (mca)')
266/71:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s)
    return re.split('[\(\)]+', s)
remo_brace('Words word ords of  hele (mca)')
266/72:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s)
    return re.split('[\(\)]+', s)
remo_brace('Words word ords of hele (mca)')
266/73:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.split('[\(\)]+', s)
    s = list(map(lambda x: re.sub('le|lateral^', 'lateral entry', x), s))
remo_brace('Words word ords of hele (mca)')
266/74:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.split('[\(\)]+', s)
    return list(map(lambda x: re.sub('le|lateral^', 'lateral entry', x), s))
remo_brace('Words word ords of hele (mca)')
266/75:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.split('[\(\)]+', s)
    return list(map(lambda x: re.sub('$le^|lateral^', 'lateral entry', x), s))
remo_brace('Words word ords of hele (mca)')
266/76:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.split('[\(\)]+', s)
    return list(map(lambda x: re.sub('$le^|$lateral^', 'lateral entry', x), s))
remo_brace('Words word ords of hele (mca)')
266/77:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.split('[\(\)]+', s)
    return list(map(lambda x: re.sub('$le^|$lateral^', 'lateral entry', x), s))
remo_brace('Words word ords of hele (mca)le')
266/78:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.split('[\(\)]+', s)
    return list(map(lambda x: re.sub('\$le^|$lateral^', 'lateral entry', x), s))
remo_brace('Words word ords of hele (mca)le')
266/79:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.split('[\(\)]+', s)
    return list(map(lambda x: re.sub('$le^|$lateral^', 'lateral entry', x), s))
remo_brace('Words word ords of hele (mca)le')
266/80:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.split('[\(\)]+', s)
    return list(map(lambda x: re.sub('$le|$lateral^', 'lateral entry', x), s))
remo_brace('Words word ords of hele (mca)le')
266/81:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.split('[\(\)]+', s)
    return list(map(lambda x: re.sub('^le$|^lateral$', 'lateral entry', x), s))
remo_brace('Words word ords of hele (mca)le')
266/82:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.split('[\(\)]+', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    return list(map(lambda x: re.sub('^le$|^lateral$', 'lateral entry', x), s))
remo_brace('Words word ords of hele (mca)le(ft)')
266/83:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application')
    s = re.split('[\(\)]+', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        if short(part) in s:
            s.remove(short(part))
            break
            
remo_brace('master of computer application (mca)')
266/84:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('[\(\)]+', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        if short(part) in s:
            s.remove(short(part))
            break
            
remo_brace('master of computer application (mca)')
266/85:
def short(s):
    return ''.join([w[0] for w in s.split()])
266/86:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('[\(\)]+', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        if short(part) in s:
            s.remove(short(part))
            break
            
remo_brace('master of computer application (mca)')
266/87:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('[\(\)]+', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        if short(part) in s:
            s.remove(short(part))
            break
    return s
            
remo_brace('master of computer application (mca)')
266/88:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('[\(\)\-]+', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        if short(part) in s:
            s.remove(short(part))
            break
    return s
            
remo_brace('master of computer application (mca) - mca')
266/89:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('[\(\)\-]+', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        if short(part) in s:
            s.remove(short(part))
            break
    return s
            
remo_brace('master of computer application (mca) -mca')
266/90:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('[\(\)\-]+', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        if short(part) in s:
            s.remove(short(part))
            
    return s
            
remo_brace('master of computer application (mca) -mca')
266/91:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('[\(\)\-]+', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    return s
            
remo_brace('master of computer application (mca) -mca')
266/92:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('[\(\)\-]+', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    return s
            
remo_brace('master of computer application (mca) - mca')
266/93:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('\s?[\(\)\-]+\s?', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    return s
            
remo_brace('master of computer application (mca) - mca')
266/94:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('\s?[\(\)\-]+\s?', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    return s
            
remo_brace('master of computer application (mdca) - mca')
266/95:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('\s?[\(\)\-]+\s?', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    return s
            
remo_brace('master of computer application (mdca) - mca')
266/96:
def remo_brace(s):
    s = re.sub('\sof|\sin ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('\s?[\(\)\-]+\s?', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    return s
            
remo_brace('master of computer application (mca) - mca (regular)')
266/97:
def remo_brace(s):
    s = re.sub('\s*of|\s*in ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    return s
            
remo_brace('master of computer application (mca) - mca (regular)')
266/98:
def remo_brace(s):
    s = re.sub('\s*of|\s*in ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('master of computer application (mca) - mca (regular)')
266/99:
def remo_brace(s):
    s = re.sub('\s*of|\s*in ','',s) 
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer applications- integrated (asas, mysore)')
266/100:
def remo_brace(s):
    s = re.sub('\s*of|\s*in ','',s) 
    s = re.sub('\.', '', s)
    s = re.sub('&', 'and', s)
    s = re.sub('^com\S+\s?app\S+\s?$', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer & applications- integrated (asas, mysore)')
266/101: data[data.spec.str.contains('\(')][['spec','discipline_group']]
266/102: data[data.spec.str.contains('\(')][['spec','discipline_group']].shape
266/103: data[data.spec.str.contains('\[]')][['spec','discipline_group']].shape
266/104: data[data.spec.str.contains('\[')][['spec','discipline_group']].shape
266/105: data[data.spec.str.contains('\[')][['spec','discipline_group']]
266/106: data[data.spec.str.contains('eng')][['spec','discipline_group']]
266/107: data[data.spec.str.contains('engg')][['spec','discipline_group']]
266/108: data[data.spec.str.contains('engg\.')][['spec','discipline_group']]
266/109: data[data.spec.str.contains('\.')][['spec']].unique()
266/110: data[data.spec.str.contains('\.')]['spec'].unique()
266/111: list(data[data.spec.str.contains('\.')]['spec'].unique())
266/112:
degree = pd.read_csv('../data/all_institutions_course_ids_live_new.csv')['programme'].astype(str)
degree['deg'] = degree.programme.str.replace('\-.*', '')
degree[['deg', 'programme']]
266/113:
degree = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1')['programme'].astype(str)
degree['deg'] = degree.programme.str.replace('\-.*', '')
degree[['deg', 'programme']]
266/114:
degree = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1').astype(str)
degree['deg'] = degree.programme.str.replace('\-.*', '')
degree[['deg', 'programme']]
266/115:
degree = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1').astype(str)
degree['deg'] = degree.programme.str.replace('\-.*', '')
list(degree['deg'].unique())
267/1:
import numpy as np
import pandas as pd
import re
267/2: data = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1').astype(str)
267/3:
data['spec'] = data['discipline'].copy()
data = data.loc[:,['discipline', 'spec', 'discipline_group', 'programme']]
267/4: data.columns
267/5: data.head()
267/6: data.programme.str.split('\-', n=1, expand=True).head()
267/7:
data['degree', 'full'] = data.programme.str.split('\-', n=1, expand=True).head()
data.drop('full', axis=1)
data.degree.head()
267/8: %debug
269/1:
import numpy as np
import pandas as pd
import re
269/2: data = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1').astype(str)
269/3:
data['spec'] = data['discipline'].copy()
data = data.loc[:,['discipline', 'spec', 'discipline_group', 'programme']]
269/4: data.columns
269/5: data.head()
269/6:
def shortgen(s):
    return s.split('-')[0]
269/7:
data['deg'] = data.programme.apply(shortgen)
data.deg.head()
269/8:
data['deg'] = data.programme.apply(shortgen)
data.deg.unique().shape
269/9:
# data['deg'] = data.programme.apply(shortgen)
data.deg.unique()
269/10:
# data['deg'] = data.programme.apply(shortgen)
data.deg = data.deg.str.
data['degree'] = data['deg'].replace(regex=True,inplace=False,to_replace='\.',value='')
269/11:
# data['deg'] = data.programme.apply(shortgen)
# data.deg = data.deg.str.
data['degree'] = data['deg'].replace(regex=True,inplace=False,to_replace='\.',value='')
269/12: data['degree'].unique()
269/13:
# data['deg'] = data.programme.apply(shortgen)
# data.deg = data.deg.str.
data['deg'].replace(regex=True,inplace=True,to_replace='\.',value='')
269/14: data['deg'].unique()
269/15: degrees = list(data[l'deg'].unique()); degrees
269/16:
degrees = list(data[l'deg'].unique())
degrees
269/17:
degrees = list(data['deg'].unique())
degrees
269/18:
# degrees = list(data['deg'].unique())
degrees = [d.strip() for d in degrees]
degrees
269/19: data['discipline'].unique().shape
269/20: data['discipline'].str.lower().unique().shape
269/21: data['spec'] = data.spec.str.lower()
269/22:
def remove_dot(s):
    return re.sub('\.', '', s)
269/23: remove_dot(m.sc(it))
269/24: remove_dot('m.sc(it)')
269/25: remove_dot('b. s. computer science-cloud computing and big data')
269/26: data['spec'] = data['spec'].apply(remove_dot)
269/27:
pattern = '|'.join(degrees)
re.sub(pattern, '', 'mba (master of somthign)')
269/28:
pattern = '|'.join(degrees)
re.sub(pattern, '', 'mba (master of somthign')
269/29:
pattern = '|'.join(degrees)
re.sub(pattern, '', 'mba \(master of somthign')
269/30:
pattern = '|'.join(degrees)
re.sub(pattern, '', 'mba \(master of somthign\)')
269/31:
pattern = '|'.join(degrees)
print(pattern)
re.sub(pattern, '', 'mba \(master of somthign\)')
269/32:
pattern = '|'.join(degrees)
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
pat.sub( '', 'mba \(master of somthign\)')
269/33:
degr = [d.split('\(')[0] for d in degrees]
print(degr)
# pattern = '|'.join(degrees)
# pat = re.compile(pattern, re.IGNORECASE)
# print(pattern)
# pat.sub( '', 'mba \(master of somthign\)')
269/34:
degr = [d.split('(')[0] for d in degrees]
print(degr)
# pattern = '|'.join(degrees)
# pat = re.compile(pattern, re.IGNORECASE)
# print(pattern)
# pat.sub( '', 'mba \(master of somthign\)')
269/35:
degree = [d.split('(')[0] for d in degrees]
pattern = '|'.join(degrees)
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
pat.sub( '', 'mba \(master of somthign\)')
269/36:
degree = [d.split('(')[0] for d in degrees]
degree
269/37:
degrees = [d.split('(')[0] for d in degrees]
pattern = '|'.join(degrees)
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
pat.sub( '', 'mba \(master of somthign\)')
269/38:
degrees = [d.split('(')[0] for d in degrees]
pattern = '|'.join(degrees)
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
pat.sub( '', 'mba (master of somthign)')
269/39:
degrees = [d.split('(')[0] for d in degrees]
degrees = [d+' ' for d in degrees]
pattern = '|'.join(degrees)
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
pat.sub( '', 'mba (master of somthign)')
269/40:
degrees = [d.split('(')[0] for d in degrees]
degrees = [d+' ' for d in degrees]
pattern = '|'.join(degrees)
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
pat.subn( '', 'mba sf(master of somthign)')
269/41:
degrees = [d.split('(')[0] for d in degrees]
degrees = [d+' ' for d in degrees]
pattern = '|'.join(degrees)
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
pat.subn( '', 'mba sf(master of somthign)')[0]
269/42:
degrees = [d.split('(')[0] for d in degrees]
degrees = [d.strip() +' ' for d in degrees]
pattern = '|'.join(degrees)
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
pat.subn( '', 'mba sf(master of somthign)')[0]
269/43: data['spec_remove_deg'] = data['spec'].apply(lambda x: return pat.subn('', x)[0])
269/44: data['spec_remove_deg'] = data['spec'].apply(lambda x: pat.subn('', x)[0])
269/45: data.spec_remove_deg.head()
269/46: data.spec_remove_deg.unique().shape
269/47: data.spec.unique().shape
269/48:
data.spec = data.spec_remove_deg
data.drop('spec_remove_deg', axis=1)
269/49:
data.spec = data.spec_remove_deg
data.drop('spec_remove_deg', axis=1, inplace=True)
269/50:
f = data.spec.str.replace('comp\S*\s*ap\S+\s?', 'computer application')
print(f.unique().shape)
# print(list(data.spec.unique()))
269/51: data.spec.unique().shape[0]
269/52:
f = data.spec.str.replace('comp\S*\s*ap\S+\s?', 'computer application')
print(f.unique().shape[0])
# print(list(data.spec.unique()))
269/53:
def remo_brace(s):
    s = re.sub('\s*engg\S+', ' engineering ', s)
    s = re.sub('\s*agri\S+', 'agriculture', s)
    s = re.sub()
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s&\s', ' and ', s)
    s = re.sub('^com\S+\s?app\S+$', 'computer application' s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/54:
def remo_brace(s):
    s = re.sub('\s*engg\S+', ' engineering ', s)
    s = re.sub('\s*agri\S+', 'agriculture', s)
    s = re.sub()
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s&\s', ' and ', s)
    s = re.sub('^com\S+\s?app\S+$', 'computer application' s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/55:
def remo_brace(s):
    s = re.sub('\s*engg\S+', ' engineering ', s)
    s = re.sub('\s*agri\S+', 'agriculture', s)
    s = re.sub('\s+sc[\.i]\S*\s*', 'science ', s)
    s = re.sub()
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s&\s', ' and ', s)
    s = re.sub('^com\S+\s?app\S+$', 'computer application' s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/56:
def remo_brace(s):
    s = re.sub('\s*engg\S+', ' engineering ', s)
    s = re.sub('\s*agri\S+', 'agriculture', s)
    s = re.sub('\s+sc[\.i]\S*\s*', 'science ', s)
    s = re.sub()
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s&\s', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/57:
def remo_brace(s):
    s = re.sub('\s*engg\S+', ' engineering ', s)
    s = re.sub('\s*agri\S+', 'agriculture', s)
    s = re.sub('\s+sc[\.i]\S*\s*', 'science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s&\s', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/58:
def short(s):
    return ''.join([w[0] for w in s.split()])
269/59:
def remo_brace(s):
    s = re.sub('\s*engg\S+', ' engineering ', s)
    s = re.sub('\s*agri\S+', 'agriculture', s)
    s = re.sub('\s+sc[\.i]\S*\s*', 'science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s&\s', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/60:
def remo_brace(s):
    s = re.sub('\s*engg\S+', ' engineering ', s)
    s = re.sub('\s*agri\S+', 'agriculture', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', 'science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s&\s', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/61:
def remo_brace(s):
    s = re.sub('\s*engg\S+', ' engineering ', s)
    s = re.sub('\s*agri\S+', 'agriculture', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', 'science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/62:
def remo_brace(s):
    s = re.sub('\s*engg\S+', ' engineering ', s)
    s = re.sub('\s*agri\S+', 'agriculture', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/63:
def remo_brace(s):
    s = re.sub('\s*engg\S+', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/64:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/65:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s.strip()
            
remo_brace('computer sc.&engg')
269/66:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while short(part) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/67:
def full(s, l):
    for elem in l:
        if ''.join([w[0] for w in elem.split()]) == s: return elem
        else: return 'False'
269/68:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('computer sc.&engg')
269/69:
def full(s, l):
    for elem in l:
        if ''.join([w[0] for w in elem.split()]) == s: return elem
        else: return 'False'
full('mba', ['mba', 'master of bussiness admin'])
269/70:
def full(s, l):
    for elem in l:
        if ''.join([w[0] for w in elem.split()]) == s: return elem
    else: return 'False'
full('mba', ['mba', 'master of bussiness admin'])
269/71:
def full(s, l):
    for elem in l:
        if ''.join([w[0] for w in elem.split()]) == s: return elem
    else: return 'False'
full('mba', ['mba', 'master bussiness admin'])
269/72:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(short(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('mba- master bussiness admisnistration (computer sc.&engg)')
269/73:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('mba- master bussiness admisnistration (computer sc.&engg)')
269/74:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return s
            
remo_brace('mba- master bussiness admisnistration (computer sc.&engg)')
269/75:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return ' '.join(s)
            
remo_brace('mba- master bussiness admisnistration (computer sc.&engg)')
269/76:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba- master bussiness admisnistration (computer sc.&engg)')
269/77:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba- masteress admisnistration (computer sc.&engg)')
269/78:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba- masess admisnistration (computer sc.&engg)')
269/79:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba-. ma.sess admisnistration (computer sc.&engg)')
269/80:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba.ma.sess admisnistration (computer sc.&engg)')
269/81:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba. masess admisnistration (computer sc.&engg)')
269/82:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.subn('\d', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba. masess admisnistration (computer sc.45&engg)')
269/83:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.subn('\d', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba. admisnistration (computer sc.45&engg)')
269/84: data[data.spec.str.contains('\.\w')][['spec','discipline_group']]
269/85: data[data.spec.str.contains('\.\')][['spec','discipline_group']]
269/86: data[data.spec.str.contains('\.[a-z]')][['spec','discipline_group']]
269/87: data[data.spec.str.contains('\.')][['spec','discipline_group']]
269/88:
def remove_dot(x):
    return re.sub('\.(\s|$|\w{1,2,3})', '', x)
remove_dot('b.sc.mechanical')
269/89:
def remove_dot(x):
    return re.sub('\.(\s|$|[a-z]{1,2,3})', '', x)
remove_dot('b.sc.mechanical')
269/90:
def remove_dot(x):
    return re.sub(r'\.(\s|$|[a-z]{1,2,3})', '', x)
remove_dot('b.sc.mechanical')
269/91:
def remove_dot(x):
    return re.sub('\.(\s|$|[a-z]{1,2,3})', '', x)
remove_dot('b.sc.mechanical')
269/92:
def remove_dot(x):
    return re.sub('\.', '', x)
remove_dot('b.sc.mechanical')
269/93:
def remove_dot(x):
    return re.sub('\.('\s')', '', x)
remove_dot('b.sc.mechanical')
269/94:
def remove_dot(x):
    return re.sub('\.(\s)', '', x)
remove_dot('b.sc.mechanical')
269/95:
def remove_dot(x):
    return re.sub('\.(\s)?', '', x)
remove_dot('b.sc.mechanical')
269/96:
def remove_dot(x):
    return re.sub('\.([a-z])', '', x)
remove_dot('b.sc.mechanical')
269/97:
def remove_dot(x):
    return re.sub('\.([a-z])', '\1', x)
remove_dot('b.sc.mechanical')
269/98:
def remove_dot(x):
    return re.sub('\.([a-z])', r'\1', x)
remove_dot('b.sc.mechanical')
269/99:
def remove_dot(x):
    return re.sub('\.([a-z]{1,2,3})', r'\1', x)
remove_dot('b.sc.mechanical')
269/100:
def remove_dot(x):
    return re.sub('\.([a-z]{1,3})', r'\1', x)
remove_dot('b.sc.mechanical')
269/101:
def remove_dot(x):
    s = ''
    for w in x.split('.'):
        print(w)
remove_dot('b.sc.mechanical')
269/102:
def remove_dot(x):
    s = ''
    for i in range(x.split('.')):
        if len(x.split('.')[i+1]) < 3: s+=x.split('.')[i]
remove_dot('b.sc.mechanical')
269/103:
def remove_dot(x):
    s = ''
    for i in range(len(x.split('.'))):
        if len(x.split('.')[i+1]) < 3: s+=x.split('.')[i]
remove_dot('b.sc.mechanical')
269/104:
def remove_dot(x):
    s = ''
    for i in range(len(x.split('.'))-1):
        if len(x.split('.')[i+1]) < 3: s+=x.split('.')[i]
remove_dot('b.sc.mechanical')
269/105:
def remove_dot(x):
    s = ''
    for i in range(len(x.split('.'))-1):
        if len(x.split('.')[i+1]) < 3: s+=x.split('.')[i]
    return s
remove_dot('b.sc.mechanical')
269/106:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(v[i+1]) < 3: s+=v[i]
    return s
remove_dot('b.sc.mechanical')
269/107:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s+=l[i]
    return s
remove_dot('b.sc.mechanical')
269/108:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s+=l[i]
        else: s = s+' '+l[i]
    return s
remove_dot('b.sc.mechanical')
269/109:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s+=l[i]
        else: s = s+' '+l[i]
    return s+' '+l[-1]
remove_dot('b.sc.mechanical')
269/110:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+' '+l[i]
    return s+' '+l[-1]
remove_dot('b.sc.mechanical')
269/111:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
#         else: s = s+' '+l[i]
    return s+' '+l[-1]
remove_dot('b.sc.mechanical')
269/112:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    return s+' '+l[-1]
remove_dot('b.sc.mechanical')
269/113:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    return s+' '+l[-1]
remove_dot('b.sc.me')
269/114:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    return s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
remove_dot('b.sc.me')
269/115:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    return s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
remove_dot('b.sc.mech')
269/116:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    return s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
remove_dot('b.sc. mech')
269/117:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    return s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
remove_dot('b.sc.')
269/118:
def remove_dot(x):
    s = ''
    l = x.split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    return s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
remove_dot('b.sc.engg.')
269/119:
def remove_dot(x):
    s = ''
    l = x.strip().split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    return s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
remove_dot('b.sc.engg.')
269/120:
def remove_dot(x):
    s = ''
    l = x.strip('.').split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    return s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
remove_dot('b.sc.engg.')
269/121:
def remove_dot(x):
    s = ''
    l = x.strip('.').split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
    return re.sub('\.', '', ans)
remove_dot('b.sc.engg.')
269/122:
def remove_dot(x):
    s = ''
    l = x.strip('.').split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
    return re.sub('\.', '', ans)
remove_dot('b.sc.engg. electrical eng')
269/123:
def remove_dot(x):
    s = ''
    l = x.strip('.').split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+' '+l[i]
    ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
    return re.sub('\.', '', ans)
remove_dot('b.sc.engg. electrical eng')
269/124:
def remove_dot(x):
    s = ''
    l = x.strip('.').split('.')
    for i in range(len(l)-1):
        if len(l[i+1]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
    return re.sub('\.', '', ans)
remove_dot('b.sc.engg. electrical eng')
269/125:
def remove_dot(x):
    s = ''
    l = x.strip('.').split('.')
    for i in range(len(l)-1):
        if len(l[i]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
    return re.sub('\.', '', ans)
remove_dot('b.sc.engg. electrical eng')
269/126:
def remove_dot(x):
    s = ''
    l = x.strip('.').split('.')
    for i in range(len(l)-1):
        if len(l[i]) < 3: s = s+l[i]+'.'
        else: s = s+l[i]
    ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
    return re.sub('\.', '', ans)
remove_dot('b.sc.engg.electrical eng')
269/127:
def remove_dot(x):
    s = ''
    l = x.strip('.').split('.')
    for i in range(len(l)-1):
        if len(l[i]) < 3: s = s+l[i]+'.'
        else: s = s+' '+l[i]
    ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
    return re.sub('\.', '', ans)
remove_dot('b.sc.engg.electrical eng')
269/128:
def remove_dot(x):
    try:
        s = ''
        l = x.strip('.').split('.')
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.sub('\.', '', ans)
    except:
        return x
remove_dot('b.sc.engg.electrical eng')
269/129:
def remove_dot(x):
    try:
        s = ''
        l = x.strip('.').split('.')
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.sub('\.', '', ans)
    except:
        return x
remove_dot('b.sc.engg.electrical eng.')
269/130:
def remove_dot(x):
    try:
        s = ''
        l = x.strip('.').split('.')
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.sub('\.', '', ans)
    except:
        return x
remove_dot('b.sc.engg.electrical eng. ')
269/131:
def remove_dot(x):
    try:
        s = ''
        l = x.strip('.').split('. ')
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.sub('\.', '', ans)
    except:
        return x
remove_dot('b.sc.engg.electrical eng. ')
269/132:
def remove_dot(x):
    try:
        s = ''
        l = x.strip('. ').split('.')
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.sub('\.', '', ans)
    except:
        return x
remove_dot('b.sc.engg.electrical eng. ')
269/133:
def remove_dot(x):
    try:
        s = ''
        l = x.strip('. ').split('.')
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.sub('\.', '', ans)
    except:
        return x
remove_dot(' master b.sc.engg.electrical eng. ')
269/134:
def remove_dot(x):
    try:
        s = ''
        l = x.strip(' . ').split('.')
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.sub('\.', '', ans)
    except:
        return x
remove_dot(' master b.sc.engg.electrical eng. ')
269/135:
def remove_dot(x):
    try:
        s = ''
        l = x.strip('. ').split('.')
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.sub('\.', '', ans).strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg.electrical eng. ')
269/136:
def remove_dot(x):
    try:
        s = ''
        l = x.strip(' .').split('.')
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.sub('\.', '', ans).strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg.electrical eng. ')
270/1:
import numpy as np
import pandas as pd
import re
270/2: data = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1').astype(str)
270/3:
data['spec'] = data['discipline'].copy()
data = data.loc[:,['discipline', 'spec', 'discipline_group', 'programme']]
270/4: data.columns
270/5: data.head()
270/6:
data['deg'] = data.programme.apply(lambda x: x.split('-')[0])
data['deg'].replace(regex=True,inplace=True,to_replace='\.',value='')
270/7:
degrees = list(data['deg'].unique())
degrees = [d.strip() for d in degrees]
degrees
270/8: data['discipline'].unique().shape
270/9: data['discipline'].str.lower().unique().shape
270/10: data['spec'] = data.spec.str.lower()
270/11:
def remove_dot(x):
    try:
        s = ''
        l = x.strip(' .').split('.')
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg.electrical eng. ')
270/12: data['spec_remove_dot'] = data['spec'].apply(remove_dot)
270/13: data[data.spec.contains('\.')][['spec', 'spec_remove_dot']].head(30)
270/14: data[data.spec.str.contains('\.')][['spec', 'spec_remove_dot']].head(30)
270/15: data[data.spec.str.contains('\..\.')][['spec', 'spec_remove_dot']].head(30)
270/16:
def remove_dot(x):
    try:
        s = ''
        l = re.strip('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg.ele-trical eng. ')
270/17:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg.ele-trical eng. ')
270/18:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg.ele-trical eng. ')
270/19:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg.el.e-trical eng. ')
270/20:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e-trical eng. ')
270/21:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e-trical eng. ')
270/22:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1] < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e-trical eng. ')
270/23:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+' '+l[i]
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e-trical eng. ')
270/24:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e-trical eng. ')
270/25:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,', x.strip())
        for i in range(len(l)-1):
            if l[i] == 'b': print(l[i:i+2])
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e-trical eng. ')
270/26:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,', x.strip())
        for i in range(len(l)-1):
            if l[i] == 'b': print(l[i+1])
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e-trical eng. ')
270/27:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,', x.strip())
        print(l)
        for i in range(len(l)-1):
            if l[i] == 'b': print(l[i+1])
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e-trical eng. ')
270/28:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        print(l)
        for i in range(len(l)-1):
            if l[i] == 'b': print(l[i+1])
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e-trical eng. ')
270/29:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e-trical eng. ')
270/30:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-ele-trical eng. ')
270/31:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-ele.ctr.ical eng. ')
270/32:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-ele.ct.r.ical eng. ')
270/33:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-ele.ct.r.ic.al eng. ')
270/34:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 4 and len(l[i+1]) < 4: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-ele.ct.r.ic.al eng. ')
270/35:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-ele.ct.r.ic.al eng. ')
270/36:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e.ct.r.ic.al eng. ')
270/37:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sci.engg-el.e.ct.r.ic.al eng. ')
270/38: data['spec_remove_dot'] = data['spec'].apply(remove_dot)
270/39: data[data.spec.str.contains('\..\.')][['spec', 'spec_remove_dot']].head(30)
270/40: data[data.spec.str.contains('\..\.')][['spec', 'spec_remove_dot']].head(100)
270/41:
data.spec = data.spec_remove_dot
data.drop('spec_remove_dot', axis=1)
data.spec.unique().shape[0]
270/42:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e.ct.r.ic.al eng. ')
270/43:
f = data.spec.str.replace('comp\S*\s*ap\S+\s?', 'computer application')
print(f.unique().shape[0])
# print(list(data.spec.unique()))
270/44:
def full(s, l):
    for elem in l:
        if ''.join([w[0] for w in elem.split()]) == s: return elem
    else: return 'False'
full('mba', ['mba', 'master bussiness admin', 'mab', 'mas buss adm'])
270/45:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.subn('\d', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba. master of business admisnistration (computer sc.45&engg)')
270/46:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.subn('\d', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('^masters?\s*|^bach\S+\s?$', '', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/47:
data['remo_br'] = data.spec.apply(remo_brace)
data[data.remo_br.str.contains('\(')]['remo_br', 'spec']
270/48:
# data['remo_br'] = data.spec.apply(remo_brace)
data[data.remo_br.str.contains('\(')][['remo_br', 'spec']]
270/49:
# data['remo_br'] = data.spec.apply(remo_brace)
data[data.remo_br.str.contains('(')][['remo_br', 'spec']]
270/50:
# data['remo_br'] = data.spec.apply(remo_brace)
data[data.remo_br.str.contains('\(')][['remo_br', 'spec']]
270/51:
# data['remo_br'] = data.spec.apply(remo_brace)
data[data.spec.str.contains('\(')][['remo_br', 'spec']]
270/52:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.subn('\d', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('^full-time$|^ft$|^reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/53:
data['remo_br'] = data.spec.apply(remo_brace)
data[data.spec.str.contains('\(')][['remo_br', 'spec']]
270/54:
# data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\(')][['remo_br', 'spec']])
270/55:
import numpy as np
import pandas as pd
import re
pd.set_option('display.height', 1000)
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
270/56:
# data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\(')][['remo_br', 'spec']])
270/57:
# data['remo_br'] = data.spec.apply(remo_brace)
data[data.spec.str.contains('\(')][['remo_br', 'spec']]
270/58:
# data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\(')][['remo_br', 'spec']])
270/59:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('master of science (information  technology)')
270/60:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S)*]\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('master of science (information  technology)')
270/61:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S*)]\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('master of science (information  technology)')
270/62:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.(\S+)]\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('master of science (information  technology)')
270/63:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.\S+]\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('master of science (information  technology)')
270/64:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.]\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('master of science (information  technology)')
270/65:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc[\.\S+]\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('master of science (information  technology)')
270/66:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('master of science (information  technology)')
270/67:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\s*', '$1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('b  sc(it)')
270/68:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\s*', '\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('b  sc(it)')
270/69:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('b  sc(it)')
270/70:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('b  sc(it)')
270/71:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = re.sub('\b(lateral entry|full time)\b', r' \1 ', s)
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('b  sc(it) le')
270/72:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('b  sc(it) le')
270/73:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('le b  sc(it)')
270/74:
def remo_brace(s):
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/75:
def remo_brace(s):
    s = re.sub('\b(b|m)[\s\.]sc\b', r'\1sc')
    s = re.sub('\ble\b', ' lateral entry ')
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('\ble\b|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/76:
def remo_brace(s):
    s = re.sub('\b(b|m)[\s\.]sc\b', r'\1sc', s)
    s = re.sub('\ble\b', ' lateral entry ', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('\ble\b|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/77:
def remo_brace(s):
    s = re.sub('\b(b|m)[\s\.]sc\b', r'\1sc', s)
    s = re.sub('^\wle^\w', ' lateral entry ', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('\ble\b|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/78:
def remo_brace(s):
    s = re.sub('\b(b|m)[\s\.]sc\b', r'\1sc', s)

    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('\ble\b|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/79:
def remo_brace(s):
    s = re.sub('\b(b|m)[\s\.]sc\b', r'\1sc', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('[\b^]le[\b$]|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/80:
def remo_brace(s):
    s = re.sub('\b(b|m)[\s\.]sc\b', r'\1sc', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('le|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/81:
def remo_brace(s):
    s = re.sub('\b(b|m)[\s\.]sc\b', r'\1sc', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/82:
def remo_brace(s):
    s = re.sub('\b(b|m)[\s\.]sc\b', r'\1sc', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('(\b\S)?\s+sc\.?\S+\b\s*', r'\1 science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/83:
def remo_brace(s):
    s = re.sub('\b(b|m)[\s\.]sc\b', r'\1sc', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\b\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/84:
def remo_brace(s):
    s = re.sub('\b(b|m)[\s\.]sc[\b$]', r'\1sc', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\b\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/85:
def remo_brace(s):
    s = re.sub('\b(b|m)\.sc[\b$]', r'\1sc', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\b\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/86:
def remo_brace(s):
    s = re.sub('(b|m)\.sc[\b$]', r'\1sc', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\b\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/87:
def remo_brace(s):
    s = re.sub('(b|m)\.sc[\b$]', \1sc', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\b\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/88:
def remo_brace(s):

    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\b\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('(le) b  sc(it)')
270/89:
def remo_brace(s):

    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\b\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
# remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('law (3  year)')
270/90:
def remo_brace(s):

    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\b\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/91:
def remo_brace(s):

    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/92:
def remo_brace(s):

    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
remo_brace('law(3 year)')
270/93:
def remo_brace(s):

    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    print(s)
    s = re.sub('\s+of|\s+in ','',s) 
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', 'computer application', s)
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba (master of business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/94:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/95:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = list(map(lambda x: re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', x), s))
    return ' '.join(s).strip()
            
remo_brace('mba(le) (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/96:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', s)
    return ' '.join(s).strip()
            
remo_brace('mba(le) (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/97:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\b(lateral entry|full time)\b(.*$)', r'\2 \1', s)
    return s
            
remo_brace('mba(le) (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/98:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r'\2 \1', s)
    return s
            
remo_brace('mba(le) (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/99:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s)
    return s
            
remo_brace('mba(le) (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/100:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    return s
            
remo_brace('mba(le) (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/101:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    s = re.subn('\s+', ' ', s)[0]
    return s
            
remo_brace('mba(le) (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/102:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    s = re.subn('\s+', ' ', s)[0]
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    return s
            
remo_brace('mba(le) one  (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/103:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    s = re.subn('\s+', ' ', s)[0]
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r' ', s)
    return s
            
remo_brace('mba(le) one  (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/104:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    s = re.subn('\s+', ' ', s)[0]
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r' ', s)
    s = re.sub('(one|two|three|four)', lambda m: nums.get(m.group(1)), s)

    return s
            
remo_brace('mba(le) one  (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/105:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    s = re.subn('\s+', ' ', s)[0]
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r' ', s)
    s = re.sub('(one|two|three|four)', lambda m: nums[m.group(1)], s)

    return s
            
remo_brace('mba(le) one  (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/106:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    s = re.subn('\s+', ' ', s)[0]
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: nums[m.group(1)], s)

    return s
            
remo_brace('mba(le) one  (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/107:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    s = re.subn('\s+', ' ', s)[0]
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)

    return s
            
remo_brace('mba(le) one  (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/108:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    s = re.subn('\s+', ' ', s)[0]
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yr\s+', 'year', s)

    return s
            
remo_brace('mba(le) one yr  (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/109:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    s = re.subn('\s+', ' ', s)[0]
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yr\s+', ' year ', s)

    return s
            
remo_brace('mba(le) one yr  (master of    business admisnistration)(mas bus adm ) (computer sc.45&engg)')
270/110:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    s = re.subn('\s+', ' ', s)[0]
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yr\s+', ' year ', s)

    return s
            
remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )  one yr   (computer sc.45&engg)')
270/111:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd year direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    s = re.subn('\s+', ' ', s)[0]
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yr\s+', ' year ', s)

    return s
            
remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/112:
data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\(')][['remo_br', 'spec']])
270/113:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s+(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yr\s+', ' year ', s)

    return s
            
remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/114:
data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\(')][['remo_br', 'spec']])
270/115:
def remo_brace(s):
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yr\s+', ' year ', s)

    return s
            
remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/116:
data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\(')][['remo_br', 'spec']])
270/117:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|lko', '', s)[0]
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of|\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yr\s+', ' year ', s)

    return s
            
remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/118:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|lko|u\s?g|p\s?g', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)

    return s
            
# remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
remo_brace('llb (three  year)')
270/119:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|lko|u\s?g|p\s?g', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    print(s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)

    return s
            
# remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
remo_brace('llb (three  year)')
270/120:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|lko|u\s?g|p\s?g', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    print(s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    print(s)
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one)|(two)|(three)|(four)', r'\1', s)
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)

    return s
            
# remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
remo_brace('llb (three  year)')
270/121:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|lko|u\s?g|p\s?g', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    print(s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    print(s)
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)

    return s
            
# remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
remo_brace('llb (three  year)')
270/122:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|lko|u\s?g|p\s?g', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)

    return s
            
# remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
remo_brace('llb (three  year)')
270/123:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|p\s?g', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('ful$l-time|ft\s?| reg\S+$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)

    return s
            
remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/124:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|p\s?g', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('com\S+\s?app\S+', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)

    return s
            
remo_brace('mba(le)(master of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/125:
data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\(')][['remo_br', 'spec']])
270/126:
#data['remo_br'] = data.spec.apply(remo_brace)
# print(data[data.spec.str.contains('\(')][['remo_br', 'spec']])
data.remo_br.unique().shape[0]
270/127:
#data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\(')][['remo_br', 'spec']])
data.remo_br.unique().shape[0]
270/128:
#data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\-')][['remo_br', 'spec']])
data.remo_br.unique().shape[0]
270/129:
#data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\_')][['remo_br', 'spec']])
data.remo_br.unique().shape[0]
270/130:
#data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('-')][['remo_br', 'spec']])
data.remo_br.unique().shape[0]
270/131:
#data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('_')][['remo_br', 'spec']])
data.remo_br.unique().shape[0]
270/132:
#data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\[]')][['remo_br', 'spec']])
data.remo_br.unique().shape[0]
270/133:
#data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\[')][['remo_br', 'spec']])
data.remo_br.unique().shape[0]
270/134:
#data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.spec.str.contains('\[')][['remo_br', 'spec']])
data.remo_br.unique().shape[0]
270/135:
#data['remo_br'] = data.spec.apply(remo_brace)
print(data[['remo_br', 'spec']])
# data.remo_br.unique().shape[0]
270/136:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S+', 'computer')
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)

    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/137:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S+', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)

    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/138:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S+', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.subn('(\w{5,})', lambda m: m if m[-1] not == 's' else m[:-1], s)
    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/139:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S+', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.subn('(\w{5,})', lambda m: m if (m[-1] != 's') else m[:-1], s)
    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/140:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S+', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.subn('([a-z]{5,})', lambda m: m if (m[-1] != 's') else m[:-1], s)
    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/141:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S+', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.sub('([a-z]{5,})', lambda m: m if (m[-1] != 's') else m[:-1], s)[0]
    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/142:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S+', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.sub(r'([a-z]{5,})', lambda m: m if (m[-1] != 's') else m[:-1], s)[0]
    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/143:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S+', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.sub(r'\s([a-z]{5,})\s', lambda m: m if (m[-1] != 's') else m[:-1], s)[0]
    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/144:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S+', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/145:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:any|all', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S+', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.sub('\s+arts?\s+', 'art', s)

    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/146:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:any|all', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S+', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.sub('\s+arts?\s+', 'art', s)

    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/147:
data['remo_br'] = data.spec.apply(remo_brace)
print(data[['remo_br', 'spec']])
# data.remo_br.unique().shape[0]
270/148:
# data['remo_br'] = data.spec.apply(remo_brace)
# print(data[['remo_br', 'spec']])
data.remo_br.unique().shape[0]
270/149:
# data['remo_br'] = data.spec.apply(remo_brace)
print(data[['remo_br', 'spec']])
270/150:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:any|all', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S*', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.sub('\s+arts?\s+', 'art', s)

    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/151:
data['remo_br'] = data.spec.apply(remo_brace)
print(data[['remo_br', 'spec']].head(300))
270/152:
# data['remo_br'] = data.spec.apply(remo_brace)
print(data[[data.remo_br.str.contains('and')]['remo_br', 'spec']].head(300))
270/153:
# data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.remo_br.str.contains('and')][['remo_br', 'spec']].head(300))
270/154:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:any|all', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S*', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.sub('\s+arts?\s+', 'art', s)
    s = re.sub('i?t and computer', 'it and computer', s)
    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
270/155:
# data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.remo_br.str.contains('and')][['remo_br', 'spec']].head(300))
270/156:
degrees = data.programme.apply(lambda x: x.split('-')[0])
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
print(degrees[:100])
len(degrees)
270/157:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
print(degrees[:100])
len(degrees)
270/158:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
print(set(degrees)[:100])
len(degrees)
270/159:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
print(set(degrees))
len(degrees)
270/160:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
degrees = set(degrees)
len(degrees)
270/161:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ']
for d in degrees:
    d.replace('\.','')
    d.replace(' in ', ' ')
    d.replace(' of ', '')
    d.replace('(', '')
    d.replace(')', '')
degrees
270/162:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ']
for d in degrees:
    d.replace('.','')
    d.replace(' in ', ' ')
    d.replace(' of ', '')
    d.replace('(', '')
    d.replace(')', '')
degrees
270/163:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ']
for d in degrees:
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', '')
    d = d.replace('(', '')
    d = d.replace(')', '')
degrees
270/164:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ']
for d in degrees:
    d = d.replace('\.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', '')
    d = d.replace('(', '')
    d = d.replace(')', '')
degrees
270/165:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ']
for d in degrees:
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', '')
    d = d.replace('(', '')
    d = d.replace(')', '')
degrees
270/166:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ']
for d in degrees:
    d = d.strip('.')
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', '')
    d = d.replace('(', '')
    d = d.replace(')', '')
degrees
270/167:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ']
for d in degrees:
    d = d.strip('\.')
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', '')
    d = d.replace('(', '')
    d = d.replace(')', '')
degrees
270/168:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ']
for d in degrees:
    d = d.strip('\.')
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', '')
    d = d.replace('(', '')
    d = d.replace(')', '')
    print(d)
degrees
270/169:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ']
for d in degrees:
    
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', '')
    d = d.replace('(', '')
    d = d.replace(')', '')
    print(d)
degrees
270/170:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ']
for d in degrees:
    
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', '')
    d = d.replace('(', '')
    d = d.replace(')', '')
    print(d)
270/171:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ', 'master of comm(helll)']
for d in degrees:
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', '')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    print(d)
270/172:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ', 'master of comm(helll)']
for d in degrees:
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    print(d)
270/173:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ', 'master of com (helll)']
for d in degrees:
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    print(d)
270/174:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ', 'master of com (helll)']
for d in degrees:
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    print(d)
270/175:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ', 'master of com (helll)']
deg = []
for d in degrees:
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    deg.append(d)
deg
270/176:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ', 'master of com (helll)']
deg = []
for d in degrees:
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    deg.append(d.strip())
deg
270/177:
# degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
# degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
# degrees = set(degrees)
degrees = ['m.b.a. ', 'master of com (helll)']
deg = []
for d in degrees:
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    deg.append(d.strip())
degrees = deg
degrees
270/178:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
degrees = set(degrees)
deg = []
for d in degrees:
    d = d.replace('.','')
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    deg.append(d.strip())
degrees = deg
degrees
270/179:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
degrees = set(degrees)
deg = []
for d in degrees:
    d = d.replace('.','')
    d = d.lower()
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    deg.append(d.strip())
degrees = deg
degrees
270/180:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
degrees = set(degrees)
deg = []
for d in degrees:
    d = d.replace('.','')
    d = d.lower()
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    deg.append(d.strip())
degrees = deg
degrees.sort()
270/181:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
degrees = set(degrees)
deg = []
for d in degrees:
    d = d.replace('.','')
    d = d.lower()
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    deg.append(d.strip())
degrees = deg
degrees.sort()
degrees
270/182:
from jupyterthemes import jtplot
jtplot.style(theme='chesterish')
271/1:
import numpy as np
import pandas as pd
import re
pd.set_option('display.height', 1000)
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
271/2:
import numpy as np
import pandas as pd
from warnings import filterwarnings
import re
filterwarnings('None')
pd.set_option('display.height', 1000)
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
271/3:
import numpy as np
import pandas as pd
from warnings import filterwarnings
import re
filterwarnings('ignore')

pd.set_option('display.height', 1000)
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
271/4:
import numpy as np
import pandas as pd
from warnings import filterwarnings
import re
filterwarnings('ignore')

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
272/1:
import numpy as np
import pandas as pd
from warnings import filterwarnings
import re
filterwarnings('ignore')
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
272/2:
import numpy as np
import pandas as pd
from warnings import filterwarnings
import re
filterwarnings('ignore')
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
272/3:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '- ', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = st.replace('_','-')
    st = st.replace('[','(')
    return st.replace(']',')')
syn_specialization('diploma in dialysis technology -  dd')
272/4: data = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1').astype(str)
272/5:
data['spec'] = data['discipline'].copy()
data = data.loc[:,['discipline', 'spec', 'discipline_group', 'programme']]
272/6: data.columns
272/7: data.head()
272/8: data['spec'].unique().shape
272/9: data['spec'].str.lower().unique().shape
272/10: data.spec = data.spec.str.lower()
272/11:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e.ct.r.ic.al eng. ')
272/12: data['spec_remove_dot'] = data['spec'].apply(remove_dot)
272/13:
data.spec = data.spec_remove_dot
data.drop('spec_remove_dot', axis=1)
data.spec.unique().shape[0]
272/14:
def full(s, l):
    for elem in l:
        if ''.join([w[0] for w in elem.split()]) == s: return elem
    else: return 'False'
full('mba', ['mba', 'master bussiness admin', 'mab', 'mas buss adm'])
272/15: data['spec'] = data['spec'].apply(con_brk)
272/16:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:any|all', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S*', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.sub('\s+arts?\s+', 'art', s)
    s = re.sub('i?t and computer', 'it and computer', s)
    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
272/17:
data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.remo_br.str.contains('and')][['remo_br', 'spec']].head(300))
272/18: data.remo_br.unique().shape[0]
272/19:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
degrees = set(degrees)
deg = []
for d in degrees:
    d = d.replace('.','')
    d = d.lower()
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    deg.append(d.strip())
degrees = deg
degrees.sort()
degrees
272/20:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
degrees = set(degrees)
deg = []
for d in degrees:
    d = d.replace('.','')
    d = d.lower()
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
#     d = d.replace('(', ' ')
#     d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    deg.append(d.strip())
degrees = deg
degrees.sort()
degrees
272/21:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
degrees = set(degrees)
deg = []
for d in degrees:
    d = d.replace('.','')
    d = d.lower()
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    deg.append(d.strip())
degrees = deg
degrees.sort()
degrees
272/22:
pattern = '[\s\b]|[\s\b]'.join(degrees)
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
272/23:
pattern = r'[\s\b]|[\s\b]'.join(degrees)
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
272/24: pat.sub('','mbaschool mba ')
272/25: pat.sub('','mbaschool mba hello')
272/26: pat.sub(' ','mbaschool mba hello')
272/27: pat.sub(' ','mbaschool ba mba hello')
272/28: pat.subn(' ','mbaschool ba mba hello')[0]
272/29:
pattern = '('+r'[\s\b]|[\s\b]'.join(degrees)+')'
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
272/30:
pattern = r'[\s\b]'+'(|)'.join(degrees)+r'[\s\b]'
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
272/31:
pattern = r'[\s\b]('+'|'.join(degrees)+r')[\s\b]'
pat = re.compile(pattern, re.IGNORECASE)
print(pattern)
272/32: pat.subn(' ','mbaschool ba mba hello')[0]
272/33: pat.sub(' ','mbaschool ba mba hello')
272/34: pat.sub(' ','mbaschool mba hello')
272/35: pat.subn(' ','mbaschool mba hello')
272/36: pat.subn(' ','mbaschool mba ba hello')
272/37: pat.subn(' ','mbaschool mba mba hello')
272/38: pat.subn(' ','mbaschool mba  mba hello')
272/39: pat.subn('  ','mbaschool mbamba hello')
272/40: pat.subn('  ','mbaschool mba mba hello')
272/41:
pattern = r'[\s\b]('+'|'.join(degrees)+r')[\b]'
pat = re.compile(pattern)
print(pattern)
272/42: pat.subn('  ','mbaschool mba mba hello')
272/43:
pattern = r'[\s]('+'|'.join(degrees)+r')[\s]'
pat = re.compile(pattern)
print(pattern)
272/44: pat.subn('  ','mbaschool mba mba hello')
272/45: pat.subn(' ','mbaschool mba mba hello')
272/46:
pattern = r'[\s\b]('+'|'.join(degrees)+r')[\s\b]'
pat = re.compile(pattern)
print(pattern)
272/47: pat.subn(' ','mbaschool hello')
272/48:
def remove_regexes(text, patterns):
    """Remove all the matches of any pattern in patterns from text."""
    bytes = bytearray(text)
    for pat in patterns:
        for m in re.finditer(pat, text):
            start, end = m.span()
            bytes[start:end] = [0] * (end-start)
    new_string = ''.join(chr(c) for c in bytes if c)
    return new_string
272/49: remove_regexes('mbaschool mba mba hello', pat)
272/50:
def remove_regexes(text, patterns):
    """Remove all the matches of any pattern in patterns from text."""
    bytes = bytearray(text, 'utf8')
    for pat in patterns:
        for m in re.finditer(pat, text):
            start, end = m.span()
            bytes[start:end] = [0] * (end-start)
    new_string = ''.join(chr(c) for c in bytes if c)
    return new_string
272/51: remove_regexes('mbaschool mba mba hello', pat)
272/52: remove_regexes('mbaschool mba mba hello', degrees)
272/53:
def remove_regexes(text, patterns):
    """Remove all the matches of any pattern in patterns from text."""
    bytes = bytearray(text, 'utf8')
    for pat in patterns:
        for m in re.finditer(pat, text):
            start, end = m.span()
            bytes[start:end] = [0] * (end-start)
    new_string = ' '.join(chr(c) for c in bytes if c)
    return new_string
272/54: remove_regexes('mbaschool mba mba hello', degrees)
272/55:
def remove_regexes(text, patterns):
    """Remove all the matches of any pattern in patterns from text."""
    bytes = bytearray(text, 'utf8')
    for pat in patterns:
        for m in re.finditer(pat, text):
            start, end = m.span()
            bytes[start:end] = [0] * (end-start)
    new_string = ''.join(chr(c) for c in bytes if c)
    return new_string
272/56: remove_regexes('mbaschool mba mba hello', degrees)
272/57:
degree = [r'[\s\b]'+d+r'[\s\b]' for d in degrees]
degree[:5]
272/58:
degree = ['[\s\b]'+d+r'[\s\b]' for d in degrees]
degree[:5]
272/59:
degree = [r'[sb]'+d+r'[\s\b]' for d in degrees]
degree[:5]
272/60:
degree = [r'[\sb]'+d+r'[\s\b]' for d in degrees]
degree[:5]
272/61:
degree = [r'[\s\b]'+d+r'[\s\b]' for d in degrees]
degree[:5]
272/62:
degree = [r'[\\s\\b]'+d+r'[\s\b]' for d in degrees]
degree[:5]
272/63:
degree = [r'[\s\b]'+d+r'[\s\b]' for d in degrees]
degree[:5]
272/64:
degree = [r'[s\b]'+d+r'[\s\b]' for d in degrees]
degree[:5]
272/65:
degree = [r'[\s\b]'+d+r'[\s\b]' for d in degrees]
degree[:5]
272/66: remove_regexes('mbaschool mba mba hello', degree)
272/67:
def remove_regexes(text, patterns):
    """Remove all the matches of any pattern in patterns from text."""
    bytes = bytearray(text, 'utf8')
    for pat in patterns:
        for m in re.finditer(pat, text):
            start, end = m.span()
            bytes[start:end] = [0] * (end-start)
    new_string = ''.join(chr(c) for c in bytes if c)+' '
    return new_string
272/68: remove_regexes('mbaschool mba mba hello', degree)
272/69:
def remove_regexes(text, patterns):
    """Remove all the matches of any pattern in patterns from text."""
    bytes = bytearray(text, 'utf8')
    for pat in patterns:
        for m in re.finditer(pat, text):
            start, end = m.span()
            bytes[start:end] = [0] * (end-start)
    new_string = ' '+''.join(chr(c) for c in bytes if c)+' '
    return new_string
272/70: remove_regexes('mbaschool mba mba hello', degree)
272/71:
degree = ['\s'+d+'\s' for d in degrees]
degree[:5]
272/72:
degree = ['\\s'+d+'\s' for d in degrees]
degree[:5]
272/73:
degree = ['\s'+d+'\s' for d in degrees]
degree[:5]
272/74:
degree = ['"\"s'+d+'\s' for d in degrees]
degree[:5]
272/75:
degree = ['\\s'+d+'\s' for d in degrees]
degree[:5]
272/76:
degree = ['\s'+d+'\s' for d in degrees]
degree[:5]
272/77:
degree = ['\\s'+d+'\s' for d in degrees]
degree[:5]
272/78: remove_regexes('mbaschool mba mba hello', degree)
272/79:
degree = ['\\\s'+d+'\s' for d in degrees]
degree[:5]
272/80:
degree = ['\s'+d+'\s' for d in degrees]
degree[:5]
272/81: remove_regexes('mbaschool mba mba hello', degree)
272/82:
degree = ['\s'+d for d in degrees]
degree[:5]
272/83: remove_regexes('mbaschool mba mba hello', degree)
272/84: remove_regexes('mba mbaschool mba mba hello mba', degree)
272/85: remove_regexes('mba mbaschool mba mba hello mba', degree).strip()
272/86: remove_regexes('mba mbaschoo ba l mba mba hello mba', degree).strip()
272/87: remove_regexes('mba mbaschoo schoolmba ba l mba mba hello mba', degree).strip()
272/88: remove_regexes('mba mbaschool schoolmba mba')
272/89:
def remove_regexes(text):
    """Remove all the matches of any pattern in patterns from text."""
    patterns = degree
    bytes = bytearray(text, 'utf8')
    for pat in patterns:
        for m in re.finditer(pat, text):
            start, end = m.span()
            bytes[start:end] = [0] * (end-start)
    new_string = ''.join(chr(c) for c in bytes if c)
    return new_string
272/90: remove_regexes('mba mbaschool schoolmba mba')
272/91: data['remo_deg'] = data['remo_br'].apply(remove_regexes)
272/92: data.remo_deg.unique().shape[0]
272/93: data.remo_deg.unique()
272/94: list()
272/95: list(data.remo_deg.unique())
272/96: data['remo_br1'] = data.remo_br.apply(lambda x: re.sub('full time', ''))
272/97: data['remo_br1'] = data.remo_br.apply(lambda x: re.sub('full time', '', x))
272/98: data['remo_br1'].unique().shape[0]
272/99: len(data.remo_deg.unique())
272/100: data.remo_br = data.remo_br1
272/101:
# data['remo_br'] = data.remo_br.apply(lambda x: re.sub('full time', '', x))
data.remo_br.unique().shape[0]
272/102:
degree = ['[\s\b]'+d for d in degrees]
degree[:5]
272/103:
degree = [r'[\s\b]'+d for d in degrees]
degree[:5]
272/104: data['remo_deg'] = data['remo_br'].apply(remove_regexes)
272/105: len(data.remo_deg.unique())
272/106: list(data.remo_deg.unique())
272/107: data.remo_deg.apply(lambda x: re.sub('^\d{2,}', '', x))
272/108: data.remo_deg.apply(lambda x: re.sub('^(\d{2,}|mca)', '', x))
272/109:
data.remo_deg = data.remo_deg.apply(lambda x: re.sub('^(\d{2,}|mca)', '', x))
data.remo_deg.apply(lambda x: x if len(x)>3 else '')
272/110:
data.remo_deg = data.remo_deg.apply(lambda x: re.sub('^(\d{2,}|mca|master computer applications?)', '', x))
data.remo_deg.apply(lambda x: x if len(x)>3 else '')
272/111: remove_regexes('mba')
272/112: remove_regexes('mba ma')
272/113:
degree = [r'[\b]'+d for d in degrees]
degree[:5]
272/114: remove_regexes('mba ma')
272/115:
degree = [r'[\s\b]'+d for d in degrees]
degree[:5]
272/116: remove_regexes('mba ma')
272/117: remove_regexes('mbashe ma')
272/118: remove_regexes('mbashe madls')
272/119:
degree = [r'[\s\b]'+d+'^[a-z]' for d in degrees]
degree[:5]
272/120: remove_regexes('mbashe madls')
272/121: remove_regexes('mba mbashe mba madls mba')
272/122:
degree = [r'[\s\b]'+d+'[^a-z]' for d in degrees]
degree[:5]
272/123: remove_regexes('mba mbashe mba madls mba')
272/124:
def remove_regexes(text):
    """Remove all the matches of any pattern in patterns from text."""
    patterns = degree
    bytes = bytearray(text, 'utf8')
    for pat in patterns:
        for m in re.finditer(pat, text):
            start, end = m.span()
            bytes[start:end] = [0] * (end-start)
    new_string = ''.join(chr(c) for c in bytes if c)+' '
    return new_string
272/125: remove_regexes('mba mbashe mba madls mba')
272/126:
degree = [r'[\s^]'+d+'[^a-z]' for d in degrees]
degree[:5]
272/127: remove_regexes('mba mbashe mba madls mba')
272/128:
degree = [r'[\s^]'+d for d in degrees]
degree[:5]
272/129: remove_regexes('mba mbashe mba madls mba')
272/130:
degree = [r'[\s^]'+d+r'[\s$]' for d in degrees]
degree[:5]
272/131: remove_regexes('mba mbashe mba madls mba')
272/132:
degree = [r'[^]'+d+r'[$]' for d in degrees]
degree[:5]
272/133: remove_regexes('mba mbashe mba madls mba')
272/134:
degree = ['\s'+d+'\s' for d in degrees]
degree[:5]
272/135:
data.remo_deg = data.remo_deg.apply(lambda x: re.sub('^(\d{2,}|mca|master computer applications?)', '', x))
data.remo_deg.apply(lambda x: x if len(x)>3 else '').unique().shape[0]
272/136:
data.remo_deg = data.remo_deg.apply(lambda x: re.sub('^(\d{2,}|mca|master computer applications?)', '', x))
data['remo_deg'] = data.remo_deg.apply(lambda x: x if len(x)>3 else '').unique().shape[0]
272/137: data.remo_deg
272/138:
degree = [d for d in degrees]
degree[:5]
272/139: remove_regexes('mba mbashe mba madls mba')
272/140:
degree = ['!\w'+d+'!\w' for d in degrees]
degree[:5]
272/141: remove_regexes('mba mbashe mba madls mba')
272/142: pattern
272/143: pattern
272/144:
patt = r'\b('+'|'.join(degrees)+r')\b'
patt
272/145:
patt = r'\b('+'|'.join(degrees)+r')\b'
re.sub(patt, ' ', 'mba mbashe mba madls mba')
272/146:
patt = r'\b('+'|'.join(degrees)+r')\b'
re.sub(patt, ' ', 'mba mbashe mba madlsmba mba')
273/1:
import numpy as np
import pandas as pd
from warnings import filterwarnings
import re
filterwarnings('ignore')
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
273/2:
import numpy as np
import pandas as pd
from warnings import filterwarnings
import re
filterwarnings('ignore')
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
273/3:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '- ', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = st.replace('_','-')
    st = st.replace('[','(')
    return st.replace(']',')')
syn_specialization('diploma in dialysis technology -  dd')
273/4: data = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1').astype(str)
273/5:
data['spec'] = data['discipline'].copy()
data = data.loc[:,['discipline', 'spec', 'discipline_group', 'programme']]
273/6: data.columns
273/7: data.head()
274/1:
import numpy as np
import pandas as pd
from warnings import filterwarnings
import re
filterwarnings('ignore')
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
274/2:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '- ', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = st.replace('_','-')
    st = st.replace('[','(')
    return st.replace(']',')')
syn_specialization('diploma in dialysis technology -  dd')
274/3: data = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1').astype(str)
274/4:
data['spec'] = data['discipline'].copy()
data = data.loc[:,['discipline', 'spec', 'discipline_group', 'programme']]
274/5: data.columns
274/6: data.head()
274/7: data['spec'].unique().shape
274/8: data['spec'].str.lower().unique().shape
274/9: data.spec = data.spec.str.lower()
274/10:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e.ct.r.ic.al eng. ')
274/11: data['spec_remove_dot'] = data['spec'].apply(remove_dot)
275/1:
import numpy as np
import pandas as pd
from warnings import filterwarnings
import re
filterwarnings('ignore')
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
275/2:
def syn_specialization(spec):
    spec = str(spec)
    specs = []
    temp = [None]*3
#     specy = re.sub('\)', ']', spec)
    tspec = re.sub('\]', '', spec)
#     specs.append(re.sub('\(', '[', specy))
    specs.append(re.sub('\[', '- ', tspec))
    specs.append(re.sub('\[', '', tspec))
    specs = list(set(specs + [spec]))
    specs = specs+temp
    return specs[:3]

def ad_brk(st):
    st = str(st)
    if '[' in st and ']' not in st:
        return st+']'
    if ']' in st and '[' not in st:
        return st.replace(']','')
    else: return st

def con_brk(st):
    st = st.replace('_','-')
    st = st.replace('[','(')
    return st.replace(']',')')
syn_specialization('diploma in dialysis technology -  dd')
275/3: data = pd.read_csv('../data/all_institutions_course_ids_live_new.csv', encoding='latin1').astype(str)
275/4:
data['spec'] = data['discipline'].copy()
data = data.loc[:,['discipline', 'spec', 'discipline_group', 'programme']]
275/5: data.columns
275/6: data.head()
275/7: data['spec'].unique().shape
275/8: data['spec'].str.lower().unique().shape
275/9: data.spec = data.spec.str.lower()
275/10:
def remove_dot(x):
    try:
        s = ''
        l = re.split('\.|\-|\_|,|\s', x.strip())
        for i in range(len(l)-1):
            if len(l[i]) < 3 and len(l[i+1]) < 3: s = s+l[i]+'.'
            else: s = s+l[i]+' '
        ans = s+' '+l[-1] if len(l[-1])>3 else s+l[-1]
        return re.subn('\.', '', ans)[0].strip()
    except:
        return x.strip()
remove_dot(' master b.sc.engg-el.e.ct.r.ic.al eng. ')
275/11: data['spec_remove_dot'] = data['spec'].apply(remove_dot)
275/12:
data.spec = data.spec_remove_dot
data.drop('spec_remove_dot', axis=1)
data.spec.unique().shape[0]
275/13:
def full(s, l):
    for elem in l:
        if ''.join([w[0] for w in elem.split()]) == s: return elem
    else: return 'False'
full('mba', ['mba', 'master bussiness admin', 'mab', 'mas buss adm'])
275/14: data['spec'] = data['spec'].apply(con_brk)
275/15:
def remo_brace(s):
    s = re.subn('mysore|asas|kochi|ase|amr|mkt|hsp|bp|sf|lko|u\s?g|:any|all', '', s)[0]
    s = re.sub('(b|m)\scom', r'\1com', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('mgmt', 'management', s)
    s = re.sub('\s*engg\S*', ' engineering ', s)
    s = re.sub('\s*agri\S+', ' agriculture ', s)
    s = re.sub('\s+sc\.?\S+\s*', ' science ', s)
    s = re.sub('\s+of |\s+in ',' ',s) 
    s = re.subn('\s+', ' ', s)[0]
    s = re.subn('\.', '', s)[0]
    s = re.sub('\s?&\s?', ' and ', s)
    s = re.sub('comp\S*', 'computer',s)
    s = re.sub('com\S+\s?app\S*', ' computer application ', s)
    s = re.subn('\s+', ' ', s)[0]
    s = re.split('\s*[\(\)\-]+\s*', s)
    s = list(map(lambda x: re.sub('full-time\s?$|ft\s?|reg\S+\s?$', 'full time', x), s))
    s = list(map(lambda x: re.sub('^le$|^lateral$|^leet$|^2nd (year|yr) direct$', 'lateral entry', x), s))
    for part in s:
        while full(part, s) in s:
            s.remove(full(part, s))
    s = ' '.join(s).strip()
    s = re.subn('\s+', ' ', s)[0]
    s = re.sub('\s*(lateral entry|full time)\s+(.*$)', r' \2 \1 ', s).strip()
    nums = {'one':1, 'two':2, 'three':3, 'four':4}
    s = re.sub('(one|two|three|four)', lambda m: str(nums[m.group(1)]), s)
    s = re.sub('\s+yrs?\s+', ' year ', s)
    s = re.sub('\s+arts?\s+', 'art', s)
    s = re.sub('i?t and computer', 'it and computer', s)
    return s
            
remo_brace('mba(le)(master: of    business admisnistration)(mas bus adm )one yr   (computer sc.45&engg)')
275/16:
data['remo_br'] = data.spec.apply(remo_brace)
print(data[data.remo_br.str.contains('and')][['remo_br', 'spec']].head(300))
275/17:
data['remo_br'] = data.remo_br.apply(lambda x: re.sub('full time', '', x))
data.remo_br.unique().shape[0]
275/18:
degrees = list(data.programme.apply(lambda x: x.split('-')[0]))
degrees.extend(data.programme.apply(lambda x: x.split('-')[-1]))
degrees = set(degrees)
deg = []
for d in degrees:
    d = d.replace('.','')
    d = d.lower()
    d = d.replace(' in ', ' ')
    d = d.replace(' of ', ' ')
    d = d.replace('(', ' ')
    d = d.replace(')', ' ')
    d = re.sub('\s+', ' ', d)
    deg.append(d.strip())
degrees = deg
degrees.sort()
degrees
275/19:
patt = r'\b('+'|'.join(degrees)+r')\b'
s = re.sub(patt, ' ', 'mba mbashe mba madlsmba mba')
s
275/20:
patt = r'\b('+'|'.join(degrees)+r')\b'
s = re.sub(patt, ' ', 'mba  mbashe shmbash madlsmba mba hsl')
s
275/21:
patt = r'\b('+'|'.join(degrees)+r')\b'
s = re.sub(patt, ' ', 'mba  mbashe shmbash madlsmba mba hsl')
s = re.sub('\s+', ' ', s)
s.strip()
   1: %history -g -f filename
